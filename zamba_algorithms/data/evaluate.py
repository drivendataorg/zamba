from collections import namedtuple
from functools import partial
from typing import Optional

from IPython.display import display
from loguru import logger
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
    top_k_accuracy_score,
    precision_score,
    recall_score,
)

from cloudpathlib import S3Path
import labslib.gregplotlib as gpl


from .metadata import load_metadata


s3p = S3Path("s3://drivendata-client-zamba")
PROCESSED_DIR = s3p / "data" / "processed"


class EvaluatePreds:
    def __init__(
        self,
        y_pred,
        y_true=None,
        zamba_label="original",
        metadata=None,
        subset="dev",
        metadata_index_col="filepath",
        blank_column="species_blank",
    ):
        """Instantate EvaluatePreds class. To automatically generate all key visualizations, run
            cls.all_visualizations() after instantiation.

        Args:
            y_pred (pd.DataFrame): predicted probability df with filepaths as the index and columns for each species
            y_true (pd.DataFrame, optional): true labels df with filepaths as the index and columns for each species.
                If not passed will use labels from the metadata. Defaults to None.
            zamba_label (str): which set of Zamba labels to use in load_metadata. Options: 'original', 'new'
            metadata (pd.DataFrame, optional): Dataframe generated by load_metadata. If not passed will be generated
                during instantiation. If `False` passed, will not load metadata. Defaults to None.
            subset (str, optional): Subset argument for the load_metadata function. Defaults to "dev".
            metadata_index_col (str, optional): Column to use as metadata index. This should be the same column that is used
                asin the index for y_true and y_pred.
        """
        if metadata is None:
            metadata = load_metadata(zamba_label=zamba_label, subset=subset)
            metadata = metadata.set_index(metadata_index_col)

        self.blank_column = blank_column

        # if y_true is not passed, use labels from the metadata
        if y_true is None:
            species_cols = metadata.filter(regex="species_").columns
            y_true = metadata[species_cols]

        # clean y_pred and y_true - remove nulls and align indices
        y_pred = EvaluatePreds._drop_null_preds(y_pred)
        self.y_true, self.y_pred = EvaluatePreds._align_dfs(y_true, y_pred)

        # order columns. This is needed for top-k accuracy, do here for consistency throughout
        self.y_pred = self.y_pred[self.y_true.columns.sort_values()]
        self.y_true = self.y_true[self.y_true.columns.sort_values()]

        # make sure all indices from true / preds are in the metadata
        if metadata is not None and metadata is not False:
            assert not len(self.y_true.index.difference(metadata.index))
            metadata = metadata.loc[self.y_true.index, :]
            self.species_cols = metadata.filter(regex="species_").columns
        else:
            metadata = None
            self.species_cols = self.y_true.columns

        self.metadata = metadata
        self.species_labels = EvaluatePreds._species_to_labels(self.species_cols)

    @staticmethod
    def _align_dfs(y_true, y_pred):
        """Aligns y_true (pd.DataFrame) and y_pred (pd.DataFrame) to only shared indices in the same order.

        Returns:
            (y_true, y_pred): aligned dataframes
        """
        # check specifically for predictions that are not in the labels (the reverse doesn't indicate a problem)
        pred_extras = y_pred.index.difference(y_true.index)
        np.setdiff1d(y_pred.index, y_true.index)
        if len(pred_extras) > 0:
            logger.info(
                f"Dropping {len(pred_extras)} filenames from the predictions that are not in the labels."
            )

        overlap = y_true.index.intersection(y_pred.index, sort=None)  # sorted overlapping index
        y_true = y_true.loc[overlap]
        y_pred = y_pred.loc[y_true.index, y_true.columns]

        assert (y_true.index == y_pred.index).all()

        return y_true, y_pred

    @staticmethod
    def _species_to_labels(species_list):
        def _reformat_column(col):
            if col.startswith("species_"):
                col = col.split("_", 1)[1]

            return col.replace("_", " ").title()

        return [_reformat_column(c) for c in species_list]

    @staticmethod
    def _drop_null_preds(y_pred):
        null_preds_idx = y_pred[y_pred.isnull().all(axis=1)].index
        if len(null_preds_idx) > 0:
            logger.warning(f"Dropping {len(null_preds_idx)} filenames with NaN predictions.")
            y_pred = y_pred.drop(null_preds_idx)

        return y_pred

    def classification_report_data(self):
        df = pd.DataFrame(
            classification_report(
                self.y_true,
                np.round(self.y_pred),
                target_names=self.y_true.columns,
                output_dict=True,
            )
        ).T

        return df

    def classification_report_plot(self):
        df = self.classification_report_data()

        display(
            df.loc[self.species_cols]
            .sort_values("f1-score", ascending=False)
            .style.background_gradient(
                "Greens",
                axis=None,
                subset=["precision", "recall", "f1-score"],
                high=0.2,
            )
            .background_gradient("gist_yarg", subset=["support"], high=2)
            .format("{:.2f}")
        )

    def top_k_accuracy_data(self, ks=[1, 3, 5, 10], sample_weight=None):
        # top_k_accuracy_score expects columns to be sorted by name
        k_scores = [
            top_k_accuracy_score(
                self.y_true.idxmax(axis=1),
                self.y_pred,
                k=k,
                sample_weight=sample_weight,
                labels=self.species_cols,
            )
            for k in ks
        ]

        return k_scores

    def top_k_accuracy_plot(self, ks=[1, 3, 5, 10], ax=None, plot_size="full"):
        """Generate a plot of top-k accuracy for varios values of k for weighted v. unweighted data

        Args:
            ks (list, optional): List of integers for which to calculate top-k accuracy. Defaults to [1, 3, 5, 10].
            ax (matplotlib.axes, optional): Axis on which to plot. If none, creates a new axis. Defaults to None.
            plot_size (str, optional): Size argument for gpl.styled_fig_ax used if an axis is created. Defaults to "full".

        Returns:
            matplotlib.axes
        """

        ys = self.top_k_accuracy_data()[: len(ks)]
        to_plot = [
            pd.Series(ys, name="Unweighted", index=[f"Top-{k}" for k in ks]).sort_values(
                ascending=False
            ),
        ]

        if self.metadata is not None:
            y_weight = self.metadata.weight[self.y_true.index]
            yws = self.top_k_accuracy_data(sample_weight=y_weight)[: len(ks)]
            to_plot.append(
                pd.Series(yws, name="Weighted", index=[f"Top-{k}" for k in ks]).sort_values(
                    ascending=False
                )
            )

        def _with_axis(ax):
            gpl.labeled_bar(
                ax,
                pd.concat(to_plot, axis=1),
                horizontal=True,
                stacked=False,
                label_formatter=gpl.pct_func_formatter(),
            )

            ax.set_title(f"Accuracies (n={self.y_true.shape[0]:,})")
            ax.set_title(f"Accuracies (n={self.y_true.shape[0]:,})")
            ax.legend(loc="right", bbox_to_anchor=(1.4, 0.9))
            plt.tight_layout()
            return ax

        if ax is not None:
            return _with_axis(ax)
        else:
            with gpl.styled_fig_ax(plot_size) as ax:
                return _with_axis(ax)

    def _threshold_data(
        self,
        metric,
        y_true=None,
        y_pred=None,
        thresholds=np.linspace(0, 1, 200),
        y_weight=None,
    ):
        """Calculates score for classifications based on a series of probability thresholds.

        Args:
            metric (function): Performance metric
            y_true (pd.DataFrame, optional): True labels. If none provided, uses self.y_true. Defaults to None.
            y_pred (pd.DataFrame, optional): Predicted probabilities. If none provided, uses self.y_pred. Defaults to None.
            thresholds (list, optional): List of probability thresholds to test. Defaults to np.linspace(0, 1, 200).
            y_weight (optional): Array-like of shape (n_samples,) to use as sample weights. Defaults to None.

        Returns:
            list of scores for each threshold
        """
        y_true = y_true if y_true is not None else self.y_true
        y_pred = y_pred if y_pred is not None else self.y_pred
        threshold_scores = [metric(y_true, y_pred > t, sample_weight=y_weight) for t in thresholds]

        return threshold_scores

    def _threshold_plot(
        self,
        ys,
        ax,
        metric,
        y_weight=None,
        metric_name=None,
        thresholds=np.linspace(0, 1, 200),
        add_label=True,
        annotate=False,
    ):
        """Generates a plot of performance over a series of thresholds

        Args:
            ys (list): scores for each threshold
            ax (matplotlib.axes, optional): Axis on which to plot. If none, creates a new axis. Defaults to None.
            metric (function): Metric function
            y_weight (array-like, optional): Sample weights, indicates whether data is weighted or unweighted. Defaults to None.
            metric_name (str, optional): Name of metric for plot title. If none, uses __name__ attribute of metric.
                Defaults to None.
            thresholds (list, optional): List of probability thresholds to test. Defaults to np.linspace(0, 1, 200).
            add_label (bool, optional): Whether to include a label in plotting that can be used in a legend. Should be set
                to true when unweighted and weighted scores are plotted together. Defaults to True.
            annotate (bool, optional): Whether to annotate the plot with the max. Defaults to False.

        Returns:
            matplotlib.axes
        """
        if metric_name is None:
            metric_name = metric.__name__.replace("_", " ").title()

        max_t = thresholds[np.argmax(ys)]
        max_y = np.max(ys)

        label = None
        if add_label:
            label = "Unweighted" if y_weight is None else "Weighted"
            label += f" (max@{max_t:.2}={max_y:.2})"
        ax.plot(thresholds, ys, label=label)

        ax.vlines(max_t, 0, max_y, color=gpl.colors.DEFAULT.gray, ls="--")
        if annotate:
            ax.text(max_t, max_y / 2, f"max@{max_t:.2}={max_y:.2}")

        ax.set_ylim((0, 1))
        ax.set_ylabel(f"{metric_name}")
        ax.set_xlabel("Threshold")

        title = f"{metric_name} v. Threshold\n(n={self.y_true.shape[0]:,})"
        if not label:
            title += " [weighted]" if y_weight is not None else ""
        ax.set_title(title)

        return ax

    def overall_threshold_plot(self, plot_size="full", axes=None, weighted=False):

        y_weight = None
        if weighted:
            y_weight = self.metadata.weight[self.y_true.index]

        accuracy_scores = self._threshold_data(accuracy_score, y_weight=y_weight)
        f1_scores = self._threshold_data(partial(f1_score, average="macro"), y_weight=y_weight)

        threshold_plot = partial(self._threshold_plot, add_label=False, annotate=True)

        def _with_axes(ax1, ax2):
            threshold_plot(accuracy_scores, ax1, accuracy_score, y_weight=y_weight)
            threshold_plot(
                f1_scores,
                ax2,
                partial(f1_score, average="macro"),
                metric_name="Macro F1",
                y_weight=y_weight,
            )

            return ax1, ax2

        if axes is not None:
            assert len(axes) == 2
            ax1, ax2 = axes
            return _with_axes(ax1, ax2)
        else:
            with gpl.styled_fig_ax(plot_size, subplots_rows=1, subplots_columns=2) as (ax1, ax2):
                return _with_axes(ax1, ax2)

    def combined_overall_threshold_plot(self, plot_size="full", axes=None):

        y_weight = self.metadata.weight[self.y_true.index]

        accuracy_scores_unweighted = self._threshold_data(accuracy_score, y_weight=None)
        f1_scores_unweighted = self._threshold_data(
            partial(f1_score, average="macro"), y_weight=None
        )
        accuracy_scores_weighted = self._threshold_data(accuracy_score, y_weight=y_weight)
        f1_scores_weighted = self._threshold_data(
            partial(f1_score, average="macro"), y_weight=y_weight
        )

        def _with_axes(ax1, ax2):
            # plot unweighted scores
            self._threshold_plot(accuracy_scores_unweighted, ax1, accuracy_score, y_weight=None)
            self._threshold_plot(
                f1_scores_unweighted,
                ax2,
                partial(f1_score, average="macro"),
                metric_name="Macro F1",
                y_weight=None,
            )

            # plot unweighted scores
            self._threshold_plot(accuracy_scores_weighted, ax1, accuracy_score, y_weight=y_weight)
            self._threshold_plot(
                f1_scores_weighted,
                ax2,
                partial(f1_score, average="macro"),
                metric_name="Macro F1",
                y_weight=y_weight,
            )

            ax1.legend()
            ax2.legend()

            return ax1, ax2

        if axes is not None:
            assert len(axes) == 2
            ax1, ax2 = axes
            return _with_axes(ax1, ax2)
        else:
            with gpl.styled_fig_ax(plot_size, subplots_rows=1, subplots_columns=2) as (ax1, ax2):
                return _with_axes(ax1, ax2)

    def location_score_data(self, k=1):
        """Calculate top-k accuracy by location.

        Returns:
            namedtuple(data, xticks): Named tuple where the first element is top-k accuracy for each location
                and the second is the text to use for xticks in the plot
        """
        ys = []
        xticks = []
        for country in self.metadata.country.unique():
            country_y_true = self.y_true.loc[self.metadata.country == country, :].idxmax(axis=1)
            country_y_pred = self.y_pred.loc[self.metadata.country == country, self.species_cols]

            xticks.append(f"{country} ({country_y_true.nunique()} species)")

            if country_y_pred.shape[1] == 2:
                country_y_pred = country_y_pred.iloc[:, 0]

            ys.append(
                top_k_accuracy_score(country_y_true, country_y_pred, k=k, labels=self.species_cols)
            )

        result = namedtuple("Result", "data xticks")

        return result(ys, xticks)

    def location_score_plot(self, metric=top_k_accuracy_score, k=1, ax=None, plot_size="full"):
        """Generates a plot of top-k accuracy by location.

        Args:
            metric (function, optional): Metric function. Defaults to top_k_accuracy_score.
            k (int, optional): Number for top-k accuracy. Defaults to 1.
            ax (matplotlib.axes, optional): Axis on which to plot. If none, creates a new axis. Defaults to None.
            plot_size (str, optional): Size argument for gpl.styled_fig_ax used if an axis is created. Defaults to "full".

        Returns:
            matplotlib.axes
        """
        (scores, xticks) = self.location_score_data(k=k)

        def _with_axis(ax):
            gpl.labeled_bar(
                ax,
                pd.Series(scores, name="Unweighted", index=xticks).sort_values(ascending=False),
                horizontal=True,
                stacked=False,
                label_formatter=gpl.pct_func_formatter(),
            )

            ax.set_title(f"Top {k} Accuracy (n={self.y_true.shape[0]:,})")
            plt.tight_layout()

            return ax

        if ax is not None:
            return _with_axis(ax)
        else:
            with gpl.styled_fig_ax(plot_size) as ax:
                return _with_axis(ax)

    def species_score_data(
        self,
        include_metrics=["accuracy", "precision", "recall", "f1 score"],
        drop_missing_species=False,
    ):
        """Calculates metrics by species

        Args:
            include_metrics (list, optional): Which metrics to include in the plot. Defaults to
                ["accuracy", "precision", "recall", "f1 score"].
            drop_missing_species (bool), default=False: Don't display species for which there are not videos compared

        Returns:
            pd.DataFrame with species as index and metrics as columns
        """
        ys = []
        index_labels = []
        for species in self.species_cols:
            if drop_missing_species and self.y_true[species].sum() == 0:
                continue

            index_labels.append(gpl.snake_to_title_case(species.split("species_")[-1]))
            ys.append(
                {
                    "accuracy": accuracy_score(self.y_true[species], self.y_pred[species] > 0.5),
                    "precision": precision_score(
                        self.y_true[species], self.y_pred[species] > 0.5, zero_division=0
                    ),
                    "recall": recall_score(self.y_true[species], self.y_pred[species] > 0.5),
                    "f1 score": f1_score(self.y_true[species], self.y_pred[species] > 0.5),
                }
            )
        score_df = pd.DataFrame(ys, index=index_labels).loc[:, include_metrics]

        return score_df

    def species_score_plot(
        self,
        axes=None,
        include_metrics=["accuracy", "precision", "recall", "f1 score"],
        drop_missing_species=False,
        sort_values: Optional[str] = "f1 score",
        sort_index: Optional[bool] = False,
        figsize=(12, 8),
        top_n=None,
        show_n=False,
    ):
        """Generates plot of various scores by species

        Args:
            axes (tuple of matplotlib.axes, optional): Tuple of axes on which to plot, which must match the length of
                include_metrics. If none, creates new axis. Defaults to None.
            include_metrics (list, optional): Which metrics to include in the plot. Defaults to
                ["accuracy", "precision", "recall", "f1 score"].
            drop_missing_species (bool), default=False: Don't display species for which there are not videos compared
            figsize (tuple of ints, optional): Figure size used when creating new axis. Defaults to (12, 8)
            top_n (int, optional): Top N best species to plot.
            show_n (bool, optioncal): Whether to add n={sample_size} to species bar labels.

        Returns:
            matplotlib.axes
        """
        scores_df = self.species_score_data(
            include_metrics=include_metrics, drop_missing_species=drop_missing_species
        )
        if sort_values:
            scores_df.sort_values(sort_values, inplace=True, ascending=True)
        if sort_index:
            scores_df.sort_index(inplace=True, ascending=False)

        if top_n is not None:
            scores_df = scores_df.tail(top_n)

        if show_n:
            # get species counts from y_true
            label_counts = self.y_true.sum(axis=0)
            label_counts.index = self._species_to_labels(label_counts.index)
            scores_df.index = scores_df.index.map(
                lambda x: f"{x}\nn={int(label_counts.to_dict()[x]):,}"
            )

        def _with_axes(axes):
            colors = gpl.colors.DEFAULT[: scores_df.shape[1]]

            for ax, col, c in zip(axes, scores_df.columns, colors):
                gpl.labeled_bar(
                    ax,
                    scores_df[col],
                    horizontal=True,
                    stacked=False,
                    label_formatter=gpl.pct_func_formatter(),
                    color=c,
                )

                ax.set_title(col.title())
                ax.set_xlim(0, 1)

            plt.tight_layout()
            return tuple(axes)

        if axes is not None:
            return _with_axes(axes)
        else:
            with gpl.styled_fig_ax(
                "custom",
                other_rc_params={"figure.figsize": figsize},
                subplots_columns=scores_df.shape[1],
                subplots_rows=1,
                subplots_kwargs={"sharey": True},
            ) as axes:
                return _with_axes(axes)

    def blank_score_data(self):
        """Calculates accuracy and F1 score for all videos based on blank v non-blank detection
            over a series of thresholds specified in _thredhold_data (defaults to np.linspace(0, 1, 200))

        Returns:
            namedtuple(accuracy_scores, f1_scores): a named tuple where the first element is the accuracy
                scores and the second is F1 scores
        """
        # specify thresholds to use as common index, same as default
        thresholds = np.linspace(0, 1, 200)
        accuracy_scores = self._threshold_data(
            accuracy_score,
            y_true=self.y_true[self.blank_column],
            y_pred=self.y_pred[self.blank_column],
            thresholds=thresholds,
        )
        f1_scores = self._threshold_data(
            f1_score,
            y_true=self.y_true[self.blank_column],
            y_pred=self.y_pred[self.blank_column],
            thresholds=thresholds,
        )

        return pd.DataFrame(
            {"accuracy": accuracy_scores, "f1_score": f1_scores, "threshold": thresholds}
        ).set_index("threshold")

    def blank_score_plot(self, axes=None, plot_size="full", misclassified_blank_threshold=0.1):
        """Plots accuracy and F1 score by threshold for blank v. non-blank detection

        Args:
            axes (matplotlib.axes, optional): Tuple of two axes on which to plot. If none, creates new axis.
                Defaults to None.
            plot_size (str, optional): Size argument for gpl.styled_fig_ax used if an axis is created. Defaults to "full".
            misclassified_blank_threshold (float, option): If more than misclassified_blank_threshold percent of blank
                videos were misclassified as a species, include a plot of predicted classes for blank videos.
                Defaults to 0.1

        Returns:
            matplotlib.axes
        """
        axes_to_return = []
        idx_blank = self.y_true[self.y_true[self.blank_column] == 1].index

        # if more than threshold are wrong, display a plot
        blank_classifications = pd.DataFrame(
            self.y_pred.loc[idx_blank].idxmax(axis=1).value_counts(normalize=True)
        ).rename(columns={0: "count"})
        if (
            blank_classifications.drop(self.blank_column, axis=0)
            if self.blank_column in blank_classifications.index
            else blank_classifications
        )["count"].sum() > misclassified_blank_threshold:
            blank_classifications.index = self._species_to_labels(blank_classifications.index)
            with gpl.styled_fig_ax("half-height") as ax:
                gpl.labeled_bar(
                    ax,
                    blank_classifications,
                    horizontal=True,
                    stacked=False,
                    label_formatter=gpl.pct_func_formatter(),
                    legend=False,
                )
                ax.set_title(
                    f"False non-blank classifications for blank videos (n={len(idx_blank)})"
                )
                plt.tight_layout()

                axes_to_return.append(ax)

        blank_score_data = self.blank_score_data()

        def _with_axes(ax1, ax2):
            self._threshold_plot(
                blank_score_data["accuracy"], ax1, accuracy_score, metric_name="Blanks Accuracy"
            )
            self._threshold_plot(
                blank_score_data["f1_score"],
                ax2,
                partial(f1_score, average="macro"),
                metric_name="Blanks F1 Score",
            )
            plt.tight_layout()

            return ax1, ax2

        if axes is not None:
            assert len(axes) == 2
            ax1, ax2 = axes
            axes_to_return.append(_with_axes(ax1, ax2))
        else:
            with gpl.styled_fig_ax(plot_size, subplots_rows=1, subplots_columns=2) as (ax1, ax2):
                axes_to_return.append(_with_axes(ax1, ax2))

        return axes_to_return

    def false_blanks_species_data(self, sort_values=True):
        false_blanks_vids = self.y_true[
            (self.y_pred.idxmax(axis=1) == self.blank_column)
            & ~self.y_true[self.blank_column].astype(bool)
        ]

        total_observed = self.y_true.sum()

        percent_of_total = false_blanks_vids.sum() / total_observed

        percent_of_total.index = self._species_to_labels(percent_of_total.index)
        percent_of_total.index = (
            percent_of_total.index + " " + total_observed.astype(int).apply(lambda x: f" ({x:,})")
        )

        if sort_values:
            percent_of_total.sort_values(inplace=True)
        else:
            percent_of_total.sort_index(ascending=False, inplace=True)

        return percent_of_total

    def false_blanks_species_plot(self, ax=None, plot_size="full", sort_values=True):
        """Generates a plot of false blank predictions by species. To get the plot data as a
            pd.DataFrame, use false_blanks_species_data

        Args:
            ax (matplotlib.axes, optional): Axis on which to plot. If none, creates a new axis. Defaults to None.
            plot_size (str, optional): Size argument for gpl.styled_fig_ax used if an axis is created. Defaults to "full".
            sort_values (bool, optional): If True, sort rows in descending order by value (false blank percentage). If False, sort rows alphabetically.

        Returns:
            matplotlib.axes: plot of false blanks by species
        """
        percent_of_total = self.false_blanks_species_data(sort_values=sort_values)

        def _with_axis(ax):
            gpl.labeled_bar(
                ax,
                percent_of_total,
                horizontal=True,
                label_formatter=gpl.pct_func_formatter(),
                annot_kwargs={"fontsize": 14},
            )
            ax.set_title("False blanks (percent of videos of a species)")

            plt.tight_layout()

            return ax

        if ax is not None:
            return _with_axis(ax)
        else:
            with gpl.styled_fig_ax(plot_size) as ax:
                return _with_axis(ax)

    def false_blanks_location_data(self):
        false_blanks_vids = self.y_true[
            (self.y_pred.idxmax(axis=1) == self.blank_column)
            & ~self.y_true[self.blank_column].astype(bool)
        ]

        percent_of_location = (
            self.metadata.loc[false_blanks_vids.index].country.value_counts()
            / self.metadata.country.value_counts()
        ).sort_values()

        return percent_of_location

    def false_blanks_location_plot(self, ax=None, plot_size="full"):
        """Generates a plot of false blank predictions by location. To get the plot data as a
            pd.DataFrame, use false_blanks_location_data

        Args:
            ax (matplotlib.axes, optional): Axis on which to plot. If none, creates a new axis. Defaults to None.
            plot_size (str, optional): Size argument for gpl.styled_fig_ax used if an axis is created. Defaults to "full".

        Returns:
            matplotlib.axes: plot of false blanks by location
        """
        percent_of_location = self.false_blanks_location_data()

        def _with_axis(ax):
            gpl.labeled_bar(
                ax,
                percent_of_location,
                horizontal=True,
                label_formatter=gpl.pct_func_formatter(),
                annot_kwargs={"fontsize": 14},
            )
            ax.set_title("False blanks (percent of videos at a location)")

            plt.tight_layout()

            return ax

        if ax is not None:
            return _with_axis(ax)
        else:
            with gpl.styled_fig_ax(plot_size) as ax:
                return _with_axis(ax)

    def confusion_matrix_data(self, exclude_blank=True):
        y_pred = self.y_pred
        y_true = self.y_true

        if exclude_blank:
            y_pred = y_pred[y_pred.idxmax(axis=1) != self.blank_column].drop(
                self.blank_column, axis=1
            )  # no predicted as blank
            y_true = y_true.loc[y_pred.index, :].drop(self.blank_column, axis=1)

        species_labels = self._species_to_labels(y_true.columns)
        cm = confusion_matrix(
            y_true.idxmax(axis=1), y_pred.idxmax(axis=1), labels=y_true.columns, normalize="true"
        )
        cm = pd.DataFrame(cm, index=species_labels, columns=species_labels)
        cm.index.name = "True label"
        cm.rename_axis("Predicted label", axis=1, inplace=True)

        return cm

    def confusion_matrix_plot(self, exclude_blank=True, ax=None, figsize=(20, 16)):
        """Generates a species confusion matrix plot. To get the plot data, use confusion_matrix_data

        Args:
            exclude_blank (bool, optional): Whether to exclude blank from the list of species. Defaults to True.
            ax (matplotlib.axes, optional): Axis on which to plot. If none, creates a new axis. Defaults to None.
            figsize (tuple, optional): Figure size. Defaults to (20, 16).

        Returns:
            matplotlib.axes: confusion matrix plot
        """
        cm = self.confusion_matrix_data(exclude_blank=exclude_blank)

        disp = ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=cm.columns)

        def _with_axis(ax):
            disp.plot(
                cmap=gpl.colors.get_palette(
                    "sequential", seq_colors=["#FFFFFF", "#CC698A", "#17344A"]
                ),
                ax=ax,
                include_values=True,
                values_format=".0%",
                xticks_rotation="vertical",
            )

            ax.set_title(
                "Confusion matrix\nDistribution of predicted classification by true label"
            )
            plt.tight_layout()

            return ax

        if ax is not None:
            return _with_axis(ax)
        else:
            with gpl.styled_fig_ax("custom", other_rc_params={"figure.figsize": figsize}) as ax:
                return _with_axis(ax)

    def common_confusion_data(self, exclude_blank=True, percent_of_species=0.05):
        """Generates a dataframe with commonly confused species

        Args:
            exclude_blank (bool, optional): Only look at species confusion and ignore blanks. Defaults to True.
            percent_of_species (float, optional): Only include a species confusion if more than this percent
                of the true videos of the species were misclassified. Defaults to 0.05.

        Returns:
            pd.Dataframe: species misclassification errors
        """
        y_pred = self.y_pred
        y_true = self.y_true

        if exclude_blank:
            y_pred = y_pred[y_pred.idxmax(axis=1) != self.blank_column]  # no predicted as blank
            y_true = y_true.loc[y_pred.index, :]

        cm_not_normed = confusion_matrix(
            y_true.idxmax(axis=1),
            y_pred.idxmax(axis=1),
            labels=y_true.columns,
        )

        np.fill_diagonal(cm_not_normed, 0)

        error_df = (
            pd.DataFrame(cm_not_normed, index=self.y_true.columns, columns=self.y_true.columns)
            .stack()
            .reset_index()
            .rename(columns={"level_0": "True label", "level_1": "Predicted label", 0: "Videos"})
            .assign(
                **{
                    "Percent all videos": lambda x: x.Videos / self.y_true.shape[0],
                    "Percent videos of true label": lambda x: x.Videos
                    / np.array([self.y_true.sum()[v] for v in x["True label"].values]),
                }
            )
            .replace(dict(zip(y_true.columns, self._species_to_labels(y_true.columns))))
            .sort_values("Percent videos of true label", ascending=False)
        ).reset_index(drop=True)

        # greater than 5% of the videos of the true species
        error_df = error_df[error_df["Percent videos of true label"] > percent_of_species]

        return error_df

    def common_confusion_plot(self, n_display=20, exclude_blank=True):
        """Displays a dataframe of commonly confused species. Returns formatted dataaframe - to get a
        pd.DataFrame that can be manipulated, use common_confusion_data

        Args:
            n_display (int, optional): How many rows of the dataframe to display. Defaults to 20.
            exclude_blank (bool, optional): Whether to exclude blank from the list of species. Defaults to True.

        Returns:
            pandas.io.formats.style.Styler: formatted dataframe for display.
        """
        cm = self.common_confusion_data(exclude_blank=exclude_blank)

        return (
            cm.head(n_display)
            .style.background_gradient(
                gpl.get_palette("sequential", seq_colors=["orange"], reverse=True)
            )
            .format({"Percent all videos": "{:,.2%}", "Percent videos of true label": "{:,.2%}"})
            .set_caption("Commonly confused species")
        )

    def all_plots(self):
        """Return axes with all performance plots, all with the default parameters"""

        return [
            self.top_k_accuracy_plot(),
            self.combined_overall_threshold_plot(),
            self.location_score_plot(),
            self.species_score_plot(),
            self.species_score_plot(sort_index=True),
            self.blank_score_plot(),
            self.false_blanks_species_plot(),
            self.false_blanks_location_plot(),
            self.confusion_matrix_plot(),
        ]

    def all_tables(self):
        """Return all styled dataframes"""
        return [self.common_confusion_plot(), self.classification_report_plot()]

    def show_tables(self):
        """Display all styled dataframes"""
        from IPython.display import display

        for t in self.all_tables():
            display(t)
