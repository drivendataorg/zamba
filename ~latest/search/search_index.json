{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Zamba","text":"<p>Zamba means \"forest\" in Lingala, a Bantu language spoken throughout the Democratic Republic of the Congo and the Republic of the Congo.</p> <p><code>zamba</code> is a tool built in Python that uses machine learning and computer vision to automatically detect and classify animals in camera trap images and videos. You can use <code>zamba</code> to:</p> <ul> <li>Identify which species appear in each image or video</li> <li>Filter out blank images or videos</li> <li>Create your own custom models that identify your species in your habitats</li> <li>Estimate the distance between animals in the frame and the video camera</li> <li>And more! \ud83d\ude48 \ud83d\ude49 \ud83d\ude4a</li> </ul> <p>The official video models in <code>zamba</code> can identify blank videos (where no animal is present) along with 32 species common to Africa and 11 species common to Europe. The official image models can identify 178 species from throughout the world. Users can also finetune models using their own labeled images and videos to then make predictions for new species and/or new ecologies.</p> <p><code>zamba</code> can be used both as a command-line tool and as a Python package. It is also available as a user-friendly website application, Zamba Cloud.</p> <p>We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the Model Zoo Wiki for others to be able to use!</p>"},{"location":"#installing-zamba","title":"Installing <code>zamba</code>","text":"<p>First, make sure you have the prerequisites installed:</p> <ul> <li>Python &gt;= 3.11</li> <li>FFmpeg &gt; 4.3</li> </ul> <p>Then run: <pre><code>pip install https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz\n</code></pre></p> <p>See the Installation page of the documentation for details.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Once you have <code>zamba</code> installed, some good starting points are:</p> <ul> <li>The Quickstart page for basic examples of usage</li> <li>The user tutorials for classifying images, classifying videos, or training a model</li> </ul>"},{"location":"#example-usage","title":"Example usage","text":"<p>Once <code>zamba</code> is installed, you can see the basic command options with: <pre><code>$ zamba --help\n\n Usage: zamba [OPTIONS] COMMAND [ARGS]...\n\n Zamba is a tool built in Python to automatically identify the species seen in camera trap\n videos from sites in Africa and Europe. Visit https://zamba.drivendata.org/docs for more\n in-depth documentation.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version                     Show zamba version and exit.                                \u2502\n\u2502 --install-completion          Install completion for the current shell.                   \u2502\n\u2502 --show-completion             Show completion for the current shell, to copy it or        \u2502\n\u2502                               customize the installation.                                 \u2502\n\u2502 --help                        Show this message and exit.                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 densepose      Run densepose algorithm on videos.                                         \u2502\n\u2502 depth          Estimate animal distance at each second in the video.                      \u2502\n\u2502 predict        Identify species in a video.                                               \u2502\n\u2502 train          Train a model on your labeled data.                                        \u2502\n| image          Tools for working with images instead of videos.                           |\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> <p><code>zamba</code> can be used \"out of the box\" to generate predictions or train a model using your own images and videos. <code>zamba</code> supports the same image formats as <code>pillow</code> and the same video formats as FFmpeg, which are listed here. Any images or videos that fail a set of validation checks will be skipped during inference or training.</p>"},{"location":"#classifying-unlabeled-images-and-videos","title":"Classifying unlabeled images and videos","text":"<p>Zamba classifies videos by default, but can easily be set to classify images instead. To get classifications for videos:</p> <p><pre><code>$ zamba predict --data-dir path/to/videos\n</code></pre> and for images:</p> <pre><code>$ zamba image predict --data-dir path/to/videos\n</code></pre> <p>By default, predictions will be saved to <code>zamba_predictions.csv</code>. Run <code>zamba predict --help</code> or <code>zamba image predict --help</code> to list all possible options to pass to <code>predict</code>.</p> <p>See the Quickstart page or the user tutorial on classifying images or classifying videos for more details.</p>"},{"location":"#training-a-model","title":"Training a model","text":"<p>Zamba defaults to training a model for classifying videos: <pre><code>$ zamba train --data-dir path/to/videos --labels path_to_labels.csv --save_dir my_trained_model\n</code></pre></p> <p>Training a model for images is similar: <pre><code>$ zamba image train --data-dir path/to/images --labels path_to_labels.csv --save_dir my_trained_model\n</code></pre></p> <p>The newly trained model will be saved to the specified save directory. The folder will contain a model checkpoint as well as training configuration, model hyperparameters, and validation and test metrics. Run <code>zamba train --help</code> or <code>zamba image train --help</code> to list all possible options to pass to <code>train</code>.</p> <p>You can use your trained model on new images or videos by editing the <code>train_configuration.yaml</code> that is generated by <code>zamba</code>. Add a <code>predict_config</code> section to the yaml that points to the checkpoint file that is generated:</p> <pre><code>...\n# generated train_config\n...\n\npredict_config:\n  checkpoint: PATH_TO_YOUR_CHECKPOINT_FILE\n</code></pre> <p>Now you can pass this configuration to the command line. See the Quickstart page or the user tutorial on training a model for more details.</p> <p>You can then share your model with others by adding it to the Model Zoo Wiki.</p>"},{"location":"#estimating-distance-between-animals-and-the-camera","title":"Estimating distance between animals and the camera","text":"<p>Depth-estimation models are also supported, but only for video files. For example: <pre><code>$ zamba depth --data-dir path/to/videos\n</code></pre></p> <p>By default, predictions will be saved to <code>depth_predictions.csv</code>. Run <code>zamba depth --help</code> to list all possible options to pass to <code>depth</code>.</p> <p>See the depth estimation page for more details.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We would love your contributions of code fixes, new models, additional training data, docs revisions, and anything else you can bring to the project!</p> <p>See the docs page on contributing to <code>zamba</code> for details.</p>"},{"location":"#reference-paper","title":"Reference paper","text":"<p>Dorne, E., Qi, J., Bull, P., Stephens, C., Bessone, M., Debetencourt, B., Fruth, B., Morgan, D., Palmer, M. S., Sanz, C., Wendefeuer, J., Crockford, C., Deschner, T., Langergraber, K. E., Piel, A. K., Robbins, M., Sommer, V., Stewart, F. A., Wittig, R. M., . . . Arandjelovic, M. (2025). Zamba: Computer vision for wildlife conservation. Proceedings of the Python in Science Conferences, 85\u2013111. https://doi.org/10.25080/crcw9835</p>"},{"location":"configurations/","title":"All configuration options","text":"<p>To make it easy to associate a model configuration with a set of results, zamba accepts a <code>yaml</code> file to define all of the relevant parameters for training or prediction. You can then store the configuration you used with the results in order to easily reproduce it in the future.</p> <p>In general, we've tried to pick defaults that are reasonable, but it is worth it to familiarize yourself with the options available.</p> <p>The primary configurations you may want to set are:</p> <ul> <li><code>VideoLoaderConfig</code>: Defines all possible parameters for how videos are loaded when working with videos</li> <li><code>PredictConfig</code>: Defines all possible parameters for model inference on videos</li> <li><code>TrainConfig</code>: Defines all possible parameters for model training on videos</li> <li><code>ImageClassificationPredictConfig</code>: Defines all possible parameters for model inference on images</li> <li><code>ImageClassificationTrainingConfig</code>: Defines all possible parameters for model training on images</li> </ul> <p>Here's a helpful diagram which shows how everything is related for the video workflows:</p> <p></p> <p></p>"},{"location":"configurations/#video-loading-arguments","title":"Video loading arguments","text":"<p>The <code>VideoLoaderConfig</code> class defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video.</p> <p>All video loading arguments can be specified either in a YAML file or when instantiating the <code>VideoLoaderConfig</code> class in Python. Some can also be specified directly in the command line.</p> <p>Each model comes with a default video loading configuration. If no user-specified video loading configuration is passed - either through a YAML file or the Python <code>VideoLoaderConfig</code> class - all video loading arguments will be set based on the defaults for the given model.</p> YAML file <pre><code>video_loader_config:\n    model_input_height: 240\n    model_input_width: 426\n    total_frames: 16\n    # ... other parameters\n</code></pre> Python <pre><code>from zamba.data.video import VideoLoaderConfig\nfrom zamba.models.config import PredictConfig\nfrom zamba.models.model_manager import predict_model\n\npredict_config = PredictConfig(data_dir=\"example_vids/\")\nvideo_loader_config = VideoLoaderConfig(\n    model_input_height=240,\n    model_input_width=426,\n    total_frames=16\n    # ... other parameters\n)\npredict_model(\n    predict_config=predict_config, video_loader_config=video_loader_config\n)\n</code></pre> <p>Let's look at the class documentation in Python.</p> <pre><code>&gt;&gt; from zamba.data.video import VideoLoaderConfig\n&gt;&gt; help(VideoLoaderConfig)\n\nclass VideoLoaderConfig(pydantic.main.BaseModel)\n |  VideoLoaderConfig(*,\n crop_bottom_pixels: int = None,\n i_frames: bool = False,\n scene_threshold: float = None,\n megadetector_lite_config: zamba.models.megadetector_lite_yolox.MegadetectorLiteYoloXConfig = None,\n frame_selection_height: int = None,\n frame_selection_width: int = None,\n total_frames: int = None,\n ensure_total_frames: bool = True,\n fps: float = None,\n early_bias: bool = False,\n frame_indices: List[int] = None,\n evenly_sample_total_frames: bool = False,\n pix_fmt: str = 'rgb24',\n model_input_height: int = None,\n model_input_width: int = None,\n cache_dir: pathlib.Path = None,\n cleanup_cache: bool = False) -&gt; None\n\n ...\n</code></pre>"},{"location":"configurations/#crop_bottom_pixels-int-optional","title":"<code>crop_bottom_pixels (int, optional)</code>","text":"<p>Number of pixels to crop from the bottom of the video (prior to resizing to <code>frame_selection_height</code>). This can sometimes be useful if your videos have a persistent timestamp/camera brand logo at the bottom. Defaults to <code>None</code></p>"},{"location":"configurations/#i_frames-bool-optional","title":"<code>i_frames (bool, optional)</code>","text":"<p>Only load the I-Frames. I-frames are highly dependent on the encoding of the video, so it is not recommended to use them unless you have verified that the i-frames of your videos are useful. Defaults to <code>False</code></p>"},{"location":"configurations/#scene_threshold-float-optional","title":"<code>scene_threshold (float, optional)</code>","text":"<p>Only load frames that correspond to scene changes, which are detected when <code>scene_threshold</code> percent of pixels are different. This can be useful for selecting frames efficiently if in general you have large animals and stable backgrounds. Defaults to <code>None</code></p>"},{"location":"configurations/#megadetector_lite_config-megadetectorliteyoloxconfig-optional","title":"<code>megadetector_lite_config (MegadetectorLiteYoloXConfig, optional)</code>","text":"<p>The <code>megadetector_lite_config</code> is used to specify any parameters that should be passed to the MegadetectorLite model for frame selection. For all possible options, see the <code>MegadetectorLiteYoloXConfig</code> class. If <code>megadetector_lite_config</code> is <code>None</code> (the default), the MegadetectorLite model will not be used to select frames.</p>"},{"location":"configurations/#frame_selection_height-int-optional-frame_selection_width-int-optional","title":"<code>frame_selection_height (int, optional), frame_selection_width (int, optional)</code>","text":"<p>Resize the video to this height and width in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size videos (setting to <code>None</code>) is recommended for MegadetectorLite, especially if your species of interest are smaller. Defaults to <code>None</code></p>"},{"location":"configurations/#total_frames-int-optional","title":"<code>total_frames (int, optional)</code>","text":"<p>Number of frames that should ultimately be returned. Defaults to <code>None</code></p>"},{"location":"configurations/#ensure_total_frames-bool","title":"<code>ensure_total_frames (bool)</code>","text":"<p>Some frame selection methods may yield varying numbers of frames depending on timestamps of the video frames. If <code>True</code>, ensure the requested number of frames is returned by either clipping or duplicating the final frame. If no frames are selected, returns an array of the desired shape with all zeros. Otherwise, return the array unchanged. Defaults to <code>True</code></p>"},{"location":"configurations/#fps-float-optional","title":"<code>fps (float, optional)</code>","text":"<p>Resample the video evenly from the entire duration to a specific number of frames per second. Use values less than 1 for rates lower than a single frame per second (e.g., <code>fps=0.5</code> will result in 1 frame every 2 seconds). Defaults to <code>None</code></p>"},{"location":"configurations/#early_bias-bool-optional","title":"<code>early_bias (bool, optional)</code>","text":"<p>Resamples to 24 fps and selects 16 frames biased toward the beginning of the video. This strategy was used by the Pri-matrix Factorization machine learning competition winner. Defaults to <code>False</code></p>"},{"location":"configurations/#frame_indices-listint-optional","title":"<code>frame_indices (list(int), optional)</code>","text":"<p>Select specific frame numbers. Note: frame selection is done after any resampling. Defaults to <code>None</code></p>"},{"location":"configurations/#evenly_sample_total_frames-bool-optional","title":"<code>evenly_sample_total_frames (bool, optional)</code>","text":"<p>Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to <code>False</code></p>"},{"location":"configurations/#pix_fmt-str-optional","title":"<code>pix_fmt (str, optional)</code>","text":"<p>FFmpeg pixel format, defaults to <code>rgb24</code> for RGB channels; can be changed to <code>bgr24</code> for BGR.</p>"},{"location":"configurations/#model_input_height-int-optional-model_input_width-int-optional","title":"<code>model_input_height (int, optional), model_input_width (int, optional)</code>","text":"<p>After frame selection, resize the video to this height and width in pixels. This controls the height and width of the video frames returned by <code>load_video_frames</code>.  Defaults to <code>None</code></p>"},{"location":"configurations/#cache_dir-path-optional","title":"<code>cache_dir (Path, optional)</code>","text":"<p>Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with <code>VIDEO_CACHE_DIR</code> environment variable. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs after the first. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs. Defaults to <code>None</code>, which means videos will not be cached.</p>"},{"location":"configurations/#cleanup_cache-bool-optional","title":"<code>cleanup_cache (bool, optional)</code>","text":"<p>Whether to delete the cache directory after training or predicting ends. Defaults to <code>False</code></p> <p></p>"},{"location":"configurations/#video-prediction-arguments","title":"Video prediction arguments","text":"<p>All possible model inference parameters for videos are defined by the <code>PredictConfig</code> class. Let's see the class documentation in Python:</p> <pre><code>&gt;&gt; from zamba.models.config import PredictConfig\n&gt;&gt; help(PredictConfig)\n\nclass PredictConfig(ZambaBaseModel)\n |  PredictConfig(*,\n data_dir: DirectoryPath = Path.cwd(),\n filepaths: FilePath = None,\n checkpoint: FilePath = None,\n model_name: zamba.models.config.ModelEnum = &lt;ModelEnum.time_distributed: 'time_distributed'&gt;,\n gpus: int = 0,\n num_workers: int = 3,\n batch_size: int = 2,\n save: bool = True,\n save_dir: Optional[Path] = None,\n overwrite: bool = False,\n dry_run: bool = False,\n proba_threshold: float = None,\n output_class_names: bool = False,\n weight_download_region: zamba.models.utils.RegionEnum = 'us',\n skip_load_validation: bool = False,\n model_cache_dir: pathlib.Path = None) -&gt; None\n\n ...\n</code></pre> <p>Either <code>data_dir</code> or <code>filepaths</code> must be specified to instantiate <code>PredictConfig</code>. If neither is specified, the current working directory will be used as the default <code>data_dir</code>.</p>"},{"location":"configurations/#data_dir-directorypath-optional","title":"<code>data_dir (DirectoryPath, optional)</code>","text":"<p>Path to the directory containing videos for inference. Defaults to the current working directory.</p>"},{"location":"configurations/#filepaths-filepath-optional","title":"<code>filepaths (FilePath, optional)</code>","text":"<p>Path to a csv containing a <code>filepath</code> column with paths to the videos that should be classified.</p>"},{"location":"configurations/#checkpoint-path-or-str-optional","title":"<code>checkpoint (Path or str, optional)</code>","text":"<p>Path to a model checkpoint to load and use for inference. If you train your own custom models, this is how you can pass those models to zamba when you want to predict on new videos. The default is <code>None</code>, which will load the pretrained checkpoint if the model specified by <code>model_name</code>.</p>"},{"location":"configurations/#model_name-time_distributedslowfasteuropeanblank_nonblank-optional","title":"<code>model_name (time_distributed|slowfast|european|blank_nonblank, optional)</code>","text":"<p>Name of the model to use for inference. The model options that ship with <code>zamba</code> are <code>blank_nonblank</code>, <code>time_distributed</code>, <code>slowfast</code>, and <code>european</code>. See the Available Models page for details. Defaults to <code>time_distributed</code></p>"},{"location":"configurations/#gpus-int-optional","title":"<code>gpus (int, optional)</code>","text":"<p>The number of GPUs to use during inference. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine.</p>"},{"location":"configurations/#num_workers-int-optional","title":"<code>num_workers (int, optional)</code>","text":"<p>The number of CPUs to use during training. The maximum value for <code>num_workers</code> is the number of CPUs available on the machine. If you are using MegadetectorLite for frame selection, it is not recommended to use the total number of CPUs available. Defaults to <code>3</code></p>"},{"location":"configurations/#batch_size-int-optional","title":"<code>batch_size (int, optional)</code>","text":"<p>The batch size to use for inference. Defaults to <code>2</code></p>"},{"location":"configurations/#save-bool","title":"<code>save (bool)</code>","text":"<p>Whether to save out predictions. If <code>False</code>, predictions are not saved. Defaults to <code>True</code>.</p>"},{"location":"configurations/#save_dir-path-optional","title":"<code>save_dir (Path, optional)</code>","text":"<p>An optional directory in which to save the model predictions and configuration yaml.  If no <code>save_dir</code> is specified and <code>save</code> is True, outputs will be written to the current working directory. Defaults to <code>None</code></p>"},{"location":"configurations/#overwrite-bool","title":"<code>overwrite (bool)</code>","text":"<p>If True, will overwrite <code>zamba_predictions.csv</code> and <code>predict_configuration.yaml</code> in <code>save_dir</code> if they exist. Defaults to False.</p>"},{"location":"configurations/#dry_run-bool-optional","title":"<code>dry_run (bool, optional)</code>","text":"<p>Specifying <code>True</code> is useful for ensuring a model implementation or configuration works properly by running only a single batch of inference. Defaults to <code>False</code></p>"},{"location":"configurations/#proba_threshold-float-between-0-and-1-optional","title":"<code>proba_threshold (float between 0 and 1, optional)</code>","text":"<p>For advanced uses, you may want the algorithm to be more or less sensitive to if a species is present. This parameter is a float, e.g., <code>0.6</code> corresponding to the probability threshold beyond which an animal is considered to be present in the video being analyzed.</p> <p>By default no threshold is passed, <code>proba_threshold=None</code>. This will return a probability from 0-1 for each species that could occur in each video. If a threshold is passed, then the final prediction value returned for each class is <code>probability &gt;= proba_threshold</code>, so that all class values become <code>0</code> (<code>False</code>, the species does not appear) or <code>1</code> (<code>True</code>, the species does appear).</p>"},{"location":"configurations/#output_class_names-bool-optional","title":"<code>output_class_names (bool, optional)</code>","text":"<p>Setting this option to <code>True</code> yields the most concise output <code>zamba</code> is capable of. The highest species probability in a video is taken to be the only species in that video, and the output returned is simply the video name and the name of the species with the highest class probability, or <code>blank</code> if the most likely classification is no animal. Defaults to <code>False</code></p>"},{"location":"configurations/#weight_download_region-useuasia","title":"<code>weight_download_region [us|eu|asia]</code>","text":"<p>Because <code>zamba</code> needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. <code>us</code> is the default, but if you are not in the US you should use either <code>eu</code> for the European Union or <code>asia</code> for Asia Pacific to make sure that these download as quickly as possible for you.</p>"},{"location":"configurations/#skip_load_validation-bool-optional","title":"<code>skip_load_validation (bool, optional)</code>","text":"<p>By default, before kicking off inference <code>zamba</code> will iterate through all of the videos in the data and verify that each can be loaded. Setting <code>skip_load_verification</code> to <code>True</code> skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future inference runs if the videos have not changed. Defaults to <code>False</code></p>"},{"location":"configurations/#model_cache_dir-path-optional","title":"<code>model_cache_dir (Path, optional)</code>","text":"<p>Cache directory where downloaded model weights will be saved. If None and the <code>MODEL_CACHE_DIR</code> environment variable is not set, will use your default cache directory (e.g. <code>~/.cache</code>). Defaults to <code>None</code></p> <p></p>"},{"location":"configurations/#video-training-arguments","title":"Video training arguments","text":"<p>All possible model training parameters for videos are defined by the <code>TrainConfig</code> class. Let's see the class documentation in Python:</p> <pre><code>&gt;&gt; from zamba.models.config import TrainConfig\n&gt;&gt; help(TrainConfig)\n\nclass TrainConfig(ZambaBaseModel)\n |  TrainConfig(*,\n labels: Union[FilePath, pandas.DataFrame],\n data_dir: DirectoryPath = # your current working directory ,\n checkpoint: FilePath = None,\n scheduler_config: Union[str, zamba.models.config.SchedulerConfig, NoneType] = 'default',\n model_name: zamba.models.config.ModelEnum = &lt;ModelEnum.time_distributed: 'time_distributed'&gt;,\n dry_run: Union[bool, int] = False,\n batch_size: int = 2,\n auto_lr_find: bool = False,\n backbone_finetune_config: zamba.models.config.BackboneFinetuneConfig =\n            BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5,\n            backbone_initial_ratio_lr=0.01, multiplier=1,\n            pre_train_bn=False, train_bn=False, verbose=True),\n gpus: int = 0,\n num_workers: int = 3,\n max_epochs: int = None,\n early_stopping_config: zamba.models.config.EarlyStoppingConfig =\n            EarlyStoppingConfig(monitor='val_macro_f1', patience=5,\n            verbose=True, mode='max'),\n weight_download_region: zamba.models.utils.RegionEnum = 'us',\n split_proportions: Dict[str, int] = {'train': 3, 'val': 1, 'holdout': 1},\n save_dir: pathlib.Path = # your current working directory ,\n overwrite: bool = False,\n skip_load_validation: bool = False,\n from_scratch: bool = False,\n use_default_model_labels: bool = True,\n model_cache_dir: pathlib.Path = None) -&gt; None\n\n ...\n</code></pre>"},{"location":"configurations/#labels-filepath-or-pddataframe-required","title":"<code>labels (FilePath or pd.DataFrame, required)</code>","text":"<p>Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for <code>filename</code> and <code>label</code>. <code>labels</code> must be specified to instantiate <code>TrainConfig</code>.</p>"},{"location":"configurations/#data_dir-directorypath-optional_1","title":"<code>data_dir (DirectoryPath, optional)</code>","text":"<p>Path to the directory containing training videos. Defaults to the current working directory.</p>"},{"location":"configurations/#checkpoint-path-or-str-optional_1","title":"<code>checkpoint (Path or str, optional)</code>","text":"<p>Path to a model checkpoint to load and resume training from. The default is <code>None</code>, which automatically loads the pretrained checkpoint for the model specified by <code>model_name</code>. Since the default <code>model_name</code> is <code>time_distributed</code> the default <code>checkpoint</code> is <code>zamba_time_distributed.ckpt</code></p>"},{"location":"configurations/#scheduler_config-zambamodelsconfigschedulerconfig-optional","title":"<code>scheduler_config (zamba.models.config.SchedulerConfig, optional)</code>","text":"<p>A PyTorch learning rate schedule to adjust the learning rate based on the number of epochs. Scheduler can either be <code>default</code> (the default), <code>None</code>, or a <code>torch.optim.lr_scheduler</code>.</p>"},{"location":"configurations/#model_name-time_distributedslowfasteuropeanblank_nonblank-optional_1","title":"<code>model_name (time_distributed|slowfast|european|blank_nonblank, optional)</code>","text":"<p>Name of the model to use for inference. The model options that ship with <code>zamba</code> are <code>blank_nonblank</code>, <code>time_distributed</code>, <code>slowfast</code>, and <code>european</code>. See the Available Models page for details. Defaults to <code>time_distributed</code></p>"},{"location":"configurations/#dry_run-bool-optional_1","title":"<code>dry_run (bool, optional)</code>","text":"<p>Specifying <code>True</code> is useful for trying out model implementations more quickly by running only a single batch of train and validation. Defaults to <code>False</code></p>"},{"location":"configurations/#batch_size-int-optional_1","title":"<code>batch_size (int, optional)</code>","text":"<p>The batch size to use for training. Defaults to <code>2</code></p>"},{"location":"configurations/#auto_lr_find-bool-optional","title":"<code>auto_lr_find (bool, optional)</code>","text":"<p>Whether to run a learning rate finder algorithm when calling <code>pytorch_lightning.trainer.tune()</code> to try to find an optimal initial learning rate. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution. See the PyTorch Lightning docs for more details. Defaults to <code>False</code></p>"},{"location":"configurations/#backbone_finetune_config-zambamodelsconfigbackbonefinetuneconfig-optional","title":"<code>backbone_finetune_config (zamba.models.config.BackboneFinetuneConfig, optional)</code>","text":"<p>Set parameters to finetune a backbone model to align with the current learning rate. Derived from Pytorch Lightning's built-in <code>BackboneFinetuning</code>. The default values are specified in the <code>BackboneFinetuneConfig</code> class: <code>BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True)</code></p>"},{"location":"configurations/#gpus-int-optional_1","title":"<code>gpus (int, optional)</code>","text":"<p>The number of GPUs to use during training. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine.</p>"},{"location":"configurations/#num_workers-int-optional_1","title":"<code>num_workers (int, optional)</code>","text":"<p>The number of CPUs to use during training. The maximum value for <code>num_workers</code> is the number of CPUs available in the system. If you are using the Megadetector, it is not recommended to use the total number of CPUs available. Defaults to <code>3</code></p>"},{"location":"configurations/#max_epochs-int-optional","title":"<code>max_epochs (int, optional)</code>","text":"<p>The maximum number of epochs to run during training. Defaults to <code>None</code></p>"},{"location":"configurations/#early_stopping_config-zambamodelsconfigearlystoppingconfig-optional","title":"<code>early_stopping_config (zamba.models.config.EarlyStoppingConfig, optional)</code>","text":"<p>Parameters to pass to Pytorch lightning's <code>EarlyStopping</code> to monitor a metric during model training and stop training when the metric stops improving. The default values are specified in the <code>EarlyStoppingConfig</code> class: <code>EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max')</code></p>"},{"location":"configurations/#weight_download_region-useuasia_1","title":"<code>weight_download_region [us|eu|asia]</code>","text":"<p>Because <code>zamba</code> needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. <code>us</code> is the default, but if you are not in the US you should use either <code>eu</code> for the European Union or <code>asia</code> for Asia Pacific to make sure that these download as quickly as possible for you.</p>"},{"location":"configurations/#split_proportions-dictstr-int-optional","title":"<code>split_proportions (dict(str, int), optional)</code>","text":"<p>The proportion of data to use during training, validation, and as a holdout set. Defaults to <code>{\"train\": 3, \"val\": 1, \"holdout\": 1}</code></p>"},{"location":"configurations/#save_dir-path-optional_1","title":"<code>save_dir (Path, optional)</code>","text":"<p>Directory in which to save model checkpoint and configuration file. If not specified, will save to a <code>version_n</code> folder in your current working directory.</p>"},{"location":"configurations/#overwrite-bool-optional","title":"<code>overwrite (bool, optional)</code>","text":"<p>If <code>True</code>, will save outputs in <code>save_dir</code> and overwrite the directory if it exists. If False, will create an auto-incremented <code>version_n</code> folder within <code>save_dir</code> with model outputs. Defaults to <code>False</code></p>"},{"location":"configurations/#skip_load_validation-bool-optional_1","title":"<code>skip_load_validation (bool, optional)</code>","text":"<p>By default, before kicking off training <code>zamba</code> will iterate through all of the videos in the training data and verify that each can be loaded. Setting <code>skip_load_verification</code> to <code>True</code> skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future training runs if the videos have not changed. Defaults to <code>False</code></p>"},{"location":"configurations/#from_scratch-bool-optional","title":"<code>from_scratch (bool, optional)</code>","text":"<p>Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None. Defaults to <code>False</code></p>"},{"location":"configurations/#use_default_model_labels-bool-optional","title":"<code>use_default_model_labels (bool, optional)</code>","text":"<p>Whether the species outputted by the model should be the default model classes (e.g. all 32 species classes for the <code>time_distributed</code> model). If you want the model classes to only be the species in your labels file (e.g. just gorillas and elephants), set to <code>False</code>. If either <code>use_default_model_labels</code> is <code>False</code> or the labels contain species that are not in the model, the model head will be replaced for finetuning. Defaults to <code>True</code></p>"},{"location":"configurations/#model_cache_dir-path-optional_1","title":"<code>model_cache_dir (Path, optional)</code>","text":"<p>Cache directory where downloaded model weights will be saved. If None and the <code>MODEL_CACHE_DIR</code> environment variable is not set, will use your default cache directory, which is often an automatic temp directory at <code>~/.cache/zamba</code>. Defaults to <code>None</code></p> <p></p>"},{"location":"configurations/#image-prediction-arguments","title":"Image prediction arguments","text":"<p>All possible model inference parameters for images are defined by the <code>ImageClassificationPredictConfig</code> class.</p> <pre><code>&gt;&gt; from zamba.images.config import ImageClassificationPredictConfig\n&gt;&gt; help(ImageClassificationPredictConfig)\n</code></pre> <p>Here's a description of all the parameters:</p>"},{"location":"configurations/#checkpoint-filepath-optional","title":"<code>checkpoint (FilePath, optional)</code>","text":"<p>Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to generate predictions. If None, defaults to a pretrained model. Defaults to None.</p>"},{"location":"configurations/#model_name-str-optional","title":"<code>model_name (str, optional)</code>","text":"<p>Name of the model to use for inference. Options are: lila.science. Defaults to lila.science.</p>"},{"location":"configurations/#filepaths-filepath-optional_1","title":"<code>filepaths (FilePath, optional)</code>","text":"<p>Path to a CSV containing images for inference, with one row per image in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all files in data_dir. Defaults to None.</p>"},{"location":"configurations/#data_dir-directorypath-optional_2","title":"<code>data_dir (DirectoryPath, optional)</code>","text":"<p>Path to a directory containing images for inference. Defaults to the working directory.</p>"},{"location":"configurations/#save-bool-optional","title":"<code>save (bool, optional)</code>","text":"<p>Whether to save out predictions. If False, predictions are not saved. Defaults to True.</p>"},{"location":"configurations/#save_dir-path-optional_2","title":"<code>save_dir (Path, optional)</code>","text":"<p>An optional directory in which to save the model predictions and configuration yaml. If no save_dir is specified and save=True, outputs will be written to the current working directory. Defaults to None.</p>"},{"location":"configurations/#overwrite-bool-optional_1","title":"<code>overwrite (bool, optional)</code>","text":"<p>If True, overwrite outputs in save_dir if they exist. Defaults to False.</p>"},{"location":"configurations/#crop_images-bool-optional","title":"<code>crop_images (bool, optional)</code>","text":"<p>Preprocess images using Megadetector or bounding box from labels file. Default is True.</p>"},{"location":"configurations/#detections_threshold-float-optional","title":"<code>detections_threshold (float, optional)</code>","text":"<p>Threshold for Megadetector. Default value is 0.2.</p>"},{"location":"configurations/#gpus-int-optional_2","title":"<code>gpus (int, optional)</code>","text":"<p>Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine.</p>"},{"location":"configurations/#num_workers-int-optional_2","title":"<code>num_workers (int, optional)</code>","text":"<p>Number of workers for parallel processing. Default is 3.</p>"},{"location":"configurations/#image_size-int-optional","title":"<code>image_size (int, optional)</code>","text":"<p>Image size for the input of the classification model. Default is 224.</p>"},{"location":"configurations/#results_file_format-resultsformat-optional","title":"<code>results_file_format (ResultsFormat, optional)</code>","text":"<p>The format in which to output the predictions. Currently 'csv' and 'megadetector' JSON formats are supported. Default is 'csv'.</p>"},{"location":"configurations/#results_file_name-path-optional","title":"<code>results_file_name (Path, optional)</code>","text":"<p>The filename for the output predictions in the save directory. Default is \"zamba_predictions.csv\".</p>"},{"location":"configurations/#model_cache_dir-path-optional_2","title":"<code>model_cache_dir (Path, optional)</code>","text":"<p>Cache directory where downloaded model weights will be saved. If None and no environment variable is set, will use your default cache directory. Defaults to None.</p>"},{"location":"configurations/#weight_download_region-str-optional","title":"<code>weight_download_region (str, optional)</code>","text":"<p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p> <p></p>"},{"location":"configurations/#image-training-arguments","title":"Image training arguments","text":"<p>All possible model training parameters for images are defined by the <code>ImageClassificationTrainingConfig</code> class.</p> <pre><code>&gt;&gt; from zamba.images.config import ImageClassificationTrainingConfig\n&gt;&gt; help(ImageClassificationTrainingConfig)\n</code></pre> <p>Here's a description of all the parameters:</p>"},{"location":"configurations/#data_dir-path-required","title":"<code>data_dir (Path, required)</code>","text":"<p>Where to find the files listed in filepaths (or where to look if filepaths is not provided).</p>"},{"location":"configurations/#labels-pddataframe-or-filepath-required","title":"<code>labels (pd.DataFrame or FilePath, required)</code>","text":"<p>Labels dataframe or path to CSV file containing labels.</p>"},{"location":"configurations/#labels_format-bboxformat-optional","title":"<code>labels_format (BboxFormat, optional)</code>","text":"<p>Format for bounding box annotations. Defaults to BboxFormat.COCO.</p>"},{"location":"configurations/#checkpoint-filepath-optional_1","title":"<code>checkpoint (FilePath, optional)</code>","text":"<p>Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to resume training. If None, defaults to a pretrained model. Defaults to None.</p>"},{"location":"configurations/#model_name-str-optional_1","title":"<code>model_name (str, optional)</code>","text":"<p>Base model name that will be loaded by timm lib (e.g. resnet50). Default is lila.science.</p>"},{"location":"configurations/#name-str-optional","title":"<code>name (str, optional)</code>","text":"<p>Classification experiment name (MLFlow). Default value is 'image-classification'.</p>"},{"location":"configurations/#max_epochs-int-optional_1","title":"<code>max_epochs (int, optional)</code>","text":"<p>Max training epochs. Default value is 100.</p>"},{"location":"configurations/#lr-float-optional","title":"<code>lr (float, optional)</code>","text":"<p>Learning rate value. Default value is 1e-5. If None, will find a good learning rate.</p>"},{"location":"configurations/#image_size-int-optional_1","title":"<code>image_size (int, optional)</code>","text":"<p>Image desired size. Default value is 224.</p>"},{"location":"configurations/#batch_size-int-optional_2","title":"<code>batch_size (int, optional)</code>","text":"<p>Batch size. Default value is 16. This is the physical batch size; use accumulated_batch_size to set the virtual batch size.</p>"},{"location":"configurations/#accumulated_batch_size-int-optional","title":"<code>accumulated_batch_size (int, optional)</code>","text":"<p>Accumulated batch size; will accumulate gradients to this virtual batch size. Useful to match batch size / learning rate from published papers. If not specified, will use batch_size.</p>"},{"location":"configurations/#early_stopping_patience-int-optional","title":"<code>early_stopping_patience (int, optional)</code>","text":"<p>Number of epochs with no improvement after which training will be stopped. Defaults to 3.</p>"},{"location":"configurations/#extra_train_augmentations-bool-optional","title":"<code>extra_train_augmentations (bool, optional)</code>","text":"<p>If false, uses simple transforms for camera trap imagery (random perspective shift, random horizontal flip, random rotation); if true, also uses more complex transforms (random perspective shift, random horizontal flip, random rotation, random grayscale, random equalize, random autocontrast, random adjust sharpness).</p>"},{"location":"configurations/#num_workers-int-optional_3","title":"<code>num_workers (int, optional)</code>","text":"<p>Number of workers to use for data loading. If None, default value is 8 (or 2/3 of available cores).</p>"},{"location":"configurations/#accelerator-str-optional","title":"<code>accelerator (str, optional)</code>","text":"<p>Accelerator type. Default is \"gpu\" if CUDA is available, otherwise \"cpu\".</p>"},{"location":"configurations/#devices-any-optional","title":"<code>devices (Any, optional)</code>","text":"<p>Devices to use for training. Default is \"auto\".</p>"},{"location":"configurations/#crop_images-bool-optional_1","title":"<code>crop_images (bool, optional)</code>","text":"<p>Preprocess images using Megadetector or bbox from labels file. Default is True.</p>"},{"location":"configurations/#detections_threshold-float-optional_1","title":"<code>detections_threshold (float, optional)</code>","text":"<p>Threshold for Megadetector. Applied only if the bbox is not specified in the labels. Default value is 0.2.</p>"},{"location":"configurations/#checkpoint_path-path-optional","title":"<code>checkpoint_path (Path, optional)</code>","text":"<p>Directory for where to save the output files; defaults to current working directory.</p>"},{"location":"configurations/#weighted_loss-bool-optional","title":"<code>weighted_loss (bool, optional)</code>","text":"<p>Use weighted loss during training. Default value is False.</p>"},{"location":"configurations/#mlflow_tracking_uri-str-optional","title":"<code>mlflow_tracking_uri (str, optional)</code>","text":"<p>MLFlow tracking URI. Default is \"./mlruns\".</p>"},{"location":"configurations/#from_scratch-bool-optional_1","title":"<code>from_scratch (bool, optional)</code>","text":"<p>Instantiate the model with base weights. Default is False.</p>"},{"location":"configurations/#use_default_model_labels-bool-optional_1","title":"<code>use_default_model_labels (bool, optional)</code>","text":"<p>By default, output the full set of default model labels rather than just the species in the labels file. Only applies if the provided labels are a subset of the default model labels. If set to False, will replace the model head for finetuning and output only the species in the provided labels file.</p>"},{"location":"configurations/#scheduler_config-schedulerconfig-or-str-optional","title":"<code>scheduler_config (SchedulerConfig or str, optional)</code>","text":"<p>Config for setting up the learning rate scheduler on the model. If \"default\", uses scheduler that was used for training. If None, will not use a scheduler. Defaults to \"default\".</p>"},{"location":"configurations/#split_proportions-dict-optional","title":"<code>split_proportions (Dict, optional)</code>","text":"<p>Split proportions (train, val, test). Default is {\"train\": 3, \"val\": 1, \"test\": 1}.</p>"},{"location":"configurations/#model_cache_dir-path-optional_3","title":"<code>model_cache_dir (Path, optional)</code>","text":"<p>Cache directory where downloaded model weights will be saved. Default is None.</p>"},{"location":"configurations/#cache_dir-path-optional_1","title":"<code>cache_dir (Path, optional)</code>","text":"<p>Path to the folder where clipped images will be saved. Applies only to training with images cropping (e.g. with bbox from coco format). Default is None.</p>"},{"location":"configurations/#weight_download_region-str-optional_1","title":"<code>weight_download_region (str, optional)</code>","text":"<p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p>"},{"location":"configurations/#species_in_label_order-list-optional","title":"<code>species_in_label_order (list, optional)</code>","text":"<p>Optional list of species in the desired order. Default is None.</p>"},{"location":"debugging/","title":"Debugging","text":"<p>Before kicking off a full run of video inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! Dry runs aren't yet implemented for <code>zamba image</code> commands.</p> CLI <pre><code>$ zamba predict --data-dir example_vids/ --dry-run\n\n$ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run\n</code></pre> Python <p>In Python, add <code>dry_run=True</code> to <code>PredictConfig</code> or <code>TrainConfig</code>: <pre><code>predict_config = PredictConfig(\n    data_dir=\"example_vids/\", dry_run=True\n)\n</code></pre></p>"},{"location":"debugging/#gpu-memory-errors","title":"GPU memory errors","text":"<p>The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes.</p>"},{"location":"debugging/#reducing-the-batch-size","title":"Reducing the batch size","text":"CLI <pre><code>zamba train --data-dir example_vids/ --labels example_labels.csv --batch-size 1\n</code></pre> Python <p>In Python, add <code>batch_size</code> to <code>PredictConfig</code> or <code>TrainConfig</code>: <pre><code>predict_config = PredictConfig(\n    data_dir=\"example_vids/\", batch_size=1\n)\n</code></pre></p>"},{"location":"debugging/#decreasing-video-size","title":"Decreasing video size","text":"<p>Resize video frames to be smaller before they are passed to the model. The default for all models is 240x426 pixels. <code>model_input_height</code> and <code>model_input_width</code> cannot be passed directly to the command line, so if you are using the CLI these must be specified in a YAML file.</p> <p>If you are using MegadetectorLite to select frames (which is the default for the official models we ship with), you can also decrease the size of the frame used at this stage by setting <code>frame_selection_height</code> and <code>frame_selection_width</code>.</p> YAML file <pre><code>video_loader_config:\n    frame_selection_height: 400  # if using megadetectorlite\n    frame_selection_width: 600  # if using megadetectorlite\n    model_input_height: 100\n    model_input_width: 100\n    total_frames: 16 # total_frames is always required\n</code></pre> Python <pre><code>video_loader_config = VideoLoaderConfig(\n    frame_selection_height=400, frame_selection_width=600,  # if using megadetectorlite\n    model_input_height=100, model_input_width=100,\n    total_frames=16,\n) # total_frames is always required\n</code></pre>"},{"location":"debugging/#reducing-num_workers","title":"Reducing <code>num_workers</code>","text":"<p>Reduce the number of workers (subprocesses) used for data loading. By default <code>num_workers</code> will be set to 3. The minimum value is 0, which means that the data will be loaded in the main process, and the maximum is one less than the number of CPUs in the system. We recommend trying 1 if 3 is too many.</p> CLI <pre><code>$ zamba predict --data-dir example_vids/ --num-workers 1\n\n$ zamba train --data-dir example_vids/ --labels example_labels.csv --num-workers 1\n</code></pre> Python <p>In Python, add <code>num_workers</code> to <code>PredictConfig</code> or <code>TrainConfig</code>: <pre><code>predict_config = PredictConfig(\n    data_dir=\"example_vids/\", num_workers=1\n)\n</code></pre></p>"},{"location":"debugging/#logging","title":"Logging","text":"<p>To check that videos are getting loaded and cached as expected, set your environment variabe <code>LOG_LEVEL</code> to <code>DEBUG</code>. The default log level is <code>INFO</code>. For example:</p> <pre><code>$ LOG_LEVEL=DEBUG zamba predict --data-dir example_vids/\n</code></pre>"},{"location":"extra-options/","title":"Guide to common optional parameters","text":"<p>There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see All Configuration Options.</p> <p>Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example:</p> <pre><code>$ zamba train --config path_to_your_config_file.yaml\n</code></pre> <p>For using a YAML file with the Python package and other details, see the YAML Configuration File page.</p>"},{"location":"extra-options/#downloading-model-weights","title":"Downloading model weights","text":"<p><code>zamba</code> needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region:</p> CLI <pre><code>zamba predict --data-dir example_vids/ --weight_download_region asia\n</code></pre> Python <p>In Python this can be specified in <code>PredictConfig</code> or <code>TrainConfig</code>: <pre><code>predict_config = PredictConfig(\n    data_dir=\"example_vids/\",\n    weight_download_region='asia',\n)\n</code></pre></p> <p>The options for <code>weight_download_region</code> are <code>us</code>, <code>eu</code>, and <code>asia</code>. Once a model's weights are downloaded, <code>zamba</code> will use the local version and will not need to perform this download again.</p>"},{"location":"extra-options/#video-size","title":"Video size","text":"<p>When <code>zamba</code> loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to superior accuracy in prediction, but will use more memory and take longer to train and/or predict. The default video loading configuration for all pretrained models resizes images to 240x426 pixels.</p> <p>Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. For example, to resize all images to 150x150 pixels instead of the default 240x426:</p> YAML file <pre><code>video_loader_config:\n    model_input_height: 150\n    model_input_width: 150\n    total_frames: 16 # total_frames must always be specified\n</code></pre> Python <p>In Python, video resizing can be specified when <code>VideoLoaderConfig</code> is instantiated:</p> <pre><code>from zamba.data.video import VideoLoaderConfig\nfrom zamba.models.config import PredictConfig\nfrom zamba.models.model_manager import predict_model\n\npredict_config = PredictConfig(data_dir=\"example_vids/\")\n\nvideo_loader_config = VideoLoaderConfig(\n    model_input_height=150, model_input_width=150, total_frames=16\n) # total_frames must always be specified\n\npredict_model(\n    predict_config=predict_config, video_loader_config=video_loader_config\n)\n</code></pre>"},{"location":"extra-options/#frame-selection","title":"Frame selection","text":"<p>Each video is simply a series of frames, or images. Most of the videos on which <code>zamba</code> was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames.</p> <p>All models only use a subset of the frames in a video, because using every frame would be far too computationally intensive, and many frames are not different enough from each other to look at independently. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments. A few common approaches are explained below.</p>"},{"location":"extra-options/#early-bias","title":"Early bias","text":"<p>Some camera traps begin recording a video when movement is detected. If this is the case, you may be more likely to see an animal towards when the video starts. Setting <code>early_bias</code> to True selects 16 frames towards the beginning of a video.</p> YAML File <pre><code>video_loader_config:\n    early_bias: True\n    # ... other parameters\n</code></pre> Python <p>In Python, <code>early_bias</code> is specified when <code>VideoLoaderConfig</code> is instantiated:</p> <pre><code>video_loader_config = VideoLoaderConfig(early_bias=True, ...)\n</code></pre> <p>This method was used by the winning solution of the Pri-matrix Factorization machine learning competition, which was the basis for <code>zamba</code> v1.</p> <p>This is a simple heuristic approach that is computationally cheap, and works decently for camera traps that are motion-triggered and short in total duration.</p>"},{"location":"extra-options/#evenly-distributed-frames","title":"Evenly distributed frames","text":"<p>A simple option is to sample frames that are evenly distributed throughout a video. For example, to select 32 evenly distributed frames:</p> YAML file <pre><code>video_loader_config:\n    total_frames: 32\n    evenly_sample_total_frames: True\n    ensure_total_frames: True\n    # ... other parameters\n</code></pre> Python <p>In Python, these arguments can be specified when <code>VideoLoaderConfig</code> is instantiated: <pre><code>video_loader_config = VideoLoaderConfig(\n    total_frames=32,\n    evenly_sample_total_frames=True,\n    ensure_total_frames=True,\n    ...\n)\n</code></pre></p>"},{"location":"extra-options/#megadetectorlite","title":"MegadetectorLite","text":"<p>You can use a pretrained object detection model called MegadetectorLite to select only the frames that are mostly likely to contain an animal. This is the default strategy for all pretrained models. The parameter <code>megadetector_lite_config</code> is used to specify any arguments that should be passed to the MegadetectorLite model. If <code>megadetector_lite_config</code> is None, the MegadetectorLite model will not be used.</p> <p>For example, to take the 16 frames with the highest probability of detection:</p> YAML file <pre><code>video_loader_config:\n    megadetector_lite_config:\n        n_frames: 16\n        fill_mode: \"score_sorted\"\n    # ... other parameters\n</code></pre> Python <p>In Python, these can be specified in the <code>megadetector_lite_config</code> argument passed to <code>VideoLoaderConfig</code>: <pre><code>video_loader_config = VideoLoaderConfig(\n    model_input_height=240,\n    model_input_width=426,\n    crop_bottom_pixels=50,\n    ensure_total_frames=True,\n    megadetector_lite_config={\n        \"confidence\": 0.25,\n        \"fill_mode\": \"score_sorted\",\n        \"n_frames\": 16,\n    },\n    total_frames=16,\n)\n\ntrain_config = TrainConfig(data_dir=\"example_vids/\", labels=\"example_labels.csv\",)\n\ntrain_model(video_loader_config=video_loader_config, train_config=train_config)\n</code></pre></p> <p>If you are using the MegadetectorLite for frame selection, there are two ways that you can specify frame resizing:</p> <ul> <li><code>frame_selection_width</code> and <code>frame_selection_height</code> resize images before they are input to the frame selection method (in this case, before being fed into MegadetectorLite). If both are <code>None</code>, the full size images will be used during frame selection. Using full size images for selection is recommended for better detection of smaller species, but will slow down training and inference.</li> <li><code>model_input_height</code> and <code>model_input_width</code> resize images after frame selection. These specify the image size that is passed to the actual model for classification.</li> </ul> <p>You can specify both of the above at once, just one, or neither. The example code feeds full-size images to MegadetectorLite, and then resizes images before running them through the neural network.</p> <p>To see all of the options that can be passed to the MegadetectorLite, see the <code>MegadetectorLiteYoloXConfig</code> class.</p>"},{"location":"extra-options/#speed-up-training","title":"Speed up training","text":"<p>Training will run faster if you increase <code>num_workers</code> and/or increase <code>batch_size</code>. <code>num_workers</code> is the number of subprocesses to use for data loading. The minimum is 0, meaning the data will be loaded in the main process, and the maximum is one less than the number of CPUs in your system. By default <code>num_workers</code> is set to 3 and <code>batch_size</code> is set to 2. Increasing either of these will use more GPU memory, and could raise an error if the memory required is more than your machine has available.</p> <p>You may need to try a few configuration of <code>num_workers</code>, <code>batch_size</code> and the image sizes above to settle on a configuration that works on your particular hardware.</p> <p>Both can be specified in either <code>predict_config</code> or <code>train_config</code>. For example, to increase <code>num_workers</code> to 5 and <code>batch_size</code> to 4 for inference:</p> YAML file <pre><code>predict_config:\n    data_dir: example_vids/\n    num_workers: 5\n    batch_size: 4\n    # ... other parameters\n</code></pre> Python <pre><code>predict_config = PredictConfig(\n    data_dir=\"example_vids/\",\n    num_workers=5,\n    batch_size=4,\n    # ... other parameters\n)\n</code></pre> <p>And that's just the tip of the iceberg! See All Configuration Options page for more possibilities.</p>"},{"location":"images-predict-tutorial/","title":"User tutorial: Classifying unlabeled images","text":"<p>This section walks through how to classify images using <code>zamba</code>. If you are new to <code>zamba</code> and just want to classify some images as soon as possible, see the Quickstart guide.</p> <p>This tutorial goes over the steps for using <code>zamba</code> if:</p> <ul> <li>You already have <code>zamba</code> installed (for details see the Installation page)</li> <li>You have unlabeled images that you want to generate labels for</li> <li>The possible class species labels for your images are included in the list of possible zamba labels. If your species are not included in this list, you can retrain a model using your own labeled data and then run inference.</li> </ul>"},{"location":"images-predict-tutorial/#basic-usage-command-line-interface","title":"Basic usage: command line interface","text":"<p>Say that we want to classify the images in a folder called <code>example_images</code> as simply as possible using all of the default settings.</p> <p>Minimum example for prediction in the command line:</p> <pre><code>$ zamba image predict --data-dir example_images/\n</code></pre>"},{"location":"images-predict-tutorial/#required-arguments","title":"Required arguments","text":"<p>To run <code>zamba images predict</code> in the command line, you must specify <code>--data-dir</code> and/or <code>--filepaths</code>.</p> <ul> <li><code>--data-dir PATH</code>: Path to the folder containing your images. If you don't also provide <code>filepaths</code>, Zamba will recursively search this folder for images.</li> <li><code>--filepaths PATH</code>: Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for <code>filepath</code>. Filepaths can be absolute on your system or relative to the data directory that your provide in <code>--data-dir</code>.</li> </ul> <p>All other flags are optional. To choose the model you want to use for prediction, either <code>--model</code> or <code>--checkpoint</code> must be specified. Use <code>--model</code> to specify one of the pretrained models that ship with <code>zamba</code>. Use <code>--checkpoint</code> to run inference with a locally saved model. <code>--model</code> defaults to <code>lila.science</code>.</p>"},{"location":"images-predict-tutorial/#basic-usage-python-package","title":"Basic usage: Python package","text":"<p>We also support using Zamba as a Python package. Say that we want to classify the images in a folder called <code>example_images</code> as simply as possible using all of the default settings.</p> <p>Minimum example for prediction using the Python package:</p> <pre><code>from zamba.images.manager import predict\nfrom zamba.images.config import ImageClassificationPredictConfig\n\npredict_config = ImageClassificationPredictConfig(data_dir=\"example_images/\")\npredict(config=predict_config)\n</code></pre> <p>The only argument that can be passed to <code>predict</code> is <code>config</code>, so the only step is to instantiate an <code>ImageClassificationPredictConfig</code> and pass it to <code>predict</code>.</p>"},{"location":"images-predict-tutorial/#required-arguments_1","title":"Required arguments","text":"<p>To run <code>predict</code> in Python, you must specify either <code>data_dir</code> or <code>filepaths</code> when <code>PredictConfig</code> is instantiated.</p> <ul> <li> <p><code>data_dir (DirectoryPath)</code>: Path to the folder containing your images. If you don't also provide <code>filepaths</code>, Zamba will recursively search this folder for images.</p> </li> <li> <p><code>filepaths (FilePath)</code>: Path to a CSV file with a column for the filepath to each image you want to classify. The CSV must have a column for <code>filepath</code>. Filepaths can be absolute or relative to the data directory provided as <code>data_dir</code>.</p> </li> </ul> <p>For detailed explanations of all possible configuration arguments, see All Optional Arguments.</p>"},{"location":"images-predict-tutorial/#default-behavior","title":"Default behavior","text":"<p>By default, the <code>lila.science</code> model will be used. <code>zamba</code> will output a <code>.csv</code> file with a row for each bounding box identified by megadetector (there may be multiple bounding boxes per image) and columns for each class (ie. species / species group). A cell in the CSV (i,j) can be interpreted as the predicted likelihood that the animal present in bounding box i is of species group j. Usually, most users want to just look at the top species prediction for each bounding box.</p> <p>By default, predictions will be saved to a file called <code>zamba_predictions.csv</code> in your working directory. You can save predictions to a custom directory using the <code>--save-dir</code> argument.</p> <pre><code>$ cat zamba_predictions.csv\nfilepath,detection_category,detection_conf,x1,y1,x2,y2,species_acinonyx_jubatus,species_aepyceros_melampus,species_alcelaphus_buselaphus...\n1.jpg,1,0.85,2015,1235,2448,1544,4.924246e-06,0.0001539439,9.6043495e-06...\n2.jpg,1,0.921,527,1246,1805,1501,5.061601e-05,3.6830465e-05,2.4510617e-05...\n3.jpg,1,0.9,1422,528,1629,816,1.1791806e-06,4.080566e-06,3.4533906e-07...\n</code></pre> <p>The <code>detection_category</code> and <code>detection_conf</code> come from MegaDetector which we use to find bounding boxes around individual animals in images. A <code>detection_category</code> of <code>\"1\"</code> indicates the presence of an animal, and <code>detection_conf</code> is the confidence the animal classifier had in the presence of an animal. The columns <code>x1</code>, <code>y1</code>, <code>x2</code>, <code>y2</code> indicate the coordinates of the top-left and bottom-right corners of the bounding box relative to the top-left corner of the image. The remaining columns are the scores assigned to each species for the individual animal in the given bounding box.</p>"},{"location":"images-predict-tutorial/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"images-predict-tutorial/#1-specify-the-path-to-your-images","title":"1. Specify the path to your images","text":"<p>Save all of your images within one parent folder.</p> <ul> <li>Images can be in nested subdirectories within the folder.</li> <li>Your images should be in be saved in formats that are suppored by Python's <code>pillow</code> library. Any images that fail a set of validation checks will be skipped during inference or training. By default, <code>zamba</code> will look for files with the following suffixes: <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code> and <code>.webp</code>. To use other image formats that are supported by pillow, set your <code>IMAGE_SUFFIXES</code> environment variable.</li> </ul> <p>Add the path to your image folder to the command-line command. For example, if your images are in a folder called <code>example_images</code>:</p> CLI <pre><code>$ zamba image predict --data-dir example_images/\n</code></pre> Python <pre><code>from zamba.images.manager import predict\nfrom zamba.images.config import ImageClassificationPredictConfig\n\npredict_config = ImageClassificationPredictConfig(data_dir=\"example_images/\")\npredict(config=predict_config)\n</code></pre>"},{"location":"images-predict-tutorial/#2-choose-a-model-for-prediction","title":"2. Choose a model for prediction","text":"<p>Right now, Zamba supports only a single model out-of-the-box for images: <code>lila.science</code>. While there's only a single model, this model has been trained to detect hundreds of species, so it's a great first pass. The <code>lila.science</code> model will be used if no model is specified.</p> <p>If you've fine-tuned a model, you can select that model instead of a built-in model by using the <code>--checkpoint</code> argument. For example:</p> CLI <pre><code>$ zamba predict --data-dir example_images/ --checkpoint zamba-image-classification-dummy_modelepoch=00-val_loss=48.372.ckpt\n</code></pre> Python <pre><code>from zamba.images.manager import predict\nfrom zamba.images.config import ImageClassificationPredictConfig\n\npredict_config = ImageClassificationPredictConfig(\n    data_dir=\"example_images/\",\n    checkpoint=\"zamba-image-classification-dummy_modelepoch=00-val_loss=48.372.ckpt\")\npredict(config=predict_config)\n</code></pre>"},{"location":"images-predict-tutorial/#3-choose-the-output-format","title":"3. Choose the output format","text":"<p>There are two options for how to format predictions:</p> <ol> <li>CSV (default): Return predictions with a row for each bounding box-filename combination and a column for each class label, with probabilities between 0 and 1. Bounding boxes are automatically created for each image, and each image can contain 0 or more bounding boxes (for example, if there are many rabbits in a single image). Cell <code>(i,j)</code> is the probability that species <code>j</code> is present in bounding box <code>i</code>.</li> <li>MegaDetector: Returns much the same information as above, but in a JSON format similar to the COCO image format that's been augmented to be more useful for camera trap data.</li> </ol> <p>Say we want to generate predictions for images in <code>example_images</code> in MegaDetector format:</p> CLI <pre><code>$ zamba image predict --data-dir example_image/ --results-file-format megadetector\n$ cat zamba_predictions.json\n{\n    \"info\": {},\n    \"detection_categories\": { \"1\": \"animal\", \"2\": \"person\", \"3\": \"vehicle\" },\n    \"classification_categories\": {\n        \"0\": \"species_acinonyx_jubatus\",\n        \"1\": \"species_aepyceros_melampus\",\n        \"2\": \"species_alcelaphus_buselaphus\",\n...\n},\n\"images\": [ {\n    \"file\": \"1.jpg\",\n    \"detections\": [\n        { \"category\": \"1\", \"conf\": 0.85, \"bbox\": [ 0.7, 0.2, 0.12, 0.4 ],\n            \"classifications\": [ [ 88, 0.9355112910270691 ],\n...\n</code></pre> Python <pre><code>from zamba.images.manager import predict\nfrom zamba.images.config import ImageClassificationPredictConfig, ResultsFormat\n\npredict_config = ImageClassificationPredictConfig(\n    data_dir=\"example_images/\",\n    results_file_format=ResultsFormat.MEGADETECTOR\n</code></pre>"},{"location":"images-predict-tutorial/#4-specify-any-additional-parameters","title":"4. Specify any additional parameters","text":"<p>And there's so much more! You can also do things like specify your region for faster model download (<code>--weight-download-region</code>), change the detection threshold above which we'll consider a bounding box contains an animal (<code>--detections-threshold</code>), or specify a different folder where your predictions should be saved (<code>--save-dir</code>). To read about a few common considerations, see the Guide to Common Optional Parameters page.</p>"},{"location":"install/","title":"Installing <code>zamba</code>","text":"<p>Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations.</p>"},{"location":"install/#to-install-zamba","title":"To install <code>zamba</code>","text":""},{"location":"install/#1-install-prerequisites","title":"1. Install prerequisites","text":"<p>Prerequisites:</p> <ul> <li>Python &gt;= 3.11</li> <li>FFmpeg</li> </ul>"},{"location":"install/#python-311","title":"Python &gt;= 3.11","text":"<p>We recommend Python installation using Anaconda for all platforms. For more information about how to install Anaconda, here are some useful YouTube videos of installation:</p> <ul> <li>Anaconda download link</li> <li>macOS installation video</li> </ul>"},{"location":"install/#ffmpeg-version-4","title":"FFmpeg version 4","text":"<p>FFmpeg is an open source library for loading videos of different codecs. Using FFmpeg means that <code>zamba</code> can be flexible in terms of the video formats we support. FFmpeg can be installed on all different platforms, but requires some additional configuration depending on the platform. Here are some videos and instructions walking through FFmpeg installation:</p> <ul> <li>FFmpeg download link</li> <li>Install on Ubuntu or Linux.<ul> <li>In the command line, enter <code>sudo apt update</code> and then <code>sudo apt install ffmpeg</code>.</li> </ul> </li> <li>MacOS install video<ul> <li>First, install Homebrew. Then run <code>brew install ffmpeg@4</code></li> <li>Follow the brew instructions to add FFmpeg to your path.</li> </ul> </li> </ul> <p>To check that <code>FFmpeg</code> is installed, run <code>ffmpeg</code>:</p> <pre><code>$ ffmpeg\n\nffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n  built with Apple clang version 12.0.0 (clang-1200.0.32.29)\n...\n</code></pre> <p>To check your installed version, run <code>ffmpeg -version</code>.</p>"},{"location":"install/#2-install-zamba","title":"2. Install <code>zamba</code>","text":"<p>On macOS, run these commands in the terminal (\u2318+space, \"Terminal\"). On Windows, run them in a command prompt, or if you installed Anaconda an anaconda prompt (Start &gt; Anaconda3 &gt; Anaconda Prompt).</p> <p>To install zamba: <pre><code>$ pip install https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz\n</code></pre></p> <p>To check what version of zamba you have installed: <pre><code>$ pip show zamba\n</code></pre></p> <p>To update zamba to the most recent version if needed: <pre><code>$ pip install -U https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz\n</code></pre></p>"},{"location":"install/#operating-systems-that-have-been-tested","title":"Operating systems that have been tested","text":""},{"location":"install/#macos","title":"macOS","text":"<p><code>zamba</code> has been tested on macOS High Sierra.</p>"},{"location":"install/#linux","title":"Linux","text":"<p><code>zamba</code> has been tested on Ubuntu regularly since 16.04. Tests run every week against the <code>ubuntu-latest</code> Github runner environment, so the version that Github uses is most likely to work. As of October 2023, that is 22.04.</p> <p>Note, for Linux, you may need to install additional system packages to get <code>zamba</code> to work. For example, on Ubuntu, you may need to install <code>build-essentials</code> to have compilers.</p> <p>FFMpeg 4, build-essentials, and some other packages that include more codecs to support additional videos and some other utilities can be installed with:</p> <pre><code>apt-get update &amp;&amp; \\\n    apt-get install -y software-properties-common &amp;&amp; \\\n    add-apt-repository -y ppa:savoury1/ffmpeg4 &amp;&amp; \\\n    apt-get update &amp;&amp; \\\n    apt-get -y install \\\n    build-essential \\\n    ffmpeg \\\n    git \\\n    libavcodec-dev \\\n    libavdevice-dev \\\n    libavfilter-dev \\\n    libavformat-dev \\\n    libavutil-dev \\\n    libsm6 \\\n    libswresample-dev \\\n    libswscale-dev \\\n    libxext6 \\\n    pkg-config \\\n    wget \\\n    x264 \\\n    x265\n</code></pre>"},{"location":"install/#windows","title":"Windows","text":"<p>Note: <code>zamba</code> does not currently work on Windows because one of our dependencies fails to build.</p> <p>You can try using Docker or WSL to run <code>zamba</code> on a Linux OS like Ubuntu on top of your Windows machine.</p>"},{"location":"install/#using-gpus","title":"Using GPU(s)","text":"<p><code>zamba</code> is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s).</p> <p>To use a GPU, you must be using an NVIDIA GPU, have installed and configured CUDA, and have installed and configured CuDNN per their specifications.</p> <p>If you are using <code>conda</code>, these dependencies can be installed through the <code>cudatoolkit</code> package. If using a GPU, you will also want to make sure that you install a compatible version of PyTorch with the version of CUDA you use. See the PyTorch installation docs for the easiest way to install the right version on your system.</p>"},{"location":"predict-tutorial/","title":"User tutorial: Classifying unlabeled videos","text":"<p>This section walks through how to classify videos using <code>zamba</code>. If you are new to <code>zamba</code> and just want to classify some videos as soon as possible, see the Quickstart guide.</p> <p>This tutorial goes over the steps for using <code>zamba</code> if:</p> <ul> <li>You already have <code>zamba</code> installed (for details see the Installation page)</li> <li>You have unlabeled videos that you want to generate labels for</li> <li>The possible class species labels for your videos are included in the list of possible zamba labels. If your species are not included in this list, you can retrain a model using your own labeled data and then run inference.</li> </ul>"},{"location":"predict-tutorial/#basic-usage-command-line-interface","title":"Basic usage: command line interface","text":"<p>Say that we want to classify the videos in a folder called <code>example_vids</code> as simply as possible using all of the default settings.</p> <p>Minimum example for prediction in the command line:</p> <pre><code>$ zamba predict --data-dir example_vids/\n</code></pre>"},{"location":"predict-tutorial/#required-arguments","title":"Required arguments","text":"<p>To run <code>zamba predict</code> in the command line, you must specify <code>--data-dir</code> and/or <code>--filepaths</code>.</p> <ul> <li><code>--data-dir PATH</code>: Path to the folder containing your videos. If you don't also provide <code>filepaths</code>, Zamba will recursively search this folder for videos.</li> <li><code>--filepaths PATH</code>: Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for <code>filepath</code>. Filepaths can be absolute on your system or relative to the data directory that your provide in <code>--data-dir</code>.</li> </ul> <p>All other flags are optional. To choose the model you want to use for prediction, either <code>--model</code> or <code>--checkpoint</code> must be specified. Use <code>--model</code> to specify one of the pretrained models that ship with <code>zamba</code>. Use <code>--checkpoint</code> to run inference with a locally saved model. <code>--model</code> defaults to <code>time_distributed</code>.</p>"},{"location":"predict-tutorial/#basic-usage-python-package","title":"Basic usage: Python package","text":"<p>Say that we want to classify the videos in a folder called <code>example_vids</code> as simply as possible using all of the default settings.</p> <p>Minimum example for prediction using the Python package:</p> <pre><code>from zamba.models.model_manager import predict_model\nfrom zamba.models.config import PredictConfig\n\npredict_config = PredictConfig(data_dir=\"example_vids/\")\npredict_model(predict_config=predict_config)\n</code></pre> <p>The only two arguments that can be passed to <code>predict_model</code> are <code>predict_config</code> and (optionally) <code>video_loader_config</code>. The first step is to instantiate <code>PredictConfig</code>. Optionally, you can also specify video loading arguments by instantiating and passing in <code>VideoLoaderConfig</code>.</p>"},{"location":"predict-tutorial/#required-arguments_1","title":"Required arguments","text":"<p>To run <code>predict_model</code> in Python, you must specify either <code>data_dir</code> or <code>filepaths</code> when <code>PredictConfig</code> is instantiated.</p> <ul> <li> <p><code>data_dir (DirectoryPath)</code>: Path to the folder containing your videos. If you don't also provide <code>filepaths</code>, Zamba will recursively search this folder for videos.</p> </li> <li> <p><code>filepaths (FilePath)</code>: Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for <code>filepath</code>. Filepaths can be absolute or relative to the data directory provided as <code>data_dir</code>.</p> </li> </ul> <p>For detailed explanations of all possible configuration arguments, see All Optional Arguments.</p>"},{"location":"predict-tutorial/#default-behavior","title":"Default behavior","text":"<p>By default, the <code>time_distributed</code> model will be used. <code>zamba</code> will output a <code>.csv</code> file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i.</p> <p>By default, predictions will be saved to a file called <code>zamba_predictions.csv</code> in your working directory. You can save predictions to a custom directory using the <code>--save-dir</code> argument.</p> <pre><code>$ cat zamba_predictions.csv\nfilepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal\neleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\nleopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\nblank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\nchimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0\n</code></pre> <p>The full prediction and video loading configuration for the process will also be saved out, in the same folder as the predictions under <code>predict_configuration.yaml</code>. To run the exact same inference process a second time, you can pass this YAML file to <code>zamba predict</code> per the Using YAML Configuration Files page: <pre><code>$ zamba predict --config predict_configuration.yaml\n</code></pre></p>"},{"location":"predict-tutorial/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"predict-tutorial/#1-specify-the-path-to-your-videos","title":"1. Specify the path to your videos","text":"<p>Save all of your videos within one parent folder.</p> <ul> <li>Videos can be in nested subdirectories within the folder.</li> <li>Your videos should be in be saved in formats that are suppored by FFmpeg, which are listed here. Any videos that fail a set of FFmpeg checks will be skipped during inference or training. By default, <code>zamba</code> will look for files with the following suffixes: <code>.avi</code>, <code>.mp4</code>, <code>.asf</code>. To use other video suffixes that are supported by FFmpeg, set your <code>VIDEO_SUFFIXES</code> environment variable.</li> </ul> <p>Add the path to your video folder. For example, if your videos are in a folder called <code>example_vids</code>:</p> CLI <pre><code>$ zamba predict --data-dir example_vids/\n</code></pre> Python <pre><code>predict_config = PredictConfig(data_dir='example_vids/')\npredict_model(predict_config=predict_config)\n</code></pre>"},{"location":"predict-tutorial/#2-choose-a-model-for-prediction","title":"2. Choose a model for prediction","text":"<p>If your camera videos contain species common to Central or West Africa, use either the <code>time_distributed</code> model or <code>slowfast</code> model model. <code>slowfast</code> is better for blank and small species detection. <code>time_distributed</code> performs better if you have many different species of interest, or are focused on duikers, chimpanzees, and/or gorillas.</p> <p>If your videos contain species common to Europe, use the <code>european</code> model.</p> <p>Add the model name to your command. The <code>time_distributed</code> model will be used if no model is specified. For example, if you want to use the <code>slowfast</code> model to classify the videos in <code>example_vids</code>:</p> CLI <pre><code>$ zamba predict --data-dir example_vids/ --model slowfast\n</code></pre> Python <pre><code>predict_config = PredictConfig(\n    data_dir='example_vids/', model_name='slowfast'\n)\npredict_model(predict_config=predict_config)\n</code></pre>"},{"location":"predict-tutorial/#3-choose-the-output-format","title":"3. Choose the output format","text":"<p>There are three options for how to format predictions, listed from most information to least:</p> <ol> <li>Store all probabilities (default): Return predictions with a row for each filename and a column for each class label, with probabilities between 0 and 1. Cell <code>(i,j)</code> is the probability that animal <code>j</code> is present in video <code>i</code>.</li> <li>Presence/absence: Return predictions with a row for each filename and a column for each class label, with cells indicating either presence or absense based on a user-specified probability threshold. Cell <code>(i, j)</code> indicates whether animal <code>j</code> is present (<code>1</code>) or not present (<code>0</code>) in video <code>i</code>. The probability threshold cutoff is specified with <code>--proba-threshold</code> in the CLI.</li> <li>Most likely class: Return predictions with a row for each filename and one column for the most likely class in each video. The most likely class can also be blank. To get the most likely class, add <code>--output-class-names</code> to your command. In Python, it can be specified by adding <code>output_class_names=True</code> when <code>PredictConfig</code> is instantiated. This is not recommended if you'd like to detect more than one species in each video.</li> </ol> <p>Say we want to generate predictions for the videos in <code>example_vids</code> indicating which animals are present in each video based on a probability threshold of 50%:</p> CLI <pre><code>$ zamba predict --data-dir example_vids/ --proba-threshold 0.5\n$ cat zamba_predictions.csv\nfilepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal\neleph.MP4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\nleopard.MP4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0\nblank.MP4,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\nchimp.MP4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n</code></pre> Python <pre><code>predict_config = PredictConfig(\n    data_dir=\"example_vids/\", proba_threshold=0.5\n)\npredict_model(predict_config=predict_config)\npredictions = pd.read_csv(\"zamba_predictions.csv\")\npredictions\n</code></pre> filepath aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal blank.MP4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 chimp.MP4 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 eleph.MP4 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 leopard.MP4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0"},{"location":"predict-tutorial/#4-specify-any-additional-parameters","title":"4. Specify any additional parameters","text":"<p>And there's so much more! You can also do things like specify your region for faster model download (<code>--weight-download-region</code>), use a saved model checkpoint (<code>--checkpoint</code>), or specify a different folder where your predictions should be saved (<code>--save-dir</code>). To read about a few common considerations, see the Guide to Common Optional Parameters page.</p>"},{"location":"predict-tutorial/#5-test-your-configuration-with-a-dry-run","title":"5. Test your configuration with a dry run","text":"<p>Before kicking off a full run of inference, we recommend testing your code with a \"dry run\". This will run one batch of inference to quickly detect any bugs. See the Debugging page for details.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This section assumes you have successfully installed <code>zamba</code> and are ready to train a model or identify species in your images and videos!</p> <p><code>zamba</code> can be used \"out of the box\" to generate predictions or train a model using your own images and videos. By default, <code>zamba</code> expects video, but using images is just as easy. To perform inference, you simply need to run <code>zamba predict</code> or <code>zamba image predict</code> followed by a set of arguments that let zamba know where your media is located, which model you want to use, and where to save your output. To train a model, you can similarly run <code>zamba train</code> or <code>zamba image train</code> and specify your labels. The following sections provide details about these separate modules.</p> <p>There are two ways to interact with the <code>zamba</code> package:</p> <ol> <li>Use <code>zamba</code> as a command line interface tool. This page provides an overview of how to use the CLI.</li> <li>Import <code>zamba</code> in Python and use it as a Python package.</li> </ol> <p>This guide uses the CLI, but you can see the prediction tutorial or the training tutorial, which have both the CLI and Python approaches documented.</p> <p>Installation is the same for both the command line interface tool and the Python package.</p> <p>All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start &gt; Anaconda3 &gt; Anaconda Prompt).</p>"},{"location":"quickstart/#how-do-i-organize-my-images-or-videos-for-zamba","title":"How do I organize my images or videos for <code>zamba</code>?","text":"<p>You can specify the path to a directory of images or videos or specify a list of filepaths in a <code>.csv</code> file. <code>zamba</code> supports the same image formats as <code>pillow</code> and the same video formats as FFmpeg, which are listed here. Any images or videos that fail a set of validation checks will be skipped during inference or training.</p> <p>For example, say we have a directory of videos called <code>example_vids</code> that we want to generate predictions for using <code>zamba</code>. Let's list the videos:</p> <pre><code>$ ls example_vids/\nblank.mp4\nchimp.mp4\neleph.mp4\nleopard.mp4\n</code></pre> <p>Here are some screenshots from those videos:</p> blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 <p>In this example, the videos have meaningful names so that we can easily compare the predictions made by <code>zamba</code>. In practice, your videos will probably be named something much less useful!</p>"},{"location":"quickstart/#generating-predictions-for-videos","title":"Generating predictions for videos","text":"<p>To generate and save predictions for your videos using the default settings, run:</p> <pre><code>$ zamba predict --data-dir example_vids/\n</code></pre> <p><code>zamba</code> will output a <code>.csv</code> file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell <code>(i,j)</code> is the probability that animal <code>j</code> is present in video <code>i</code>.  Comprehensive predictions are helpful when a single video contains multiple species.</p> <p>Predictions will be saved to <code>zamba_predictions.csv</code> in the current working directory by default. You can save out predictions to a different folder using the <code>--save-dir</code> argument.</p> <p>Adding the argument <code>--output-class-names</code> will simplify the predictions to return only the most likely animal in each video:</p> <pre><code>$ zamba predict --data-dir example_vids/ --output-class-names\n$ cat zamba_predictions.csv\nblank.mp4,blank\nchimp.mp4,chimpanzee_bonobo\neleph.mp4,elephant\nleopard.mp4,leopard\n</code></pre> <p>There are pretrained models that ship with <code>zamba</code>: <code>blank_nonblank</code>, <code>time_distributed</code>, <code>slowfast</code>, and <code>european</code>. Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default <code>zamba</code> will use the <code>time_distributed</code> model. Add the <code>--model</code> argument to specify one of other options:</p> <pre><code>$ zamba predict --data-dir example_vids/ --model slowfast\n</code></pre>"},{"location":"quickstart/#generating-predictions-for-images","title":"Generating predictions for images","text":"<p>To generate and save predictions for your images using the default settings, run:</p> <pre><code>$ zamba image predict --data-dir example_images/\n</code></pre> <p>Predictions will be saved to <code>zamba_predictions.csv</code> in the current working directory by default. You can save out predictions to a different folder using the <code>--save-dir</code> argument. <code>zamba</code> will output a <code>.csv</code> file with rows labeled by each detected bounding box in each filename and columns for each class (ie. species):</p> <pre><code>$ cat zamba_predictions.csv\nfilepath,detection_category,detection_conf,x1,y1,x2,y2,species_acinonyx_jubatus,species_aepyceros_melampus,species_alcelaphus_buselaphus...\n1.jpg,1,0.85,2015,1235,2448,1544,4.924246e-06,0.0001539439,9.6043495e-06...\n2.jpg,1,0.921,527,1246,1805,1501,5.061601e-05,3.6830465e-05,2.4510617e-05...\n3.jpg,1,0.9,1422,528,1629,816,1.1791806e-06,4.080566e-06,3.4533906e-07...\n</code></pre> <p>The <code>detection_category</code> and <code>detection_conf</code> come from MegaDetector which we use to find bounding boxes around individual animals in images. A <code>detection_category</code> of <code>\"1\"</code> indicates the presence of an animal, and <code>detection_conf</code> is the confidence the animal classifier had in the presence of an animal. The columns <code>x1</code>, <code>y1</code>, <code>x2</code>, <code>y2</code> indicate the coordinates of the top-left and bottom-right corners of the bounding box relative to the top-left corner of the image. The remaining columns are the scores assigned to each species for the individual animal in the given bounding box. These scores will sum to one.</p>"},{"location":"quickstart/#training-a-model","title":"Training a model","text":"<p>You can continue training one of the models that ships with <code>zamba</code> by either:</p> <ul> <li>Finetuning with additional labeled images or videos where the species are included in the list of <code>zamba</code> class labels</li> <li>Finetuning with labeled images or videos that include new species</li> </ul> <p>In either case, the commands for training are the same. Say that we have labels for the videos in the <code>example_vids</code> folder saved in <code>example_labels.csv</code>. To train a model, run:</p> <pre><code>$ zamba train --data-dir example_vids/ --labels example_labels.csv\n</code></pre> <p>The labels file must have columns for both filepath and label. The filepath column should contain either absolute paths or paths relative to the <code>data-dir</code>. Optionally, there can also be columns for <code>split</code> (<code>train</code>, <code>val</code>, or <code>holdout</code>) and <code>site</code>. Let's print the example labels:</p> <pre><code>$ cat example_labels.csv\nfilepath,label\nblank.MP4,blank\nchimp.MP4,chimpanzee_bonobo\neleph.MP4,elephant\nleopard.MP4,leopard\n</code></pre> <p>By default, the trained model and additional training output will be saved to a <code>version_n</code> folder in the current working directory. For example,</p> <pre><code>$ zamba train --data-dir example_vids/ --labels example_labels.csv\n$ ls version_0/\nhparams.yaml\ntime_distributed.ckpt\ntrain_configuration.yamml\nval_metrics.json\n...\n</code></pre> <p>For images, all the above is true except the command is <code>zamba image train</code>.</p>"},{"location":"quickstart/#downloading-model-weights","title":"Downloading model weights","text":"<p><code>zamba</code> needs to download the \"weights\" files for the models it uses to make predictions. On first run, it will download ~200-2000 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, <code>zamba</code> will use the local version and will not need to perform this download again. If you are not in the United States, we recommend running the above command with the additional flag either <code>--weight_download_region eu</code> or <code>--weight_download_region asia</code> depending on your location. The closer you are to the server, the faster the downloads will be.</p> <p></p>"},{"location":"quickstart/#getting-help","title":"Getting help","text":"<p>Once zamba is installed, you can see more details of each function with <code>--help</code>. For example, you can run <code>zamba predict --help</code>:</p> <pre><code>Usage: zamba predict [OPTIONS]\n\n  Identify species in a video.\n\n  This is a command line interface for prediction on camera trap footage.\n  Given a path to camera trap footage, the predict function use a deep\n  learning model to predict the presence or absense of a variety of species of\n  common interest to wildlife researchers working with camera trap data.\n\n  If an argument is specified in both the command line and in a yaml file, the\n  command line input will take precedence.\n\nOptions:\n  --data-dir PATH                 Path to folder containing videos.\n  --filepaths PATH                Path to csv containing `filepath` column\n                                  with videos.\n  --model [time_distributed|slowfast|european]\n                                  Model to use for inference. Model will be\n                                  superseded by checkpoint if provided.\n                                  [default: time_distributed]\n  --checkpoint PATH               Model checkpoint path to use for inference.\n                                  If provided, model is not required.\n  --gpus INTEGER                  Number of GPUs to use for inference. If not\n                                  specifiied, will use all GPUs found on\n                                  machine.\n  --batch-size INTEGER            Batch size to use for training.\n  --save / --no-save              Whether to save out predictions. If you want\n                                  to specify the output directory, use\n                                  save_dir instead.\n  --save-dir PATH                 An optional directory in which to save the\n                                  model predictions and configuration yaml.\n                                  Defaults to the current working directory if\n                                  save is True.\n  --dry-run / --no-dry-run        Runs one batch of inference to check for\n                                  bugs.\n  --config PATH                   Specify options using yaml configuration\n                                  file instead of through command line\n                                  options.\n  --proba-threshold FLOAT         Probability threshold for classification\n                                  between 0 and 1. If specified binary\n                                  predictions are returned with 1 being\n                                  greater than the threshold, 0 being less\n                                  than or equal to. If not specified,\n                                  probabilities between 0 and 1 are returned.\n  --output-class-names / --no-output-class-names\n                                  If True, we just return a video and the name\n                                  of the most likely class. If False, we\n                                  return a probability or indicator (depending\n                                  on --proba_threshold) for every possible\n                                  class.\n  --num-workers INTEGER           Number of subprocesses to use for data\n                                  loading.\n  --weight-download-region [us|eu|asia]\n                                  Server region for downloading weights.\n  --skip-load-validation / --no-skip-load-validation\n                                  Skip check that verifies all videos can be\n                                  loaded prior to inference. Only use if\n                                  you're very confident all your videos can be\n                                  loaded.\n  -o, --overwrite                 Overwrite outputs in the save directory if\n                                  they exist.\n  -y, --yes                       Skip confirmation of configuration and\n                                  proceed right to prediction.\n  --help                          Show this message and exit.\n</code></pre> <p>Or if you are training a model, you can run <code>zamba train --help</code>:</p> <pre><code>$ zamba train --help\n\nUsage: zamba train [OPTIONS]\n\n  Train a model on your labeled data.\n\n  If an argument is specified in both the command line and in a yaml file, the\n  command line input will take precedence.\n\nOptions:\n  --data-dir PATH                 Path to folder containing videos.\n  --labels PATH                   Path to csv containing video labels.\n  --model [time_distributed|slowfast|european]\n                                  Model to train. Model will be superseded by\n                                  checkpoint if provided.  [default:\n                                  time_distributed]\n  --checkpoint PATH               Model checkpoint path to use for training.\n                                  If provided, model is not required.\n  --config PATH                   Specify options using yaml configuration\n                                  file instead of through command line\n                                  options.\n  --batch-size INTEGER            Batch size to use for training.\n  --gpus INTEGER                  Number of GPUs to use for training. If not\n                                  specifiied, will use all GPUs found on\n                                  machine.\n  --dry-run / --no-dry-run        Runs one batch of train and validation to\n                                  check for bugs.\n  --save-dir PATH                 An optional directory in which to save the\n                                  model checkpoint and configuration file. If\n                                  not specified, will save to a `version_n`\n                                  folder in your working directory.\n  --num-workers INTEGER           Number of subprocesses to use for data\n                                  loading.\n  --weight-download-region [us|eu|asia]\n                                  Server region for downloading weights.\n  --skip-load-validation / --no-skip-load-validation\n                                  Skip check that verifies all videos can be\n                                  loaded prior to training. Only use if you're\n                                  very confident all your videos can be\n                                  loaded.\n  -y, --yes                       Skip confirmation of configuration and\n                                  proceed right to training.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"train-tutorial/","title":"User tutorial: Training a model on labeled videos","text":"<p>This section walks through how to train a model using <code>zamba</code>. If you are new to <code>zamba</code> and just want to classify some videos as soon as possible, see the Quickstart guide.</p> <p>This tutorial goes over the steps for using <code>zamba</code> if:</p> <ul> <li>You already have <code>zamba</code> installed (for details see the Installation page)</li> <li>You have labeled videos that you want to use to train or finetune a model</li> </ul> <p><code>zamba</code> can run two types of model training:</p> <ul> <li>Finetuning a model with labels that are a subset of the possible zamba labels</li> <li>Finetuning a model to predict an entirely new set of labels</li> </ul> <p>The process is the same for both cases.</p>"},{"location":"train-tutorial/#basic-usage-command-line-interface","title":"Basic usage: command line interface","text":"<p>By default, the <code>time_distributed</code> species classification model is used. Say that we want to finetune that model based on the videos in <code>example_vids</code> and the labels in <code>example_labels.csv</code>.</p> <pre><code>$ cat example_labels.csv\n\nfilepath,label\nblank.MP4,blank\nchimp.MP4,chimpanzee_bonobo\neleph.MP4,elephant\nleopard.MP4,leopard\n</code></pre> <p>Training at the command line would look like:</p> <pre><code>$ zamba train --data-dir example_vids/ --labels example_labels.csv\n</code></pre>"},{"location":"train-tutorial/#required-arguments","title":"Required arguments","text":"<p>To run <code>zamba train</code> in the command line, you must specify <code>labels</code>.</p> <ul> <li><code>--labels PATH</code>: Path to a CSV containing the video labels to use as ground truth during training. There must be columns for both <code>filepath</code> and <code>label</code>. Optionally, there can also be columns for <code>split</code> (which can have one of the three values for each row: <code>train</code>, <code>val</code>, or <code>holdout</code>) or <code>site</code> (which can contain any string identifying the location of the camera, used to allocate videos to splits if not already specified).</li> </ul> <p>If the video filepaths in the labels csv are not absolute, be sure to provide the <code>data-dir</code> to which the filepaths are relative.</p> <ul> <li><code>--data-dir PATH</code>: Path to the folder containing your labeled videos.</li> </ul>"},{"location":"train-tutorial/#basic-usage-python-package","title":"Basic usage: Python package","text":"<p>To do the same thing as above using the library code, this would look like:</p> <pre><code>from zamba.models.model_manager import train_model\nfrom zamba.models.config import TrainConfig\n\ntrain_config = TrainConfig(\n    data_dir=\"example_vids/\", labels=\"example_labels.csv\"\n)\ntrain_model(train_config=train_config)\n</code></pre> <p>The only two arguments that can be passed to <code>train_model</code> are <code>train_config</code> and (optionally) <code>video_loader_config</code>. The first step is to instantiate <code>TrainConfig</code>. Optionally, you can also specify video loading arguments by instantiating and passing in <code>VideoLoaderConfig</code>.</p> <p>You'll want to go over the documentation to familiarize yourself with the options in both of these configurations since what you choose can have a large impact on the results of your model. We've tried to include in the documentation sane defaults and recommendations for how to set these parameters. For detailed explanations of all possible configuration arguments, see All Configuration Options.</p>"},{"location":"train-tutorial/#model-output-classes","title":"Model output classes","text":"<p>The classes your trained model will predict are determined by which model you choose and whether the species in your labels are a subset of that model's default labels. This table outlines the default behavior for a set of common scenarios.</p> Classes in labels csv Model What we infer Classes trained model predicts cat, blank <code>blank_nonblank</code> binary model where one is \"blank\" blank zebra, grizzly, blank <code>time_distributed</code> multiclass but not a subset of the zamba labels zebra, grizzly, blank elephant, antelope_duiker, blank <code>time_distributed</code> multiclass and a subset of the zamba labels all African forest zamba species"},{"location":"train-tutorial/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"train-tutorial/#1-specify-the-path-to-your-videos","title":"1. Specify the path to your videos","text":"<p>Save all of your videos in a folder.</p> <ul> <li>They can be in nested directories within the folder.</li> <li>Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here. Any videos that fail a set of FFmpeg checks will be skipped during inference or training. By default, <code>zamba</code> will look for files with the following suffixes: <code>.avi</code>, <code>.mp4</code>, <code>.asf</code>. To use other video suffixes that are supported by FFmpeg, set your <code>VIDEO_SUFFIXES</code> environment variable.</li> </ul> <p>Add the path to your video folder with <code>--data-dir</code>. For example, if your videos are in a folder called <code>example_vids</code>, add <code>--data-dir example_vids/</code> to your command.</p> CLI <pre><code>$ zamba train --data-dir example_vids\n</code></pre> Python <pre><code>from zamba.models.config import TrainConfig\nfrom zamba.models.model_manager import train_model\n\ntrain_config = TrainConfig(data_dir='example_vids/')\ntrain_model(train_config=train_config)\n</code></pre> <p>Note that the above will not run yet because labels are not specified.</p> <p>The more training data you have, the better the resulting model will be. We recommend having a minimum of 100 videos per species. Having an imbalanced dataset - for example, where most of the videos are blank - is okay as long as there are enough examples of each individual species.</p>"},{"location":"train-tutorial/#2-specify-your-labels","title":"2. Specify your labels","text":"<p>Your labels should be saved in a <code>.csv</code> file with columns for filepath and label. For example:</p> <pre><code>$ cat example_labels.csv\nfilepath,label\neleph.MP4,elephant\nleopard.MP4,leopard\nblank.MP4,blank\nchimp.MP4,chimpanzee_bonobo\n</code></pre> <p>Add the path to your labels with <code>--labels</code>.  For example, if your videos are in a folder called <code>example_vids</code> and your labels are saved in <code>example_labels.csv</code>:</p> CLI <pre><code>$ zamba train --data-dir example_vids/ --labels example_labels.csv\n</code></pre> Python <p>In Python, the labels are passed in when <code>TrainConfig</code> is instantiated. The Python package allows you to pass in labels as either a file path or a pandas dataframe: <pre><code>labels_dataframe = pd.read_csv('example_labels.csv', index_col='filepath')\ntrain_config = TrainConfig(\n    data_dir='example_vids/', labels=labels_dataframe\n)\ntrain_model(train_config=train_config)\n</code></pre></p>"},{"location":"train-tutorial/#labels-zamba-has-seen-before","title":"Labels <code>zamba</code> has seen before","text":"<p>Your labels may be included in the list of <code>zamba</code> class labels that the provided models are trained to predict. If so, the relevant model that ships with <code>zamba</code> will essentially be used as a checkpoint, and model training will resume from that checkpoint.</p> <p>By default, the model you train will continue to output all of the Zamba class labels, not just the ones in your dataset. For different behavior, see <code>use_default_model_labels</code>.</p>"},{"location":"train-tutorial/#completely-new-labels","title":"Completely new labels","text":"<p>You can also train a model to predict completely new labels - the world is your oyster! (We'd love to see a model trained to predict oysters.) If this is the case, the model architecture will replace the final neural network layer with a new head that predicts your labels instead of those that ship with <code>zamba</code>.</p> <p>You can then make your model available to others by adding it to the Model Zoo on our wiki.</p>"},{"location":"train-tutorial/#3-choose-a-model-for-training","title":"3. Choose a model for training","text":"<p>Any of the models that ship with <code>zamba</code> can be trained. If you're training on entirely new species or new ecologies, we recommend starting with the <code>time_distributed</code> model as this model is less computationally intensive than the <code>slowfast</code> model.</p> <p>However, if you're tuning a model to a subset of species (e.g. a <code>european_beaver</code> or <code>blank</code> model), use the model that was trained on data that is most similar to your new data.</p> <p>Add the model name to your command with <code>--model</code>. The <code>time_distributed</code> model will be used if no model is specified. For example, if you want to continue training the <code>european</code> model based on the videos in <code>example_euro_vids</code> and the labels in <code>example_euro_labels.csv</code>:</p> CLI <pre><code>$ zamba train --data-dir example_euro_vids/ --labels example_euro_labels.csv --model european\n</code></pre> Python <pre><code>train_config = TrainConfig(\n    data_dir=\"example_euro_vids/\",\n    labels=\"example_euro_labels.csv\",\n    model_name=\"european\",\n)\ntrain_model(train_config=train_config)\n</code></pre>"},{"location":"train-tutorial/#4-specify-any-additional-parameters","title":"4. Specify any additional parameters","text":"<p>And there's so much more! You can also do things like specify your region for faster model download (<code>--weight-download-region</code>), start training from a saved model checkpoint (<code>--checkpoint</code>), or specify a different path where your model should be saved (<code>--save-dir</code>). To read about a few common considerations, see the Guide to Common Optional Parameters page.</p>"},{"location":"train-tutorial/#5-test-your-configuration-with-a-dry-run","title":"5. Test your configuration with a dry run","text":"<p>Before kicking off the full model training, we recommend testing your code with a \"dry run\". This will run one training and validation batch for one epoch to quickly detect any bugs. See the Debugging page for details.</p>"},{"location":"train-tutorial/#files-that-get-written-out-during-training","title":"Files that get written out during training","text":"<p>You can specify where the outputs should be saved with <code>--save-dir</code>. If no save directory is specified, <code>zamba</code> will write out incremental <code>version_n</code> folders to your current working directory. For example, a model finetuned from the provided <code>time_distributed</code> model (the default) will be saved in <code>version_0</code>.</p> <p><code>version_0</code> contains:</p> <ul> <li><code>train_configuration.yaml</code>: The full model configuration used to generate the given model, including <code>video_loader_config</code> and <code>train_config</code>. To continue training using the same configuration, or to train another model using the same configuration, you can pass in <code>train_configurations.yaml</code> (see Specifying Model Configurations with a YAML File) along with the <code>labels</code> filepath.</li> <li><code>hparams.yaml</code>: Model hyperparameters. These are included in the checkpoint file as well.</li> <li><code>time_distributed.ckpt</code>: Model checkpoint. You can continue training from this checkpoint by passing it to <code>zamba train</code> with the <code>--checkpoint</code> flag:     <pre><code>$ zamba train --checkpoint version_0/time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv\n</code></pre></li> <li><code>events.out.tfevents.1632250686.ip-172-31-15-179.14229.0</code>: TensorBoard logs. You can view these with tensorboard:     <pre><code>$ tensorboard --logdir version_0/\n</code></pre></li> <li><code>val_metrics.json</code>: The model's performance on the validation subset</li> <li><code>test_metrics.json</code>: The model's performance on the test (holdout) subset</li> <li><code>splits.csv</code>: Which files were used for training, validation, and as a holdout set. If split is specified in the labels file passed to training, <code>splits.csv</code> will not be saved out.</li> </ul>"},{"location":"yaml-config/","title":"Using YAML configuration files","text":"<p>In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files (<code>.yml</code> or <code>.yaml</code>) are commonly used to serialize data in an easily readable way.</p>"},{"location":"yaml-config/#basic-structure","title":"Basic structure","text":"<p>The basic structure of a YAML model configuration depends on whether you're working with videos or images:</p>"},{"location":"yaml-config/#video-workflows","title":"Video Workflows","text":"<p>For video workflows, the basic YAML structure includes:</p> <pre><code>$ cat video_config.yaml\nvideo_loader_config:\n  model_input_height: 240\n  model_input_width: 426\n  total_frames: 16\n  # other video loading parameters\n\ntrain_config:\n  model_name: time_distributed\n  data_dir: example_vids/\n  labels: example_labels.csv\n  # other training parameters, eg. batch_size\n\npredict_config:\n  model_name: time_distributed\n  data_dir: example_vids/\n  # other prediction parameters, eg. batch_size\n</code></pre> <p>For example, the configuration below will predict labels for the videos in <code>example_vids</code> using the <code>time_distributed</code> model. When videos are loaded, each will be resized to 240x426 pixels and 16 frames will be selected:</p> <pre><code>video_loader_config:\n  model_input_height: 240\n  model_input_width: 426\n  total_frames: 16\n\npredict_config:\n  model_name: time_distributed\n  data_dir: example_vids/\n</code></pre>"},{"location":"yaml-config/#image-workflows","title":"Image Workflows","text":"<p>For image classification workflows, the structure is:</p> <pre><code>$ cat image_config.yaml\ntrain_config:\n  model_name: lila.science\n  data_dir: example_imgs/\n  labels: example_labels.csv\n  image_size: 224\n  # other training parameters, eg. batch_size\n\npredict_config:\n  model_name: lila.science\n  data_dir: example_imgs/\n  image_size: 224\n  # other prediction parameters, eg. crop_images\n</code></pre> <p>For example, this configuration will predict labels for the images in <code>example_imgs</code> using the <code>lila.science</code> model:</p> <pre><code>predict_config:\n  model_name: lila.science\n  data_dir: example_imgs/\n  image_size: 224\n  crop_images: true\n</code></pre>"},{"location":"yaml-config/#required-arguments","title":"Required arguments","text":""},{"location":"yaml-config/#for-video-workflows","title":"For Video Workflows","text":"<p>Either <code>predict_config</code> or <code>train_config</code> is required, based on whether you will be running inference or training a model. See All Configuration Options for a full list of what can be specified under each class. To run inference, <code>data_dir</code>and/or <code>filepaths</code> must be specified. To train a model, <code>labels</code> must be specified.</p> <p>In <code>video_loader_config</code>, you must specify at least <code>model_input_height</code>, <code>model_input_width</code>, and <code>total_frames</code>. While this is the minimum required, we strongly recommend being intentional in your choice of frame selection method. <code>total_frames</code> by itself will just take the first <code>n</code> frames. For a full list of frame selection methods, see the section on Video loading arguments.</p> <ul> <li>For <code>time_distributed</code>, <code>european</code>, or <code>blank_nonblank</code>, <code>total_frames</code> must be 16</li> <li>For <code>slowfast</code>, <code>total_frames</code> must be 32</li> </ul>"},{"location":"yaml-config/#for-image-workflows","title":"For Image Workflows","text":"<p>Either <code>predict_config</code> or <code>train_config</code> is required in the image pipeline. To run inference, <code>data_dir</code> and/or <code>filepaths</code> must be specified. To train a model, both <code>data_dir</code> and <code>labels</code> must be specified.</p> <p>For image workflows, <code>image_size</code> is a key parameter that defines the input dimensions for the model.</p>"},{"location":"yaml-config/#command-line-interface","title":"Command line interface","text":"<p>A YAML configuration file can be passed to the command line interface with the <code>--config</code> argument. For example, say the example configuration above is saved as <code>video_config.yaml</code>. To run prediction:</p> <p>For video workflows:</p> <pre><code>$ zamba predict --config video_config.yaml\n$ zamba train --config video_config.yaml\n</code></pre> <p>For image workflows:</p> <pre><code>$ zamba image predict --config image_config.yaml\n$ zamba image train --config image_config.yaml\n</code></pre> <p>Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in the help output (e.g., <code>zamba predict --help</code> or <code>zamba image train --help</code>) must be passed in a YAML file (see the Quickstart guide for details).</p>"},{"location":"yaml-config/#python-package","title":"Python package","text":""},{"location":"yaml-config/#video-workflows_1","title":"Video Workflows","text":"<p>The main API for video workflows is the <code>ModelManager</code> class that can be accessed with:</p> <pre><code>from zamba.models.model_manager import ModelManager\n</code></pre> <p>The <code>ModelManager</code> class is used by <code>zamba</code>'s command line interface to handle preprocessing the filenames, loading the videos, training the model, performing inference, and saving predictions. Therefore any functionality available to the command line interface is accessible via the <code>ModelManager</code> class.</p> <p>To instantiate the <code>ModelManager</code> based on a configuration file saved at <code>video_config.yaml</code>: <pre><code>&gt;&gt;&gt; manager = ModelManager.from_yaml('video_config.yaml')\n</code></pre></p> <p>We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the <code>ModelManager</code> class. To run inference or training: <pre><code>manager.predict() # inference\nmanager.train() # training\n</code></pre></p> <p>In our user tutorials, we refer to <code>train_model</code> and <code>predict_model</code> functions. The <code>ModelManager</code> class calls these same functions behind the scenes when <code>.predict()</code> or <code>.train()</code> is run.</p>"},{"location":"yaml-config/#image-workflows_1","title":"Image Workflows","text":"<p>For image classification workflows, the main API is the <code>ZambaImagesManager</code> class. A full workflow might look something like this:</p> <pre><code>from zamba.images.manager import ZambaImagesManager\nfrom zamba.images.config import ImageClassifictionPredictConfig\nmanager = ZambaImagesManager()\nconfig = ImageClassifictionPredictConfig.parse_file('image_config.yaml')\nmanager.predict(config)\n</code></pre>"},{"location":"yaml-config/#default-configurations","title":"Default configurations","text":"<p>In the command line, the default configuration for each model is passed in using a specified YAML file that ships with <code>zamba</code>. You can see the default configuration YAML files on Github in the <code>config.yaml</code> file within each model's folder.</p> <p>For example, the default configuration for the <code>time_distributed</code> model is:</p> <pre><code>train_config:\n  scheduler_config:\n    scheduler: MultiStepLR\n    scheduler_params:\n      gamma: 0.5\n      milestones:\n      - 3\n      verbose: true\n  model_name: time_distributed\n  backbone_finetune_config:\n    backbone_initial_ratio_lr: 0.01\n    multiplier: 1\n    pre_train_bn: true\n    train_bn: false\n    unfreeze_backbone_at_epoch: 3\n    verbose: true\n  early_stopping_config:\n    patience: 5\nvideo_loader_config:\n  model_input_height: 240\n  model_input_width: 426\n  crop_bottom_pixels: 50\n  fps: 4\n  total_frames: 16\n  ensure_total_frames: true\n  megadetector_lite_config:\n    confidence: 0.25\n    fill_mode: score_sorted\n    n_frames: 16\npredict_config:\n  model_name: time_distributed\npublic_checkpoint: time_distributed_9e710aa8c92d25190a64b3b04b9122bdcb456982.ckpt\n</code></pre>"},{"location":"yaml-config/#templates","title":"Templates","text":"<p>To make modifying the existing defaults easier, we've set up the official models as templates in the <code>templates</code> folder.</p> <p>Just fill in your data directory and labels, make any desired tweaks to the model config, and then kick off some training. Happy modeling!</p>"},{"location":"api-reference/data-metadata/","title":"zamba.data.metadata","text":""},{"location":"api-reference/data-metadata/#zamba.data.metadata.create_site_specific_splits","title":"<code>create_site_specific_splits(site, proportions, random_state=989)</code>","text":"<p>Splits sites into distinct groups whose sizes roughly matching the given proportions. Null sites are randomly assigned to groups using the provided proportions.</p> <p>Parameters:</p> Name Type Description Default <code>site</code> <code>Series</code> <p>A series of sites, one element per observation,</p> required <code>proportions</code> <code>dict</code> <p>A dict whose keys are the resulting groups and whose values are the rough proportion of data in each group.</p> required <code>seed</code> <code>int</code> <p>Seed for random split of null sites.</p> required Example <p>Split data into groups where each site is in one and only one group with roughly 50-25-25 train-val-holdout proportions.</p> <p>create_site_specific_splits(site, proportions={\"train\": 2, \"val\": 1, \"holdout\": 1})</p> <p>Returns:</p> Type Description <p>pd.Series: A series containing the resulting split, one element per observation.</p> Source code in <code>zamba/data/metadata.py</code> <pre><code>def create_site_specific_splits(\n    site: pd.Series,\n    proportions: Dict[str, int],\n    random_state: Optional[Union[int, np.random.mtrand.RandomState]] = 989,\n):\n    \"\"\"Splits sites into distinct groups whose sizes roughly matching the given proportions. Null\n    sites are randomly assigned to groups using the provided proportions.\n\n    Args:\n        site (pd.Series): A series of sites, one element per observation,\n        proportions (dict): A dict whose keys are the resulting groups and whose values are the\n            rough proportion of data in each group.\n        seed (int): Seed for random split of null sites.\n\n    Example:\n        Split data into groups where each site is in one and only one group with roughly 50-25-25\n        train-val-holdout proportions.\n\n        &gt;&gt;&gt; create_site_specific_splits(site, proportions={\"train\": 2, \"val\": 1, \"holdout\": 1})\n\n    Returns:\n        pd.Series: A series containing the resulting split, one element per observation.\n\n    \"\"\"\n\n    assignments = {}\n    sites = site.value_counts(dropna=True).sort_values(ascending=False).index\n    n_subgroups = sum(proportions.values())\n    for i, subset in enumerate(\n        roundrobin(*([subset] * proportions[subset] for subset in proportions))\n    ):\n        for group in sites[i::n_subgroups]:\n            assignments[group] = subset\n\n    # Divide null sites among the groups\n    null_sites = site.isnull()\n    if null_sites.sum() &gt; 0:\n        logger.debug(f\"{null_sites.sum():,} null sites randomly assigned to groups.\")\n        null_groups = []\n        for group, group_proportion in proportions.items():\n            null_group = f\"{group}-{uuid4()}\"\n            null_groups.append(null_group)\n            assignments[null_group] = group\n\n        rng = (\n            np.random.RandomState(random_state) if isinstance(random_state, int) else random_state\n        )\n        site = site.copy()\n        site.loc[null_sites] = rng.choice(\n            null_groups,\n            p=np.asarray(list(proportions.values())) / sum(proportions.values()),\n            size=null_sites.sum(),\n            replace=True,\n        )\n\n    return site.replace(assignments)\n</code></pre>"},{"location":"api-reference/data-metadata/#zamba.data.metadata.roundrobin","title":"<code>roundrobin(*iterables)</code>","text":"<p>roundrobin('ABC', 'D', 'EF') --&gt; A D E B F C</p> Source code in <code>zamba/data/metadata.py</code> <pre><code>def roundrobin(*iterables):\n    \"roundrobin('ABC', 'D', 'EF') --&gt; A D E B F C\"\n    # From https://docs.python.org/3/library/itertools.html#recipes\n    # Recipe credited to George Sakkis\n    num_active = len(iterables)\n    nexts = itertools.cycle(iter(it).__next__ for it in iterables)\n    while num_active:\n        try:\n            for next in nexts:\n                yield next()\n        except StopIteration:\n            # Remove the iterator we just exhausted from the cycle.\n            num_active -= 1\n            nexts = itertools.cycle(itertools.islice(nexts, num_active))\n</code></pre>"},{"location":"api-reference/data-video/","title":"zamba.data.video","text":""},{"location":"api-reference/data-video/#zamba.data.video.VideoLoaderConfig","title":"<code>VideoLoaderConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for load_video_frames.</p> <p>Parameters:</p> Name Type Description Default <code>crop_bottom_pixels</code> <code>int</code> <p>Number of pixels to crop from the bottom of the video (prior to resizing to <code>video_height</code>).</p> required <code>i_frames</code> <code>bool</code> <p>Only load the I-Frames. See https://en.wikipedia.org/wiki/Video_compression_picture_types#Intra-coded_(I)frames/slices(key_frames)</p> required <code>scene_threshold</code> <code>float</code> <p>Only load frames that correspond to scene changes. See http://www.ffmpeg.org/ffmpeg-filters.html#select_002c-aselect</p> required <code>megadetector_lite_config</code> <code>MegadetectorLiteYoloXConfig</code> <p>Configuration of MegadetectorLiteYoloX frame selection model.</p> required <code>frame_selection_height</code> <code>int</code> <p>Resize the video to this height in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size images (setting to None) is recommended for MegadetectorLite, especially if your species of interest are smaller.</p> required <code>frame_selection_width</code> <code>int</code> <p>Resize the video to this width in pixels, prior to frame selection.</p> required <code>total_frames</code> <code>int</code> <p>Number of frames that should ultimately be returned.</p> required <code>ensure_total_frames</code> <code>bool</code> <p>Selecting the number of frames by resampling may result in one more or fewer frames due to rounding. If True, ensure the requested number of frames is returned by either clipping or duplicating the final frame. Raises an error if no frames have been selected. Otherwise, return the array unchanged.</p> required <code>fps</code> <code>float</code> <p>Resample the video evenly from the entire duration to a specific number of frames per second.</p> required <code>early_bias</code> <code>bool</code> <p>Resamples to 24 fps and selects 16 frames biased toward the front (strategy used by competition winner).</p> required <code>frame_indices</code> <code>list(int)</code> <p>Select specific frame numbers. Note: frame selection is done after any resampling.</p> required <code>evenly_sample_total_frames</code> <code>bool</code> <p>Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False.</p> required <code>pix_fmt</code> <code>str</code> <p>ffmpeg pixel format, defaults to 'rgb24' for RGB channels; can be changed to 'bgr24' for BGR.</p> required <code>model_input_height</code> <code>int</code> <p>After frame selection, resize the video to this height in pixels.</p> required <code>model_input_width</code> <code>int</code> <p>After frame selection, resize the video to this width in pixels.</p> required <code>cache_dir</code> <code>Path</code> <p>Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable. Defaults to None, which means videos will not be cached. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs.</p> required <code>cleanup_cache</code> <code>bool</code> <p>Whether to delete the cache dir after training or predicting ends. Defaults to False.</p> required Source code in <code>zamba/data/video.py</code> <pre><code>class VideoLoaderConfig(BaseModel):\n    \"\"\"\n    Configuration for load_video_frames.\n\n    Args:\n        crop_bottom_pixels (int, optional): Number of pixels to crop from the bottom of the video\n            (prior to resizing to `video_height`).\n        i_frames (bool, optional): Only load the I-Frames. See\n            https://en.wikipedia.org/wiki/Video_compression_picture_types#Intra-coded_(I)_frames/slices_(key_frames)\n        scene_threshold (float, optional): Only load frames that correspond to scene changes.\n            See http://www.ffmpeg.org/ffmpeg-filters.html#select_002c-aselect\n        megadetector_lite_config (MegadetectorLiteYoloXConfig, optional): Configuration of\n            MegadetectorLiteYoloX frame selection model.\n        frame_selection_height (int, optional): Resize the video to this height in pixels, prior to\n            frame selection. If None, the full size video will be used for frame selection. Using full\n            size images (setting to None) is recommended for MegadetectorLite, especially if your\n            species of interest are smaller.\n        frame_selection_width (int, optional): Resize the video to this width in pixels, prior to\n            frame selection.\n        total_frames (int, optional): Number of frames that should ultimately be returned.\n        ensure_total_frames (bool): Selecting the number of frames by resampling may result in one\n            more or fewer frames due to rounding. If True, ensure the requested number of frames\n            is returned by either clipping or duplicating the final frame. Raises an error if no\n            frames have been selected. Otherwise, return the array unchanged.\n        fps (float, optional): Resample the video evenly from the entire duration to a specific\n            number of frames per second.\n        early_bias (bool, optional): Resamples to 24 fps and selects 16 frames biased toward the\n            front (strategy used by competition winner).\n        frame_indices (list(int), optional): Select specific frame numbers. Note: frame selection\n            is done after any resampling.\n        evenly_sample_total_frames (bool, optional): Reach the total number of frames specified by\n            evenly sampling from the duration of the video. Defaults to False.\n        pix_fmt (str, optional): ffmpeg pixel format, defaults to 'rgb24' for RGB channels; can be\n            changed to 'bgr24' for BGR.\n        model_input_height (int, optional): After frame selection, resize the video to this height\n            in pixels.\n        model_input_width (int, optional): After frame selection, resize the video to this width in\n            pixels.\n        cache_dir (Path, optional): Cache directory where preprocessed videos will be saved\n            upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable.\n            Defaults to None, which means videos will not be cached. Provided there is enough space\n            on your machine, it is highly encouraged to cache videos for training as this will\n            speed up all subsequent epochs. If you are predicting on the same videos with the\n            same video loader configuration, this will save time on future runs.\n        cleanup_cache (bool): Whether to delete the cache dir after training or predicting ends.\n            Defaults to False.\n    \"\"\"\n\n    crop_bottom_pixels: Optional[int] = None\n    i_frames: Optional[bool] = False\n    scene_threshold: Optional[float] = None\n    megadetector_lite_config: Optional[MegadetectorLiteYoloXConfig] = None\n    frame_selection_height: Optional[int] = None\n    frame_selection_width: Optional[int] = None\n    total_frames: Optional[int] = None\n    ensure_total_frames: Optional[bool] = True\n    fps: Optional[float] = None\n    early_bias: Optional[bool] = False\n    frame_indices: Optional[List[int]] = None\n    evenly_sample_total_frames: Optional[bool] = False\n    pix_fmt: Optional[str] = \"rgb24\"\n    model_input_height: Optional[int] = None\n    model_input_width: Optional[int] = None\n    cache_dir: Optional[Path] = None\n    cleanup_cache: bool = False\n\n    class Config:\n        extra = \"forbid\"\n\n    @validator(\"cache_dir\", always=True)\n    def validate_video_cache_dir(cls, cache_dir):\n        \"\"\"Set up cache directory for preprocessed videos. Config argument takes precedence\n        over environment variable.\n        \"\"\"\n        if cache_dir is None:\n            cache_dir = os.getenv(\"VIDEO_CACHE_DIR\", None)\n\n            if cache_dir in [\"\", \"0\"]:\n                cache_dir = None\n\n        if cache_dir is not None:\n            cache_dir = Path(cache_dir)\n            cache_dir.mkdir(parents=True, exist_ok=True)\n\n        return cache_dir\n\n    @root_validator(skip_on_failure=True)\n    def check_height_and_width(cls, values):\n        if (values[\"frame_selection_height\"] is None) ^ (values[\"frame_selection_width\"] is None):\n            raise ValueError(\n                f\"Must provide both frame_selection_height and frame_selection_width or neither. Values provided are {values}.\"\n            )\n        if (values[\"model_input_height\"] is None) ^ (values[\"model_input_width\"] is None):\n            raise ValueError(\n                f\"Must provide both model_input_height and model_input_width or neither. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def check_fps_compatibility(cls, values):\n        if values[\"fps\"] and (\n            values[\"evenly_sample_total_frames\"] or values[\"i_frames\"] or values[\"scene_threshold\"]\n        ):\n            raise ValueError(\n                f\"fps cannot be used with evenly_sample_total_frames, i_frames, or scene_threshold. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def check_i_frame_compatibility(cls, values):\n        if values[\"scene_threshold\"] and values[\"i_frames\"]:\n            raise ValueError(\n                f\"i_frames cannot be used with scene_threshold. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def check_early_bias_compatibility(cls, values):\n        if values[\"early_bias\"] and (\n            values[\"i_frames\"]\n            or values[\"scene_threshold\"]\n            or values[\"total_frames\"]\n            or values[\"evenly_sample_total_frames\"]\n            or values[\"fps\"]\n        ):\n            raise ValueError(\n                f\"early_bias cannot be used with i_frames, scene_threshold, total_frames, evenly_sample_total_frames, or fps. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def check_frame_indices_compatibility(cls, values):\n        if values[\"frame_indices\"] and (\n            values[\"total_frames\"]\n            or values[\"scene_threshold\"]\n            or values[\"i_frames\"]\n            or values[\"early_bias\"]\n            or values[\"evenly_sample_total_frames\"]\n        ):\n            raise ValueError(\n                f\"frame_indices cannot be used with total_frames, scene_threshold, i_frames, early_bias, or evenly_sample_total_frames. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def check_megadetector_lite_compatibility(cls, values):\n        if values[\"megadetector_lite_config\"] and (\n            values[\"early_bias\"] or values[\"evenly_sample_total_frames\"]\n        ):\n            raise ValueError(\n                f\"megadetector_lite_config cannot be used with early_bias or evenly_sample_total_frames. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def check_evenly_sample_total_frames_compatibility(cls, values):\n        if values[\"evenly_sample_total_frames\"] is True and values[\"total_frames\"] is None:\n            raise ValueError(\n                f\"total_frames must be specified if evenly_sample_total_frames is used. Values provided are {values}.\"\n            )\n        if values[\"evenly_sample_total_frames\"] and (\n            values[\"scene_threshold\"]\n            or values[\"i_frames\"]\n            or values[\"fps\"]\n            or values[\"early_bias\"]\n        ):\n            raise ValueError(\n                f\"evenly_sample_total_frames cannot be used with scene_threshold, i_frames, fps, or early_bias. Values provided are {values}.\"\n            )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_total_frames(cls, values):\n        if values[\"megadetector_lite_config\"] is not None:\n            # set n frames for megadetector_lite_config if only specified by total_frames\n            if values[\"megadetector_lite_config\"].n_frames is None:\n                values[\"megadetector_lite_config\"].n_frames = values[\"total_frames\"]\n\n            # set total frames if only specified in megadetector_lite_config\n            if values[\"total_frames\"] is None:\n                values[\"total_frames\"] = values[\"megadetector_lite_config\"].n_frames\n\n        return values\n</code></pre>"},{"location":"api-reference/data-video/#zamba.data.video.VideoLoaderConfig.validate_video_cache_dir","title":"<code>validate_video_cache_dir(cache_dir)</code>","text":"<p>Set up cache directory for preprocessed videos. Config argument takes precedence over environment variable.</p> Source code in <code>zamba/data/video.py</code> <pre><code>@validator(\"cache_dir\", always=True)\ndef validate_video_cache_dir(cls, cache_dir):\n    \"\"\"Set up cache directory for preprocessed videos. Config argument takes precedence\n    over environment variable.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = os.getenv(\"VIDEO_CACHE_DIR\", None)\n\n        if cache_dir in [\"\", \"0\"]:\n            cache_dir = None\n\n    if cache_dir is not None:\n        cache_dir = Path(cache_dir)\n        cache_dir.mkdir(parents=True, exist_ok=True)\n\n    return cache_dir\n</code></pre>"},{"location":"api-reference/data-video/#zamba.data.video.ensure_frame_number","title":"<code>ensure_frame_number(arr, total_frames)</code>","text":"<p>Ensures the array contains the requested number of frames either by clipping frames from the end or dulpicating the last frame.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Array of video frames with shape (frames, height, width, channel).</p> required <code>total_frames</code> <code>int</code> <p>Desired number of frames in output array.</p> required Source code in <code>zamba/data/video.py</code> <pre><code>def ensure_frame_number(arr, total_frames: int):\n    \"\"\"Ensures the array contains the requested number of frames either by clipping frames from\n    the end or dulpicating the last frame.\n\n    Args:\n        arr (np.ndarray): Array of video frames with shape (frames, height, width, channel).\n        total_frames (int): Desired number of frames in output array.\n    \"\"\"\n    if (total_frames is None) or (arr.shape[0] == total_frames):\n        return arr\n    elif arr.shape[0] == 0:\n        logger.warning(\n            \"No frames selected. Returning an array in the desired shape with all zeros.\"\n        )\n        return np.zeros((total_frames, arr.shape[1], arr.shape[2], arr.shape[3]), dtype=\"int\")\n    elif arr.shape[0] &gt; total_frames:\n        logger.info(\n            f\"Clipping {arr.shape[0] - total_frames} frames \"\n            f\"(original: {arr.shape[0]}, requested: {total_frames}).\"\n        )\n        return arr[:total_frames]\n    elif arr.shape[0] &lt; total_frames:\n        logger.info(\n            f\"Duplicating last frame {total_frames - arr.shape[0]} times \"\n            f\"(original: {arr.shape[0]}, requested: {total_frames}).\"\n        )\n        return np.concatenate(\n            [arr, np.tile(arr[-1], (total_frames - arr.shape[0], 1, 1, 1))], axis=0\n        )\n</code></pre>"},{"location":"api-reference/data-video/#zamba.data.video.get_cached_array_path","title":"<code>get_cached_array_path(vid_path, config)</code>","text":"<p>Get the path to where the cached array would be, if it exists.</p> <p>vid_path: string path to the video, or Path config: VideoLoaderConfig</p> <p>returns: Path object to the cached data</p> Source code in <code>zamba/data/video.py</code> <pre><code>def get_cached_array_path(vid_path, config):\n    \"\"\"Get the path to where the cached array would be, if it exists.\n\n    vid_path: string path to the video, or Path\n    config: VideoLoaderConfig\n\n    returns: Path object to the cached data\n    \"\"\"\n    assert isinstance(config, VideoLoaderConfig)\n\n    # don't include `cleanup_cache` or `cache_dir` in the hashed config\n    # NOTE: sorting the keys avoids a cache miss if we see the same config in a different order;\n    # might not be necessary with a VideoLoaderConfig\n    config_dict = config.dict()\n    keys = config_dict.keys() - {\"cleanup_cache\", \"cache_dir\"}\n    hashed_part = {k: config_dict[k] for k in sorted(keys)}\n\n    # hash config for inclusion in path\n    hash_str = hashlib.sha1(str(hashed_part).encode(\"utf-8\")).hexdigest()\n    logger.opt(lazy=True).debug(f\"Generated hash {hash_str} from {hashed_part}\")\n\n    # strip leading \"/\" in absolute path\n    vid_path = AnyPath(str(vid_path).lstrip(\"/\"))\n\n    # if the video is in S3, drop the prefix and bucket name\n    if isinstance(vid_path, S3Path):\n        vid_path = AnyPath(vid_path.key)\n\n    cache_dir = config.cache_dir\n    npy_path = AnyPath(cache_dir) / hash_str / vid_path.with_suffix(\".npy\")\n    return npy_path\n</code></pre>"},{"location":"api-reference/data-video/#zamba.data.video.load_and_repeat_image","title":"<code>load_and_repeat_image(path, target_size=(224, 224), repeat_count=4)</code>","text":"<p>Loads an image, resizes it, and repeats it N times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to the image file.</p> required <code>target_size</code> <p>A tuple (w, h) representing the desired width and height of the resized image.</p> <code>(224, 224)</code> <code>repeat_count</code> <p>Number of times to repeat the image.</p> <code>4</code> <p>Returns:</p> Type Description <p>A NumPy array of shape (N, h, w, 3) representing the repeated image.</p> Source code in <code>zamba/data/video.py</code> <pre><code>def load_and_repeat_image(path, target_size=(224, 224), repeat_count=4):\n    \"\"\"\n    Loads an image, resizes it, and repeats it N times.\n\n    Args:\n        path: Path to the image file.\n        target_size: A tuple (w, h) representing the desired width and height of the resized image.\n        repeat_count: Number of times to repeat the image.\n\n    Returns:\n        A NumPy array of shape (N, h, w, 3) representing the repeated image.\n    \"\"\"\n    image = cv2.imread(str(path))\n\n    # Resize the image in same way as video frames are in `load_video_frames`\n    image = cv2.resize(\n        image,\n        target_size,\n        # https://stackoverflow.com/a/51042104/1692709\n        interpolation=(\n            cv2.INTER_LINEAR\n            if image.shape[1] &lt; target_size[0]  # compare image width with target width\n            else cv2.INTER_AREA\n        ),\n    )\n\n    image_array = np.expand_dims(image, axis=0)\n\n    # Repeat the image N times\n    repeated_image = np.repeat(image_array, repeat_count, axis=0)\n\n    return repeated_image\n</code></pre>"},{"location":"api-reference/data-video/#zamba.data.video.load_video_frames","title":"<code>load_video_frames(filepath, config=None, **kwargs)</code>","text":"<p>Loads frames from videos using fast ffmpeg commands.</p> <p>Supports images as well, but it is inefficient since we just replicate the frames.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>PathLike</code> <p>Path to the video.</p> required <code>config</code> <code>VideoLoaderConfig</code> <p>Configuration for video loading.</p> <code>None</code> <code>**kwargs</code> <p>Optionally, arguments for VideoLoaderConfig can be passed in directly.</p> <code>{}</code> <p>Returns:</p> Type Description <p>np.ndarray: An array of video frames with dimensions (time x height x width x channels).</p> Source code in <code>zamba/data/video.py</code> <pre><code>def load_video_frames(\n    filepath: os.PathLike,\n    config: Optional[VideoLoaderConfig] = None,\n    **kwargs,\n):\n    \"\"\"Loads frames from videos using fast ffmpeg commands.\n\n    Supports images as well, but it is inefficient since we just replicate the frames.\n\n    Args:\n        filepath (os.PathLike): Path to the video.\n        config (VideoLoaderConfig, optional): Configuration for video loading.\n        **kwargs: Optionally, arguments for VideoLoaderConfig can be passed in directly.\n\n    Returns:\n        np.ndarray: An array of video frames with dimensions (time x height x width x channels).\n    \"\"\"\n    if not Path(filepath).exists():\n        raise FileNotFoundError(f\"No file found at {filepath}\")\n\n    if config is None:\n        config = VideoLoaderConfig(**kwargs)\n\n    if Path(filepath).suffix.lower() in IMAGE_SUFFIXES:\n        return load_and_repeat_image(\n            filepath,\n            target_size=(config.model_input_width, config.model_input_height),\n            repeat_count=config.total_frames,\n        )\n\n    video_stream = get_video_stream(filepath)\n    w = int(video_stream[\"width\"])\n    h = int(video_stream[\"height\"])\n\n    pipeline = ffmpeg.input(str(filepath))\n    pipeline_kwargs = {}\n\n    if (config.crop_bottom_pixels is not None) and (config.crop_bottom_pixels &gt; 0):\n        # scale to ensure all frames are the same height and we can crop\n        pipeline = pipeline.filter(\"scale\", f\"{w},{h}\")\n        pipeline = pipeline.crop(\"0\", \"0\", \"iw\", f\"ih-{config.crop_bottom_pixels}\")\n        h = h - config.crop_bottom_pixels\n\n    if config.evenly_sample_total_frames:\n        config.fps = config.total_frames / float(video_stream[\"duration\"])\n\n    if config.early_bias:\n        config.fps = 24  # competition frame selection assumes 24 frames per second\n        config.total_frames = 16  # used for ensure_total_frames\n\n    if config.fps:\n        pipeline = pipeline.filter(\"fps\", fps=config.fps, round=\"up\")\n\n    if config.i_frames:\n        pipeline = pipeline.filter(\"select\", \"eq(pict_type,PICT_TYPE_I)\")\n\n    if config.scene_threshold:\n        pipeline = pipeline.filter(\"select\", f\"gt(scene,{config.scene_threshold})\")\n\n    if config.frame_selection_height and config.frame_selection_width:\n        pipeline = pipeline.filter(\n            \"scale\", f\"{config.frame_selection_width},{config.frame_selection_height}\"\n        )\n        w, h = config.frame_selection_width, config.frame_selection_height\n\n    if config.early_bias:\n        config.frame_indices = [2, 8, 12, 18, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156]\n\n    if config.frame_indices:\n        pipeline = pipeline.filter(\"select\", \"+\".join(f\"eq(n,{f})\" for f in config.frame_indices))\n        pipeline_kwargs = {\"vsync\": 0}\n\n    pipeline = pipeline.output(\n        \"pipe:\", format=\"rawvideo\", pix_fmt=config.pix_fmt, **pipeline_kwargs\n    )\n\n    try:\n        out, err = pipeline.run(capture_stdout=True, capture_stderr=True)\n    except ffmpeg.Error as exc:\n        raise ZambaFfmpegException(exc.stderr)\n\n    arr = np.frombuffer(out, np.uint8).reshape([-1, h, w, 3])\n\n    if config.megadetector_lite_config is not None:\n        mdlite = MegadetectorLiteYoloX(config=config.megadetector_lite_config)\n        detection_probs = mdlite.detect_video(video_arr=arr)\n\n        arr = mdlite.filter_frames(arr, detection_probs)\n\n    if (config.model_input_height is not None) and (config.model_input_width is not None):\n        resized_frames = np.zeros(\n            (arr.shape[0], config.model_input_height, config.model_input_width, 3), np.uint8\n        )\n        for ix, f in enumerate(arr):\n            if (f.shape[0] != config.model_input_height) or (\n                f.shape[1] != config.model_input_width\n            ):\n                f = cv2.resize(\n                    f,\n                    (config.model_input_width, config.model_input_height),\n                    # https://stackoverflow.com/a/51042104/1692709\n                    interpolation=(\n                        cv2.INTER_LINEAR\n                        if f.shape[1] &lt; config.model_input_width\n                        else cv2.INTER_AREA\n                    ),\n                )\n            resized_frames[ix, ...] = f\n        arr = np.array(resized_frames)\n\n    if config.ensure_total_frames:\n        arr = ensure_frame_number(arr, total_frames=config.total_frames)\n\n    return arr\n</code></pre>"},{"location":"api-reference/densepose_config/","title":"zamba.models.densepose.config","text":""},{"location":"api-reference/densepose_config/#zamba.models.densepose.config.DensePoseConfig","title":"<code>DensePoseConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration for running dense pose on videos.</p> <p>Parameters:</p> Name Type Description Default <code>video_loader_config</code> <code>VideoLoaderConfig</code> <p>Configuration for loading videos</p> required <code>output_type</code> <code>str</code> <p>one of DensePoseOutputEnum (currently \"segmentation\" or \"chimp_anatomy\").</p> required <code>render_output</code> <code>bool</code> <p>Whether to save a version of the video with the output overlaid on top. Defaults to False.</p> required <code>embeddings_in_json</code> <code>bool</code> <p>Whether to save the embeddings matrices in the json of the DensePose result. Setting to True can result in large json files. Defaults to False.</p> required <code>data_dir</code> <code>Path</code> <p>Where to find the files listed in filepaths (or where to look if filepaths is not provided).</p> required <code>filepaths</code> <code>Path</code> <p>Path to a CSV file with a list of filepaths to process.</p> required <code>save_dir</code> <code>Path</code> <p>Directory for where to save the output files; defaults to os.getcwd().</p> required <code>cache_dir</code> <code>Path</code> <p>Path for downloading and saving model weights. Defaults to env var <code>MODEL_CACHE_DIR</code> or the OS app cache dir.</p> required <code>weight_download_region</code> <code>RegionEnum</code> <p>region where to download weights; should be one of RegionEnum (currently 'us', 'asia', and 'eu'). Defaults to 'us'.</p> required Source code in <code>zamba/models/densepose/config.py</code> <pre><code>class DensePoseConfig(ZambaBaseModel):\n    \"\"\"Configuration for running dense pose on videos.\n\n    Args:\n        video_loader_config (VideoLoaderConfig): Configuration for loading videos\n        output_type (str): one of DensePoseOutputEnum (currently \"segmentation\" or \"chimp_anatomy\").\n        render_output (bool): Whether to save a version of the video with the output overlaid on top.\n            Defaults to False.\n        embeddings_in_json (bool): Whether to save the embeddings matrices in the json of the\n            DensePose result. Setting to True can result in large json files. Defaults to False.\n        data_dir (Path): Where to find the files listed in filepaths (or where to look if\n            filepaths is not provided).\n        filepaths (Path, optional): Path to a CSV file with a list of filepaths to process.\n        save_dir (Path, optional): Directory for where to save the output files;\n            defaults to os.getcwd().\n        cache_dir (Path, optional): Path for downloading and saving model weights. Defaults\n            to env var `MODEL_CACHE_DIR` or the OS app cache dir.\n        weight_download_region (RegionEnum, optional): region where to download weights; should\n            be one of RegionEnum (currently 'us', 'asia', and 'eu'). Defaults to 'us'.\n    \"\"\"\n\n    video_loader_config: VideoLoaderConfig\n    output_type: DensePoseOutputEnum\n    render_output: bool = False\n    embeddings_in_json: bool = False\n    data_dir: Path\n    filepaths: Optional[Path] = None\n    save_dir: Optional[Path] = None\n    cache_dir: Optional[Path] = None\n    weight_download_region: RegionEnum = RegionEnum(\"us\")\n\n    _validate_cache_dir = validator(\"cache_dir\", allow_reuse=True, always=True)(\n        validate_model_cache_dir\n    )\n\n    def run_model(self):\n        \"\"\"Use this configuration to execute DensePose via the DensePoseManager\"\"\"\n        if not isinstance(self.output_type, DensePoseOutputEnum):\n            self.output_type = DensePoseOutputEnum(self.output_type)\n\n        if self.output_type == DensePoseOutputEnum.segmentation.value:\n            model = MODELS[\"animals\"]\n        elif self.output_type == DensePoseOutputEnum.chimp_anatomy.value:\n            model = MODELS[\"chimps\"]\n        else:\n            raise Exception(f\"invalid {self.output_type}\")\n\n        output_dir = Path(os.getcwd()) if self.save_dir is None else self.save_dir\n\n        dpm = DensePoseManager(\n            model, model_cache_dir=self.cache_dir, download_region=self.weight_download_region\n        )\n\n        for fp in tqdm(self.filepaths.filepath, desc=\"Videos\"):\n            fp = Path(fp)\n\n            vid_arr, labels = dpm.predict_video(fp, video_loader_config=self.video_loader_config)\n\n            # serialize the labels generated by densepose to json\n            output_path = output_dir / f\"{fp.stem}_denspose_labels.json\"\n            dpm.serialize_video_output(\n                labels, filename=output_path, write_embeddings=self.embeddings_in_json\n            )\n\n            # re-render the video with the densepose labels visualized on top of the video\n            if self.render_output:\n                output_path = output_dir / f\"{fp.stem}_denspose_video{''.join(fp.suffixes)}\"\n                visualized_video = dpm.visualize_video(\n                    vid_arr, labels, output_path=output_path, fps=self.video_loader_config.fps\n                )\n\n            # write out the anatomy present in each frame to a csv for later analysis\n            if self.output_type == DensePoseOutputEnum.chimp_anatomy.value:\n                output_path = output_dir / f\"{fp.stem}_denspose_anatomy.csv\"\n                dpm.anatomize_video(\n                    visualized_video,\n                    labels,\n                    output_path=output_path,\n                    fps=self.video_loader_config.fps,\n                )\n\n    _get_filepaths = root_validator(allow_reuse=True, pre=False, skip_on_failure=True)(\n        get_video_filepaths\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_files(cls, values):\n        # if globbing from data directory, already have valid dataframe\n        if isinstance(values[\"filepaths\"], pd.DataFrame):\n            files_df = values[\"filepaths\"]\n        else:\n            # make into dataframe even if only one column for clearer indexing\n            files_df = pd.DataFrame(pd.read_csv(values[\"filepaths\"]))\n\n        if \"filepath\" not in files_df.columns:\n            raise ValueError(f\"{values['filepaths']} must contain a `filepath` column.\")\n\n        # can only contain one row per filepath\n        duplicated = files_df.filepath.duplicated()\n        if duplicated.sum() &gt; 0:\n            logger.warning(\n                f\"Found {duplicated.sum():,} duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\"\n            )\n            files_df = files_df[[\"filepath\"]].drop_duplicates()\n\n        values[\"filepaths\"] = check_files_exist_and_load(\n            df=files_df,\n            data_dir=values[\"data_dir\"],\n            skip_load_validation=True,\n        )\n        return values\n</code></pre>"},{"location":"api-reference/densepose_config/#zamba.models.densepose.config.DensePoseConfig.run_model","title":"<code>run_model()</code>","text":"<p>Use this configuration to execute DensePose via the DensePoseManager</p> Source code in <code>zamba/models/densepose/config.py</code> <pre><code>def run_model(self):\n    \"\"\"Use this configuration to execute DensePose via the DensePoseManager\"\"\"\n    if not isinstance(self.output_type, DensePoseOutputEnum):\n        self.output_type = DensePoseOutputEnum(self.output_type)\n\n    if self.output_type == DensePoseOutputEnum.segmentation.value:\n        model = MODELS[\"animals\"]\n    elif self.output_type == DensePoseOutputEnum.chimp_anatomy.value:\n        model = MODELS[\"chimps\"]\n    else:\n        raise Exception(f\"invalid {self.output_type}\")\n\n    output_dir = Path(os.getcwd()) if self.save_dir is None else self.save_dir\n\n    dpm = DensePoseManager(\n        model, model_cache_dir=self.cache_dir, download_region=self.weight_download_region\n    )\n\n    for fp in tqdm(self.filepaths.filepath, desc=\"Videos\"):\n        fp = Path(fp)\n\n        vid_arr, labels = dpm.predict_video(fp, video_loader_config=self.video_loader_config)\n\n        # serialize the labels generated by densepose to json\n        output_path = output_dir / f\"{fp.stem}_denspose_labels.json\"\n        dpm.serialize_video_output(\n            labels, filename=output_path, write_embeddings=self.embeddings_in_json\n        )\n\n        # re-render the video with the densepose labels visualized on top of the video\n        if self.render_output:\n            output_path = output_dir / f\"{fp.stem}_denspose_video{''.join(fp.suffixes)}\"\n            visualized_video = dpm.visualize_video(\n                vid_arr, labels, output_path=output_path, fps=self.video_loader_config.fps\n            )\n\n        # write out the anatomy present in each frame to a csv for later analysis\n        if self.output_type == DensePoseOutputEnum.chimp_anatomy.value:\n            output_path = output_dir / f\"{fp.stem}_denspose_anatomy.csv\"\n            dpm.anatomize_video(\n                visualized_video,\n                labels,\n                output_path=output_path,\n                fps=self.video_loader_config.fps,\n            )\n</code></pre>"},{"location":"api-reference/densepose_manager/","title":"zamba.models.densepose.densepose_manager","text":""},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager","title":"<code>DensePoseManager</code>","text":"Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>class DensePoseManager:\n    def __init__(\n        self,\n        model=MODELS[\"chimps\"],\n        model_cache_dir: Optional[Path] = None,\n        download_region=RegionEnum(\"us\"),\n    ):\n        \"\"\"Create a DensePoseManager object.\n\n        Parameters\n        ----------\n        model : dict, optional (default MODELS['chimps'])\n            A dictionary with the densepose model defintion like those defined in MODELS.\n        \"\"\"\n        if not DENSEPOSE_AVAILABLE:\n            raise ImportError(\n                \"Densepose not installed. See: https://zamba.drivendata.org/docs/stable/models/densepose/#installation\"\n            )\n\n        model_cache_dir = model_cache_dir or get_model_cache_dir()\n\n        # setup configuration for densepose\n        self.cfg = get_cfg()\n        add_densepose_config(self.cfg)\n\n        self.cfg.merge_from_file(model[\"config\"])\n\n        if not (model_cache_dir / model[\"weights\"]).exists():\n            logger.info(f\"Available weights: {list(model_cache_dir.glob('*'))}\")\n            logger.info(f\"Downloading weights {model['weights']} to {model_cache_dir}\")\n            model_cache_dir.mkdir(parents=True, exist_ok=True)\n            self.cfg.MODEL.WEIGHTS = download_weights(\n                model[\"weights\"], model_cache_dir, download_region\n            )\n        else:\n            self.cfg.MODEL.WEIGHTS = str(model_cache_dir / model[\"weights\"])\n\n        # automatically use CPU if no cuda available\n        if not torch.cuda.is_available():\n            self.cfg.MODEL.DEVICE = \"cpu\"\n\n        self.cfg.freeze()\n\n        logging.getLogger(\"fvcore\").setLevel(\"CRITICAL\")  # silence noisy detectron2 logging\n        # set up predictor with the configuration\n        self.predictor = DefaultPredictor(self.cfg)\n\n        # we have a specific texture atlas for chimps with relevant regions\n        # labeled that we can use instead of the default segmentation\n        self.visualizer = model[\"viz_class\"](\n            self.cfg,\n            device=self.cfg.MODEL.DEVICE,\n            **model.get(\"viz_class_kwargs\", {}),\n        )\n\n        # set up utilities for use with visualizer\n        self.vis_extractor = create_extractor(self.visualizer)\n        self.vis_embedder = build_densepose_embedder(self.cfg)\n        self.vis_class_to_mesh_name = get_class_to_mesh_name_mapping(self.cfg)\n        self.vis_mesh_vertex_embeddings = {\n            mesh_name: self.vis_embedder(mesh_name).to(self.cfg.MODEL.DEVICE)\n            for mesh_name in self.vis_class_to_mesh_name.values()\n            if self.vis_embedder.has_embeddings(mesh_name)\n        }\n\n        if \"anatomy_color_mapping\" in model:\n            self.anatomy_color_mapping = pd.read_csv(model[\"anatomy_color_mapping\"], index_col=0)\n        else:\n            self.anatomy_color_mapping = None\n\n    def predict_image(self, image):\n        \"\"\"Run inference to get the densepose results for an image.\n\n        Parameters\n        ----------\n        image :\n            numpy array (unit8) of an image in BGR format or path to an image\n\n        Returns\n        -------\n        tuple\n            Returns the image array as passed or loaded and the the densepose Instances as results.\n        \"\"\"\n        if isinstance(image, (str, Path)):\n            image = read_image(image, format=\"BGR\")\n\n        return image, self.predict(image)\n\n    def predict_video(self, video, video_loader_config=None, pbar=True):\n        \"\"\"Run inference to get the densepose results for a video.\n\n        Parameters\n        ----------\n        video :\n            numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video\n        video_loader_config : VideoLoaderConfig, optional\n            A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\")\n        pbar : bool, optional\n            Whether to display a progress bar, by default True\n\n        Returns\n        -------\n        tuple\n            Tuple of (video_array, list of densepose results per frame)\n        \"\"\"\n        if isinstance(video, (str, Path)):\n            video = load_video_frames(video, config=video_loader_config)\n\n        pbar = tqdm if pbar else lambda x, **kwargs: x\n\n        return video, [\n            self.predict_image(img)[1] for img in pbar(video, desc=\"Frames\")\n        ]  # just the predictions\n\n    def predict(self, image_arr):\n        \"\"\"Main call to DensePose for inference. Runs inference on an image array.\n\n        Parameters\n        ----------\n        image_arr : numpy array\n            BGR image array\n\n        Returns\n        -------\n        Instances\n            Detection instances with boxes, scores, and densepose estimates.\n        \"\"\"\n        with torch.no_grad():\n            instances = self.predictor(image_arr)[\"instances\"]\n\n        return instances\n\n    def serialize_video_output(self, instances, filename=None, write_embeddings=False):\n        serialized = {\n            \"frames\": [\n                self.serialize_image_output(\n                    frame_instances, filename=None, write_embeddings=write_embeddings\n                )\n                for frame_instances in instances\n            ]\n        }\n\n        if filename is not None:\n            with Path(filename).open(\"w\") as f:\n                json.dump(serialized, f, indent=2)\n\n        return serialized\n\n    def serialize_image_output(self, instances, filename=None, write_embeddings=False):\n        \"\"\"Convert the densepose output into Python-native objects that can\n            be written and read with json.\n\n        Parameters\n        ----------\n        instances : Instance\n            The output from the densepose model\n        filename : (str, Path), optional\n            If not None, the filename to write the output to, by default None\n        \"\"\"\n        if isinstance(instances, list):\n            img_height, img_width = instances[0].image_size\n        else:\n            img_height, img_width = instances.image_size\n\n        boxes = instances.get(\"pred_boxes\").tensor\n        scores = instances.get(\"scores\").tolist()\n        labels = instances.get(\"pred_classes\").tolist()\n\n        try:\n            pose_result = instances.get(\"pred_densepose\")\n        except KeyError:\n            pose_result = None\n\n        # include embeddings + segmentation if they exist and they are requested\n        write_embeddings = write_embeddings and (pose_result is not None)\n\n        serialized = {\n            \"instances\": [\n                {\n                    \"img_height\": img_height,\n                    \"img_width\": img_width,\n                    \"box\": boxes[i].cpu().tolist(),\n                    \"score\": scores[i],\n                    \"label\": {\n                        \"value\": labels[i],\n                        \"mesh_name\": self.vis_class_to_mesh_name[labels[i]],\n                    },\n                    \"embedding\": (\n                        pose_result.embedding[[i], ...].cpu().tolist()\n                        if write_embeddings\n                        else None\n                    ),\n                    \"segmentation\": (\n                        pose_result.coarse_segm[[i], ...].cpu().tolist()\n                        if write_embeddings\n                        else None\n                    ),\n                }\n                for i in range(len(instances))\n            ]\n        }\n\n        if filename is not None:\n            with Path(filename).open(\"w\") as f:\n                json.dump(serialized, f, indent=2)\n\n        return serialized\n\n    def deserialize_output(self, instances_dict=None, filename=None):\n        if filename is not None:\n            with Path(filename).open(\"r\") as f:\n                instances_dict = json.load(f)\n\n        # handle image case\n        is_image = False\n        if \"frames\" not in instances_dict:\n            instances_dict = {\"frames\": [instances_dict]}\n            is_image = True\n\n        frames = []\n        for frame in instances_dict[\"frames\"]:\n            heights, widths, boxes, scores, labels, embeddings, segmentations = zip(\n                *[\n                    (\n                        i[\"img_height\"],\n                        i[\"img_width\"],\n                        i[\"box\"],\n                        i[\"score\"],\n                        i[\"label\"][\"value\"],\n                        i[\"embedding\"] if i[\"embedding\"] is not None else [np.nan],\n                        i[\"segmentation\"] if i[\"segmentation\"] is not None else [np.nan],\n                    )\n                    for i in frame[\"instances\"]\n                ]\n            )\n\n            frames.append(\n                Instances(\n                    (heights[0], widths[0]),\n                    pred_boxes=boxes,\n                    scores=scores,\n                    pred_classes=labels,\n                    pred_densepose=DensePoseEmbeddingPredictorOutput(\n                        embedding=torch.tensor(embeddings),\n                        coarse_segm=torch.tensor(segmentations),\n                    ),\n                )\n            )\n\n        # if image or single frame, just return the instance\n        if is_image:\n            return frames[0]\n        else:\n            return frames\n\n    def visualize_image(self, image_arr, outputs, output_path=None):\n        \"\"\"Visualize the pose information.\n\n        Parameters\n        ----------\n        image_arr : numpy array (unit8) BGR\n            The numpy array representing the image.\n        outputs :\n            The outputs from running DensePoseManager.predict*\n        output_path : str or Path, optional\n            If not None, write visualization to this path; by default None\n\n        Returns\n        -------\n        numpy array (unit8) BGR\n            DensePose outputs visualized on top of the image.\n        \"\"\"\n        bw_image = cv2.cvtColor(image_arr, cv2.COLOR_BGR2GRAY)\n        bw_image = np.tile(bw_image[:, :, np.newaxis], [1, 1, 3])\n        data = self.vis_extractor(outputs)\n        image_vis = self.visualizer.visualize(bw_image, data)\n\n        if output_path is not None:\n            cv2.imwrite(str(output_path), image_vis)\n\n        return image_vis\n\n    def anatomize_image(self, visualized_img_arr, outputs, output_path=None):\n        \"\"\"Convert the pose information into the percent of pixels in the detection\n            bounding box that correspond to each part of the anatomy in an image.\n\n        Parameters\n        ----------\n        visualized_img_arr : numpy array (unit8) BGR\n            The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image).\n        outputs :\n            The outputs from running DensePoseManager.predict*\n\n        Returns\n        -------\n        pandas.DataFrame\n            DataFrame with percent of pixels of the bounding box that correspond to each anatomical part\n        \"\"\"\n        if self.anatomy_color_mapping is None:\n            raise ValueError(\n                \"No anatomy_color_mapping provided to track anatomy; did you mean to use a different MODEL?\"\n            )\n\n        # no detections, return empty df for joining later (e.g., in anatomize_video)\n        if not outputs:\n            return pd.DataFrame([])\n\n        _, _, N, bboxes_xywh, pred_classes = self.visualizer.extract_and_check_outputs_and_boxes(\n            self.vis_extractor(outputs)\n        )\n\n        all_detections = []\n        for n in range(N):\n            x, y, w, h = bboxes_xywh[n].int().cpu().numpy()\n            detection_area = visualized_img_arr[y : y + h, x : x + w]\n\n            detection_stats = {\n                name: (detection_area == np.array([[[color.B, color.G, color.R]]]))\n                .all(axis=-1)\n                .sum()\n                / (h * w)  # calc percent of bounding box with this color\n                for name, color in self.anatomy_color_mapping.iterrows()\n            }\n\n            detection_stats[\"x\"] = x\n            detection_stats[\"y\"] = y\n            detection_stats[\"h\"] = h\n            detection_stats[\"w\"] = w\n\n            all_detections.append(detection_stats)\n\n        results = pd.DataFrame(all_detections)\n\n        if output_path is not None:\n            results.to_csv(output_path, index=False)\n\n        return results\n\n    def visualize_video(\n        self, video_arr, outputs, output_path=None, frame_size=None, fps=30, pbar=True\n    ):\n        \"\"\"Visualize the pose information on a video\n\n        Parameters\n        ----------\n        video_arr : numpy array (unit8) BGR, time first\n            The numpy array representing the video.\n        outputs :\n            The outputs from running DensePoseManager.predict*\n        output_path : str or Path, optional\n            If not None, write visualization to this path (should be .mp4); by default None\n        frame_size : (innt, float), optional\n            If frame_size is float, scale up or down by that float value; if frame_size is an integer,\n            set width to that size and scale height appropriately.\n        fps : int\n            frames per second for output video if writing; defaults to 30\n        pbar : bool\n            display a progress bar\n\n        Returns\n        -------\n        numpy array (unit8) BGR\n            DensePose outputs visualized on top of the image.\n        \"\"\"\n        pbar = tqdm if pbar else lambda x, **kwargs: x\n\n        out_frames = np.array(\n            [\n                self.visualize_image(\n                    image_arr,\n                    output,\n                )\n                for image_arr, output in pbar(\n                    zip(video_arr, outputs), total=video_arr.shape[0], desc=\"Visualize frames\"\n                )\n            ]\n        )\n\n        if output_path is not None:\n            # get new size for output video if scaling\n            if frame_size is None:\n                frame_size = video_arr.shape[2]  # default to same size\n\n            # if float, scale as a multiple\n            if isinstance(frame_size, float):\n                frame_width = round(video_arr.shape[2] * frame_size)\n                frame_height = round(video_arr.shape[1] * frame_size)\n\n            # if int, use as width of the video and scale height proportionally\n            elif isinstance(frame_size, int):\n                frame_width = frame_size\n                scale = frame_width / video_arr.shape[2]\n                frame_height = round(video_arr.shape[1] * scale)\n\n            # setup output for writing\n            output_path = output_path.with_suffix(\".mp4\")\n            out = cv2.VideoWriter(\n                str(output_path),\n                cv2.VideoWriter_fourcc(*\"mp4v\"),\n                max(1, int(fps)),\n                (frame_width, frame_height),\n            )\n\n            for f in pbar(out_frames, desc=\"Write frames\"):\n                if (f.shape[0] != frame_height) or (f.shape[1] != frame_width):\n                    f = cv2.resize(\n                        f,\n                        (frame_width, frame_height),\n                        # https://stackoverflow.com/a/51042104/1692709\n                        interpolation=(\n                            cv2.INTER_LINEAR if f.shape[1] &lt; frame_width else cv2.INTER_AREA\n                        ),\n                    )\n                out.write(f)\n\n            out.release()\n\n        return out_frames\n\n    def anatomize_video(self, visualized_video_arr, outputs, output_path=None, fps=30):\n        \"\"\"Convert the pose information into the percent of pixels in the detection\n            bounding box that correspond to each part of the anatomy in a video.\n\n        Parameters\n        ----------\n        visualized_video_arr : numpy array (unit8) BGR\n            The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video).\n        outputs :\n            The outputs from running DensePoseManager.predict*\n\n        Returns\n        -------\n        numpy array (unit8) BGR\n            DensePose outputs visualized on top of the image.\n        \"\"\"\n        all_detections = []\n\n        for ix in range(visualized_video_arr.shape[0]):\n            detection_df = self.anatomize_image(visualized_video_arr[ix, ...], outputs[ix])\n            detection_df[\"frame\"] = ix\n            detection_df[\"seconds\"] = ix / fps\n            all_detections.append(detection_df)\n\n        results = pd.concat(all_detections)\n\n        if output_path is not None:\n            results.to_csv(output_path, index=False)\n\n        return results\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.__init__","title":"<code>__init__(model=MODELS['chimps'], model_cache_dir=None, download_region=RegionEnum('us'))</code>","text":"<p>Create a DensePoseManager object.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.__init__--parameters","title":"Parameters","text":"<p>model : dict, optional (default MODELS['chimps'])     A dictionary with the densepose model defintion like those defined in MODELS.</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def __init__(\n    self,\n    model=MODELS[\"chimps\"],\n    model_cache_dir: Optional[Path] = None,\n    download_region=RegionEnum(\"us\"),\n):\n    \"\"\"Create a DensePoseManager object.\n\n    Parameters\n    ----------\n    model : dict, optional (default MODELS['chimps'])\n        A dictionary with the densepose model defintion like those defined in MODELS.\n    \"\"\"\n    if not DENSEPOSE_AVAILABLE:\n        raise ImportError(\n            \"Densepose not installed. See: https://zamba.drivendata.org/docs/stable/models/densepose/#installation\"\n        )\n\n    model_cache_dir = model_cache_dir or get_model_cache_dir()\n\n    # setup configuration for densepose\n    self.cfg = get_cfg()\n    add_densepose_config(self.cfg)\n\n    self.cfg.merge_from_file(model[\"config\"])\n\n    if not (model_cache_dir / model[\"weights\"]).exists():\n        logger.info(f\"Available weights: {list(model_cache_dir.glob('*'))}\")\n        logger.info(f\"Downloading weights {model['weights']} to {model_cache_dir}\")\n        model_cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cfg.MODEL.WEIGHTS = download_weights(\n            model[\"weights\"], model_cache_dir, download_region\n        )\n    else:\n        self.cfg.MODEL.WEIGHTS = str(model_cache_dir / model[\"weights\"])\n\n    # automatically use CPU if no cuda available\n    if not torch.cuda.is_available():\n        self.cfg.MODEL.DEVICE = \"cpu\"\n\n    self.cfg.freeze()\n\n    logging.getLogger(\"fvcore\").setLevel(\"CRITICAL\")  # silence noisy detectron2 logging\n    # set up predictor with the configuration\n    self.predictor = DefaultPredictor(self.cfg)\n\n    # we have a specific texture atlas for chimps with relevant regions\n    # labeled that we can use instead of the default segmentation\n    self.visualizer = model[\"viz_class\"](\n        self.cfg,\n        device=self.cfg.MODEL.DEVICE,\n        **model.get(\"viz_class_kwargs\", {}),\n    )\n\n    # set up utilities for use with visualizer\n    self.vis_extractor = create_extractor(self.visualizer)\n    self.vis_embedder = build_densepose_embedder(self.cfg)\n    self.vis_class_to_mesh_name = get_class_to_mesh_name_mapping(self.cfg)\n    self.vis_mesh_vertex_embeddings = {\n        mesh_name: self.vis_embedder(mesh_name).to(self.cfg.MODEL.DEVICE)\n        for mesh_name in self.vis_class_to_mesh_name.values()\n        if self.vis_embedder.has_embeddings(mesh_name)\n    }\n\n    if \"anatomy_color_mapping\" in model:\n        self.anatomy_color_mapping = pd.read_csv(model[\"anatomy_color_mapping\"], index_col=0)\n    else:\n        self.anatomy_color_mapping = None\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.anatomize_image","title":"<code>anatomize_image(visualized_img_arr, outputs, output_path=None)</code>","text":"<p>Convert the pose information into the percent of pixels in the detection     bounding box that correspond to each part of the anatomy in an image.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.anatomize_image--parameters","title":"Parameters","text":"<p>visualized_img_arr : numpy array (unit8) BGR     The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image). outputs :     The outputs from running DensePoseManager.predict*</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.anatomize_image--returns","title":"Returns","text":"<p>pandas.DataFrame     DataFrame with percent of pixels of the bounding box that correspond to each anatomical part</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def anatomize_image(self, visualized_img_arr, outputs, output_path=None):\n    \"\"\"Convert the pose information into the percent of pixels in the detection\n        bounding box that correspond to each part of the anatomy in an image.\n\n    Parameters\n    ----------\n    visualized_img_arr : numpy array (unit8) BGR\n        The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image).\n    outputs :\n        The outputs from running DensePoseManager.predict*\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with percent of pixels of the bounding box that correspond to each anatomical part\n    \"\"\"\n    if self.anatomy_color_mapping is None:\n        raise ValueError(\n            \"No anatomy_color_mapping provided to track anatomy; did you mean to use a different MODEL?\"\n        )\n\n    # no detections, return empty df for joining later (e.g., in anatomize_video)\n    if not outputs:\n        return pd.DataFrame([])\n\n    _, _, N, bboxes_xywh, pred_classes = self.visualizer.extract_and_check_outputs_and_boxes(\n        self.vis_extractor(outputs)\n    )\n\n    all_detections = []\n    for n in range(N):\n        x, y, w, h = bboxes_xywh[n].int().cpu().numpy()\n        detection_area = visualized_img_arr[y : y + h, x : x + w]\n\n        detection_stats = {\n            name: (detection_area == np.array([[[color.B, color.G, color.R]]]))\n            .all(axis=-1)\n            .sum()\n            / (h * w)  # calc percent of bounding box with this color\n            for name, color in self.anatomy_color_mapping.iterrows()\n        }\n\n        detection_stats[\"x\"] = x\n        detection_stats[\"y\"] = y\n        detection_stats[\"h\"] = h\n        detection_stats[\"w\"] = w\n\n        all_detections.append(detection_stats)\n\n    results = pd.DataFrame(all_detections)\n\n    if output_path is not None:\n        results.to_csv(output_path, index=False)\n\n    return results\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.anatomize_video","title":"<code>anatomize_video(visualized_video_arr, outputs, output_path=None, fps=30)</code>","text":"<p>Convert the pose information into the percent of pixels in the detection     bounding box that correspond to each part of the anatomy in a video.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.anatomize_video--parameters","title":"Parameters","text":"<p>visualized_video_arr : numpy array (unit8) BGR     The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video). outputs :     The outputs from running DensePoseManager.predict*</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.anatomize_video--returns","title":"Returns","text":"<p>numpy array (unit8) BGR     DensePose outputs visualized on top of the image.</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def anatomize_video(self, visualized_video_arr, outputs, output_path=None, fps=30):\n    \"\"\"Convert the pose information into the percent of pixels in the detection\n        bounding box that correspond to each part of the anatomy in a video.\n\n    Parameters\n    ----------\n    visualized_video_arr : numpy array (unit8) BGR\n        The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video).\n    outputs :\n        The outputs from running DensePoseManager.predict*\n\n    Returns\n    -------\n    numpy array (unit8) BGR\n        DensePose outputs visualized on top of the image.\n    \"\"\"\n    all_detections = []\n\n    for ix in range(visualized_video_arr.shape[0]):\n        detection_df = self.anatomize_image(visualized_video_arr[ix, ...], outputs[ix])\n        detection_df[\"frame\"] = ix\n        detection_df[\"seconds\"] = ix / fps\n        all_detections.append(detection_df)\n\n    results = pd.concat(all_detections)\n\n    if output_path is not None:\n        results.to_csv(output_path, index=False)\n\n    return results\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict","title":"<code>predict(image_arr)</code>","text":"<p>Main call to DensePose for inference. Runs inference on an image array.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict--parameters","title":"Parameters","text":"<p>image_arr : numpy array     BGR image array</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict--returns","title":"Returns","text":"<p>Instances     Detection instances with boxes, scores, and densepose estimates.</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def predict(self, image_arr):\n    \"\"\"Main call to DensePose for inference. Runs inference on an image array.\n\n    Parameters\n    ----------\n    image_arr : numpy array\n        BGR image array\n\n    Returns\n    -------\n    Instances\n        Detection instances with boxes, scores, and densepose estimates.\n    \"\"\"\n    with torch.no_grad():\n        instances = self.predictor(image_arr)[\"instances\"]\n\n    return instances\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict_image","title":"<code>predict_image(image)</code>","text":"<p>Run inference to get the densepose results for an image.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict_image--parameters","title":"Parameters","text":"<p>image :     numpy array (unit8) of an image in BGR format or path to an image</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict_image--returns","title":"Returns","text":"<p>tuple     Returns the image array as passed or loaded and the the densepose Instances as results.</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def predict_image(self, image):\n    \"\"\"Run inference to get the densepose results for an image.\n\n    Parameters\n    ----------\n    image :\n        numpy array (unit8) of an image in BGR format or path to an image\n\n    Returns\n    -------\n    tuple\n        Returns the image array as passed or loaded and the the densepose Instances as results.\n    \"\"\"\n    if isinstance(image, (str, Path)):\n        image = read_image(image, format=\"BGR\")\n\n    return image, self.predict(image)\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict_video","title":"<code>predict_video(video, video_loader_config=None, pbar=True)</code>","text":"<p>Run inference to get the densepose results for a video.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict_video--parameters","title":"Parameters","text":"<p>video :     numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video video_loader_config : VideoLoaderConfig, optional     A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\") pbar : bool, optional     Whether to display a progress bar, by default True</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.predict_video--returns","title":"Returns","text":"<p>tuple     Tuple of (video_array, list of densepose results per frame)</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def predict_video(self, video, video_loader_config=None, pbar=True):\n    \"\"\"Run inference to get the densepose results for a video.\n\n    Parameters\n    ----------\n    video :\n        numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video\n    video_loader_config : VideoLoaderConfig, optional\n        A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\")\n    pbar : bool, optional\n        Whether to display a progress bar, by default True\n\n    Returns\n    -------\n    tuple\n        Tuple of (video_array, list of densepose results per frame)\n    \"\"\"\n    if isinstance(video, (str, Path)):\n        video = load_video_frames(video, config=video_loader_config)\n\n    pbar = tqdm if pbar else lambda x, **kwargs: x\n\n    return video, [\n        self.predict_image(img)[1] for img in pbar(video, desc=\"Frames\")\n    ]  # just the predictions\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.serialize_image_output","title":"<code>serialize_image_output(instances, filename=None, write_embeddings=False)</code>","text":"<p>Convert the densepose output into Python-native objects that can     be written and read with json.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.serialize_image_output--parameters","title":"Parameters","text":"<p>instances : Instance     The output from the densepose model filename : (str, Path), optional     If not None, the filename to write the output to, by default None</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def serialize_image_output(self, instances, filename=None, write_embeddings=False):\n    \"\"\"Convert the densepose output into Python-native objects that can\n        be written and read with json.\n\n    Parameters\n    ----------\n    instances : Instance\n        The output from the densepose model\n    filename : (str, Path), optional\n        If not None, the filename to write the output to, by default None\n    \"\"\"\n    if isinstance(instances, list):\n        img_height, img_width = instances[0].image_size\n    else:\n        img_height, img_width = instances.image_size\n\n    boxes = instances.get(\"pred_boxes\").tensor\n    scores = instances.get(\"scores\").tolist()\n    labels = instances.get(\"pred_classes\").tolist()\n\n    try:\n        pose_result = instances.get(\"pred_densepose\")\n    except KeyError:\n        pose_result = None\n\n    # include embeddings + segmentation if they exist and they are requested\n    write_embeddings = write_embeddings and (pose_result is not None)\n\n    serialized = {\n        \"instances\": [\n            {\n                \"img_height\": img_height,\n                \"img_width\": img_width,\n                \"box\": boxes[i].cpu().tolist(),\n                \"score\": scores[i],\n                \"label\": {\n                    \"value\": labels[i],\n                    \"mesh_name\": self.vis_class_to_mesh_name[labels[i]],\n                },\n                \"embedding\": (\n                    pose_result.embedding[[i], ...].cpu().tolist()\n                    if write_embeddings\n                    else None\n                ),\n                \"segmentation\": (\n                    pose_result.coarse_segm[[i], ...].cpu().tolist()\n                    if write_embeddings\n                    else None\n                ),\n            }\n            for i in range(len(instances))\n        ]\n    }\n\n    if filename is not None:\n        with Path(filename).open(\"w\") as f:\n            json.dump(serialized, f, indent=2)\n\n    return serialized\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.visualize_image","title":"<code>visualize_image(image_arr, outputs, output_path=None)</code>","text":"<p>Visualize the pose information.</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.visualize_image--parameters","title":"Parameters","text":"<p>image_arr : numpy array (unit8) BGR     The numpy array representing the image. outputs :     The outputs from running DensePoseManager.predict* output_path : str or Path, optional     If not None, write visualization to this path; by default None</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.visualize_image--returns","title":"Returns","text":"<p>numpy array (unit8) BGR     DensePose outputs visualized on top of the image.</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def visualize_image(self, image_arr, outputs, output_path=None):\n    \"\"\"Visualize the pose information.\n\n    Parameters\n    ----------\n    image_arr : numpy array (unit8) BGR\n        The numpy array representing the image.\n    outputs :\n        The outputs from running DensePoseManager.predict*\n    output_path : str or Path, optional\n        If not None, write visualization to this path; by default None\n\n    Returns\n    -------\n    numpy array (unit8) BGR\n        DensePose outputs visualized on top of the image.\n    \"\"\"\n    bw_image = cv2.cvtColor(image_arr, cv2.COLOR_BGR2GRAY)\n    bw_image = np.tile(bw_image[:, :, np.newaxis], [1, 1, 3])\n    data = self.vis_extractor(outputs)\n    image_vis = self.visualizer.visualize(bw_image, data)\n\n    if output_path is not None:\n        cv2.imwrite(str(output_path), image_vis)\n\n    return image_vis\n</code></pre>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.visualize_video","title":"<code>visualize_video(video_arr, outputs, output_path=None, frame_size=None, fps=30, pbar=True)</code>","text":"<p>Visualize the pose information on a video</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.visualize_video--parameters","title":"Parameters","text":"<p>video_arr : numpy array (unit8) BGR, time first     The numpy array representing the video. outputs :     The outputs from running DensePoseManager.predict* output_path : str or Path, optional     If not None, write visualization to this path (should be .mp4); by default None frame_size : (innt, float), optional     If frame_size is float, scale up or down by that float value; if frame_size is an integer,     set width to that size and scale height appropriately. fps : int     frames per second for output video if writing; defaults to 30 pbar : bool     display a progress bar</p>"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager.DensePoseManager.visualize_video--returns","title":"Returns","text":"<p>numpy array (unit8) BGR     DensePose outputs visualized on top of the image.</p> Source code in <code>zamba/models/densepose/densepose_manager.py</code> <pre><code>def visualize_video(\n    self, video_arr, outputs, output_path=None, frame_size=None, fps=30, pbar=True\n):\n    \"\"\"Visualize the pose information on a video\n\n    Parameters\n    ----------\n    video_arr : numpy array (unit8) BGR, time first\n        The numpy array representing the video.\n    outputs :\n        The outputs from running DensePoseManager.predict*\n    output_path : str or Path, optional\n        If not None, write visualization to this path (should be .mp4); by default None\n    frame_size : (innt, float), optional\n        If frame_size is float, scale up or down by that float value; if frame_size is an integer,\n        set width to that size and scale height appropriately.\n    fps : int\n        frames per second for output video if writing; defaults to 30\n    pbar : bool\n        display a progress bar\n\n    Returns\n    -------\n    numpy array (unit8) BGR\n        DensePose outputs visualized on top of the image.\n    \"\"\"\n    pbar = tqdm if pbar else lambda x, **kwargs: x\n\n    out_frames = np.array(\n        [\n            self.visualize_image(\n                image_arr,\n                output,\n            )\n            for image_arr, output in pbar(\n                zip(video_arr, outputs), total=video_arr.shape[0], desc=\"Visualize frames\"\n            )\n        ]\n    )\n\n    if output_path is not None:\n        # get new size for output video if scaling\n        if frame_size is None:\n            frame_size = video_arr.shape[2]  # default to same size\n\n        # if float, scale as a multiple\n        if isinstance(frame_size, float):\n            frame_width = round(video_arr.shape[2] * frame_size)\n            frame_height = round(video_arr.shape[1] * frame_size)\n\n        # if int, use as width of the video and scale height proportionally\n        elif isinstance(frame_size, int):\n            frame_width = frame_size\n            scale = frame_width / video_arr.shape[2]\n            frame_height = round(video_arr.shape[1] * scale)\n\n        # setup output for writing\n        output_path = output_path.with_suffix(\".mp4\")\n        out = cv2.VideoWriter(\n            str(output_path),\n            cv2.VideoWriter_fourcc(*\"mp4v\"),\n            max(1, int(fps)),\n            (frame_width, frame_height),\n        )\n\n        for f in pbar(out_frames, desc=\"Write frames\"):\n            if (f.shape[0] != frame_height) or (f.shape[1] != frame_width):\n                f = cv2.resize(\n                    f,\n                    (frame_width, frame_height),\n                    # https://stackoverflow.com/a/51042104/1692709\n                    interpolation=(\n                        cv2.INTER_LINEAR if f.shape[1] &lt; frame_width else cv2.INTER_AREA\n                    ),\n                )\n            out.write(f)\n\n        out.release()\n\n    return out_frames\n</code></pre>"},{"location":"api-reference/depth_config/","title":"zamba.models.depth_estimation.config","text":""},{"location":"api-reference/depth_config/#zamba.models.depth_estimation.config.DepthEstimationConfig","title":"<code>DepthEstimationConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration for running depth estimation on videos.</p> <p>Parameters:</p> Name Type Description Default <code>filepaths</code> <code>FilePath</code> <p>Path to a CSV containing videos for inference, with one row per video in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all files in data_dir. Defaults to None.</p> required <code>data_dir</code> <code>DirectoryPath</code> <p>Path to a directory containing videos for inference. Defaults to the working directory.</p> required <code>save_to</code> <code>Path or DirectoryPath</code> <p>Either a filename or a directory in which to save the output csv. If a directory is provided, predictions will be saved to depth_predictions.csv in that directory. Defaults to the working directory.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite output csv path if it exists. Defaults to False.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for inference. Defaults to 64. Note: a batch is a set of frames, not videos, for the depth model.</p> required <code>model_cache_dir</code> <code>Path</code> <p>Path for downloading and saving model weights. Defaults to env var <code>MODEL_CACHE_DIR</code> or the OS app cache dir.</p> required <code>weight_download_region</code> <code>str</code> <p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p> required <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. The maximum value is the number of CPUs in the system. Defaults to 8.</p> required <code>gpus</code> <code>int</code> <p>Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine.</p> required Source code in <code>zamba/models/depth_estimation/config.py</code> <pre><code>class DepthEstimationConfig(ZambaBaseModel):\n    \"\"\"Configuration for running depth estimation on videos.\n\n    Args:\n        filepaths (FilePath, optional): Path to a CSV containing videos for inference, with one row per\n            video in the data_dir. There must be a column called 'filepath' (absolute or\n            relative to the data_dir). If None, uses all files in data_dir. Defaults to None.\n        data_dir (DirectoryPath): Path to a directory containing videos for inference.\n            Defaults to the working directory.\n        save_to (Path or DirectoryPath, optional): Either a filename or a directory in which to\n            save the output csv. If a directory is provided, predictions will be saved to\n            depth_predictions.csv in that directory. Defaults to the working directory.\n        overwrite (bool): If True, overwrite output csv path if it exists. Defaults to False.\n        batch_size (int): Batch size to use for inference. Defaults to 64. Note: a batch is a set\n            of frames, not videos, for the depth model.\n        model_cache_dir (Path, optional): Path for downloading and saving model weights.\n            Defaults to env var `MODEL_CACHE_DIR` or the OS app cache dir.\n        weight_download_region (str): s3 region to download pretrained weights from. Options are\n            \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".\n        num_workers (int): Number of subprocesses to use for data loading. The maximum value is\n           the number of CPUs in the system. Defaults to 8.\n        gpus (int): Number of GPUs to use for inference. Defaults to all of the available GPUs\n            found on the machine.\n    \"\"\"\n\n    filepaths: Optional[Union[FilePath, pd.DataFrame]] = None\n    data_dir: DirectoryPath = \"\"\n    save_to: Optional[Path] = None\n    overwrite: bool = False\n    batch_size: int = 64\n    model_cache_dir: Optional[Path] = None\n    weight_download_region: RegionEnum = RegionEnum(\"us\")\n    num_workers: int = 8\n    gpus: int = GPUS_AVAILABLE\n\n    class Config:\n        # support pandas dataframe\n        arbitrary_types_allowed = True\n\n    def run_model(self):\n        dm = DepthEstimationManager(\n            model_cache_dir=self.model_cache_dir,\n            batch_size=self.batch_size,\n            weight_download_region=self.weight_download_region,\n            num_workers=self.num_workers,\n            gpus=self.gpus,\n        )\n\n        predictions = dm.predict(self.filepaths)\n        self.save_to.parent.mkdir(parents=True, exist_ok=True)\n        predictions.to_csv(self.save_to, index=False)\n        logger.info(f\"Depth predictions saved to {self.save_to}\")\n\n    _get_filepaths = root_validator(allow_reuse=True, pre=False, skip_on_failure=True)(\n        get_video_filepaths\n    )\n\n    _validate_cache_dir = validator(\"model_cache_dir\", allow_reuse=True, always=True)(\n        validate_model_cache_dir\n    )\n\n    _validate_gpus = validator(\"gpus\", allow_reuse=True, pre=True)(validate_gpus)\n\n    @root_validator(skip_on_failure=True)\n    def validate_save_to(cls, values):\n        save_to = values[\"save_to\"]\n        if save_to is None:\n            save_path = Path(os.getcwd()) / \"depth_predictions.csv\"\n        elif save_to.suffix:\n            save_path = save_to\n        else:\n            save_path = save_to / \"depth_predictions.csv\"\n\n        if save_path.exists() and not values[\"overwrite\"]:\n            raise ValueError(\n                f\"{save_path} already exists. If you would like to overwrite, set overwrite=True.\"\n            )\n\n        values[\"save_to\"] = save_path\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_files(cls, values):\n        # if globbing from data directory, already have valid dataframe\n        if isinstance(values[\"filepaths\"], pd.DataFrame):\n            files_df = values[\"filepaths\"]\n        else:\n            # make into dataframe even if only one column for clearer indexing\n            files_df = pd.DataFrame(pd.read_csv(values[\"filepaths\"]))\n\n        if \"filepath\" not in files_df.columns:\n            raise ValueError(f\"{values['filepaths']} must contain a `filepath` column.\")\n\n        # can only contain one row per filepath\n        duplicated = files_df.filepath.duplicated()\n        if duplicated.sum() &gt; 0:\n            logger.warning(\n                f\"Found {duplicated.sum():,} duplicate row(s) in filepaths csv. Dropping duplicates.\"\n            )\n            files_df = files_df[[\"filepath\"]].drop_duplicates()\n\n        values[\"filepaths\"] = check_files_exist_and_load(\n            df=files_df,\n            data_dir=values[\"data_dir\"],\n            skip_load_validation=True,  # just check files exist\n        ).filepath.values.tolist()\n\n        return values\n</code></pre>"},{"location":"api-reference/depth_manager/","title":"zamba.models.depth_estimation.depth_manager","text":""},{"location":"api-reference/depth_manager/#zamba.models.depth_estimation.depth_manager.DepthDataset","title":"<code>DepthDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>zamba/models/depth_estimation/depth_manager.py</code> <pre><code>class DepthDataset(torch.utils.data.Dataset):\n    def __init__(self, filepaths):\n        # these are hardcoded because they depend on the trained model weights used for inference\n        self.height = 270\n        self.width = 480\n        self.channels = 3\n        self.window_size = 2\n        # first frames are swapped; this maintains the bug in the original code\n        self.order = [-1, -2, 0, 1, 2]\n        self.num_frames = self.window_size * 2 + 1\n        self.fps = 1\n\n        mdlite = MegadetectorLiteYoloX()\n        cached_frames = dict()\n        detection_dict = dict()\n\n        transform = depth_transforms(size=(self.height, self.width))\n\n        logger.info(f\"Running object detection on {len(filepaths)} videos.\")\n        for video_filepath in tqdm(filepaths):\n            # get video array at 1 fps, use full size for detecting objects\n            logger.debug(f\"Loading video: {video_filepath}\")\n            try:\n                arr = load_video_frames(video_filepath, fps=self.fps)\n            except:  # noqa: E722\n                logger.warning(f\"Video {video_filepath} could not be loaded. Skipping.\")\n                continue\n\n            # add video entry to cached dict with length (number of seconds since fps=1)\n            cached_frames[video_filepath] = dict(video_length=len(arr))\n\n            # get detections in each frame\n            logger.debug(f\"Detecting video: {video_filepath}\")\n            detections_per_frame = mdlite.detect_video(video_arr=arr)\n\n            # iterate over frames\n            for frame_idx, (detections, scores) in enumerate(detections_per_frame):\n                # if anything is detected in the frame, save out relevant frames\n                if len(detections) &gt; 0:\n                    logger.debug(f\"{len(detections)} detection(s) found at second {frame_idx}.\")\n\n                    # get frame indices around frame with detection\n                    min_frame = frame_idx - self.window_size\n                    max_frame = frame_idx + self.window_size\n\n                    # add relevant resized frames to dict if not already added\n                    # if index is before start or after end of video, use an array of zeros\n                    for i in range(min_frame, max_frame + 1):\n                        if f\"frame_{i}\" not in cached_frames[video_filepath].keys():\n                            try:\n                                selected_frame = arr[i]\n                            except:  # noqa: E722\n                                selected_frame = np.zeros(\n                                    (self.height, self.width, self.channels), dtype=int\n                                )\n\n                            # transform puts channels first and resizes\n                            cached_frames[video_filepath][f\"frame_{i}\"] = transform(\n                                torch.tensor(selected_frame)\n                            ).numpy()\n\n                            del selected_frame\n\n                    # iterate over detections in frame to create universal detection ID\n                    for i, (detection, score) in enumerate(zip(detections, scores)):\n                        universal_det_id = f\"{i}_{frame_idx}_{video_filepath}\"\n\n                        # save out bounding box and score info in case we want to mask out portions\n                        detection_dict[universal_det_id] = dict(\n                            bbox=detection,\n                            score=score,\n                            frame=frame_idx,\n                            video=video_filepath,\n                        )\n\n            del arr\n            del detections_per_frame\n\n        self.detection_dict = detection_dict\n        self.detection_indices = list(detection_dict.keys())\n        self.cached_frames = cached_frames\n\n    def __len__(self):\n        return len(self.detection_indices)\n\n    def __getitem__(self, index):\n        \"\"\"Given a detection index, returns a tuple containing the tensor of stacked frames,\n        video filename, and time into the video for the target frame.\n        \"\"\"\n\n        # get detection info\n        detection_idx = self.detection_indices[index]\n        det_metadata = self.detection_dict[detection_idx]\n        det_frame = det_metadata[\"frame\"]\n        det_video = det_metadata[\"video\"]\n\n        # set up input array of frames within window of detection\n        # frames are stacked channel-wise\n        input = np.concatenate(\n            [self.cached_frames[det_video][f\"frame_{det_frame + i}\"] for i in self.order]\n        )\n\n        # to tensor and normalize\n        tensor = torch.from_numpy(input) / 255.0\n\n        # keep track of video name and time as well\n        return tensor, det_video, det_frame\n</code></pre>"},{"location":"api-reference/depth_manager/#zamba.models.depth_estimation.depth_manager.DepthDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Given a detection index, returns a tuple containing the tensor of stacked frames, video filename, and time into the video for the target frame.</p> Source code in <code>zamba/models/depth_estimation/depth_manager.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"Given a detection index, returns a tuple containing the tensor of stacked frames,\n    video filename, and time into the video for the target frame.\n    \"\"\"\n\n    # get detection info\n    detection_idx = self.detection_indices[index]\n    det_metadata = self.detection_dict[detection_idx]\n    det_frame = det_metadata[\"frame\"]\n    det_video = det_metadata[\"video\"]\n\n    # set up input array of frames within window of detection\n    # frames are stacked channel-wise\n    input = np.concatenate(\n        [self.cached_frames[det_video][f\"frame_{det_frame + i}\"] for i in self.order]\n    )\n\n    # to tensor and normalize\n    tensor = torch.from_numpy(input) / 255.0\n\n    # keep track of video name and time as well\n    return tensor, det_video, det_frame\n</code></pre>"},{"location":"api-reference/depth_manager/#zamba.models.depth_estimation.depth_manager.DepthEstimationManager","title":"<code>DepthEstimationManager</code>","text":"Source code in <code>zamba/models/depth_estimation/depth_manager.py</code> <pre><code>class DepthEstimationManager:\n    def __init__(\n        self,\n        model_cache_dir: Path,\n        gpus: int,\n        weight_download_region: RegionEnum = RegionEnum(\"us\"),\n        batch_size: int = 64,\n        tta: int = 2,\n        num_workers: int = 8,\n    ):\n        \"\"\"Create a depth estimation manager object\n\n        Args:\n            model_cache_dir (Path): Path for downloading and saving model weights.\n            gpus (int): Number of GPUs to use for inference.\n            weight_download_region (str): s3 region to download pretrained weights from.\n                Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific).\n                Defaults to \"us\".\n            batch_size (int, optional): Batch size to use for inference. Defaults to 64.\n                Note: a batch is a set of frames, not videos, for the depth model.\n            tta (int, optional): Number of flips to apply for test time augmentation.\n            num_workers (int): Number of subprocesses to use for data loading. The maximum value is\n                the number of CPUs in the system. Defaults to 8.\n        \"\"\"\n        self.batch_size = batch_size\n        self.tta = tta\n        self.num_workers = num_workers\n        self.gpus = gpus\n\n        model = MODELS[\"depth\"]\n\n        self.model_weights = model_cache_dir / model[\"weights\"]\n        if not self.model_weights.exists():\n            model_cache_dir.mkdir(parents=True, exist_ok=True)\n            self.model_weights = download_weights(\n                model[\"weights\"], model_cache_dir, weight_download_region\n            )\n\n        if self.gpus &gt; 0:\n            self.device = \"cuda\"\n        else:\n            self.device = \"cpu\"\n\n    def predict(self, filepaths):\n        \"\"\"Generate predictions for a list of video filepaths.\"\"\"\n\n        # load model\n        model = torch.jit.load(self.model_weights, map_location=self.device).eval()\n\n        # load dataset\n        test_dataset = DepthDataset(filepaths)\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            sampler=None,\n            collate_fn=None,\n            num_workers=self.num_workers,\n            pin_memory=False,\n            persistent_workers=True,\n        )\n\n        logger.info(\"Generating depth predictions for detected animals.\")\n        predictions = []\n        with torch.no_grad():\n            with tqdm(test_loader) as pbar:\n                distance: torch.Tensor = torch.zeros(self.batch_size, device=self.device)\n                for image, filepath, time in pbar:\n                    bs = image.size(0)\n                    image = image.to(self.device, non_blocking=True)\n\n                    distance.zero_()\n                    logits = model(image)\n                    logits = logits.squeeze(1)\n                    distance[:bs] += logits\n\n                    if self.tta &gt; 1:\n                        logits = model(torch.flip(image, dims=[-1]))\n                        logits = logits.squeeze(1)\n                        distance[:bs] += logits\n\n                    distance /= self.tta\n\n                    time = time.numpy()\n\n                    for d, vid, t in zip(distance.cpu().numpy(), filepath, time):\n                        predictions.append((vid, t, d))\n\n        predictions = pd.DataFrame(\n            predictions,\n            columns=[\"filepath\", \"time\", \"distance\"],\n        ).round(\n            {\"distance\": 1}\n        )  # round to useful number of decimal places\n\n        logger.info(\"Processing output.\")\n        # post process to add nans for frames where no animal was detected\n        videos = list(test_dataset.cached_frames.keys())\n        lengths = [np.arange(test_dataset.cached_frames[v][\"video_length\"]) for v in videos]\n\n        # create one row per frame for duration of video\n        output = pd.Series(index=videos, data=lengths).explode().to_frame().reset_index()\n        output.columns = [\"filepath\", \"time\"]\n\n        # merge in predictions\n        if len(predictions) &gt; 0:\n            output = output.merge(predictions, on=[\"filepath\", \"time\"], how=\"outer\")\n        else:\n            # create empty distance column\n            output[\"distance\"] = np.nan\n        return output\n</code></pre>"},{"location":"api-reference/depth_manager/#zamba.models.depth_estimation.depth_manager.DepthEstimationManager.__init__","title":"<code>__init__(model_cache_dir, gpus, weight_download_region=RegionEnum('us'), batch_size=64, tta=2, num_workers=8)</code>","text":"<p>Create a depth estimation manager object</p> <p>Parameters:</p> Name Type Description Default <code>model_cache_dir</code> <code>Path</code> <p>Path for downloading and saving model weights.</p> required <code>gpus</code> <code>int</code> <p>Number of GPUs to use for inference.</p> required <code>weight_download_region</code> <code>str</code> <p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p> <code>RegionEnum('us')</code> <code>batch_size</code> <code>int</code> <p>Batch size to use for inference. Defaults to 64. Note: a batch is a set of frames, not videos, for the depth model.</p> <code>64</code> <code>tta</code> <code>int</code> <p>Number of flips to apply for test time augmentation.</p> <code>2</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. The maximum value is the number of CPUs in the system. Defaults to 8.</p> <code>8</code> Source code in <code>zamba/models/depth_estimation/depth_manager.py</code> <pre><code>def __init__(\n    self,\n    model_cache_dir: Path,\n    gpus: int,\n    weight_download_region: RegionEnum = RegionEnum(\"us\"),\n    batch_size: int = 64,\n    tta: int = 2,\n    num_workers: int = 8,\n):\n    \"\"\"Create a depth estimation manager object\n\n    Args:\n        model_cache_dir (Path): Path for downloading and saving model weights.\n        gpus (int): Number of GPUs to use for inference.\n        weight_download_region (str): s3 region to download pretrained weights from.\n            Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific).\n            Defaults to \"us\".\n        batch_size (int, optional): Batch size to use for inference. Defaults to 64.\n            Note: a batch is a set of frames, not videos, for the depth model.\n        tta (int, optional): Number of flips to apply for test time augmentation.\n        num_workers (int): Number of subprocesses to use for data loading. The maximum value is\n            the number of CPUs in the system. Defaults to 8.\n    \"\"\"\n    self.batch_size = batch_size\n    self.tta = tta\n    self.num_workers = num_workers\n    self.gpus = gpus\n\n    model = MODELS[\"depth\"]\n\n    self.model_weights = model_cache_dir / model[\"weights\"]\n    if not self.model_weights.exists():\n        model_cache_dir.mkdir(parents=True, exist_ok=True)\n        self.model_weights = download_weights(\n            model[\"weights\"], model_cache_dir, weight_download_region\n        )\n\n    if self.gpus &gt; 0:\n        self.device = \"cuda\"\n    else:\n        self.device = \"cpu\"\n</code></pre>"},{"location":"api-reference/depth_manager/#zamba.models.depth_estimation.depth_manager.DepthEstimationManager.predict","title":"<code>predict(filepaths)</code>","text":"<p>Generate predictions for a list of video filepaths.</p> Source code in <code>zamba/models/depth_estimation/depth_manager.py</code> <pre><code>def predict(self, filepaths):\n    \"\"\"Generate predictions for a list of video filepaths.\"\"\"\n\n    # load model\n    model = torch.jit.load(self.model_weights, map_location=self.device).eval()\n\n    # load dataset\n    test_dataset = DepthDataset(filepaths)\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        sampler=None,\n        collate_fn=None,\n        num_workers=self.num_workers,\n        pin_memory=False,\n        persistent_workers=True,\n    )\n\n    logger.info(\"Generating depth predictions for detected animals.\")\n    predictions = []\n    with torch.no_grad():\n        with tqdm(test_loader) as pbar:\n            distance: torch.Tensor = torch.zeros(self.batch_size, device=self.device)\n            for image, filepath, time in pbar:\n                bs = image.size(0)\n                image = image.to(self.device, non_blocking=True)\n\n                distance.zero_()\n                logits = model(image)\n                logits = logits.squeeze(1)\n                distance[:bs] += logits\n\n                if self.tta &gt; 1:\n                    logits = model(torch.flip(image, dims=[-1]))\n                    logits = logits.squeeze(1)\n                    distance[:bs] += logits\n\n                distance /= self.tta\n\n                time = time.numpy()\n\n                for d, vid, t in zip(distance.cpu().numpy(), filepath, time):\n                    predictions.append((vid, t, d))\n\n    predictions = pd.DataFrame(\n        predictions,\n        columns=[\"filepath\", \"time\", \"distance\"],\n    ).round(\n        {\"distance\": 1}\n    )  # round to useful number of decimal places\n\n    logger.info(\"Processing output.\")\n    # post process to add nans for frames where no animal was detected\n    videos = list(test_dataset.cached_frames.keys())\n    lengths = [np.arange(test_dataset.cached_frames[v][\"video_length\"]) for v in videos]\n\n    # create one row per frame for duration of video\n    output = pd.Series(index=videos, data=lengths).explode().to_frame().reset_index()\n    output.columns = [\"filepath\", \"time\"]\n\n    # merge in predictions\n    if len(predictions) &gt; 0:\n        output = output.merge(predictions, on=[\"filepath\", \"time\"], how=\"outer\")\n    else:\n        # create empty distance column\n        output[\"distance\"] = np.nan\n    return output\n</code></pre>"},{"location":"api-reference/exceptions/","title":"zamba.exceptions","text":""},{"location":"api-reference/images-classifier/","title":"zamba.images.classifier","text":""},{"location":"api-reference/images-config/","title":"zamba.images.config","text":""},{"location":"api-reference/images-config/#zamba.images.config.ImageClassificationPredictConfig","title":"<code>ImageClassificationPredictConfig</code>","text":"<p>               Bases: <code>ZambaImageConfig</code></p> <p>Configuration for using an image classification model for inference.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>DirectoryPath</code> <p>Path to a directory containing images for inference. Defaults to the current working directory.</p> required <code>filepaths</code> <code>FilePath</code> <p>Path to a CSV containing images for inference, with one row per image in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all image files in data_dir. Defaults to None.</p> required <code>checkpoint</code> <code>FilePath</code> <p>Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to generate predictions. If None, defaults to a pretrained model. Defaults to None.</p> required <code>model_name</code> <code>str</code> <p>Name of the model to use for inference. Currently supports 'lila.science'. Defaults to 'lila.science'.</p> required <code>save</code> <code>bool</code> <p>Whether to save out predictions. If False, predictions are not saved. Defaults to True.</p> required <code>save_dir</code> <code>Path</code> <p>An optional directory in which to save the model  predictions and configuration yaml. If no save_dir is specified and save=True,  outputs will be written to the current working directory. Defaults to None.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite outputs in save_dir if they exist. Defaults to False.</p> required <code>crop_images</code> <code>bool</code> <p>Whether to preprocess images using Megadetector or bounding boxes from labels file. Focuses the model on regions of interest. Defaults to True.</p> required <code>detections_threshold</code> <code>float</code> <p>Confidence threshold for Megadetector detections. Only applied when crop_images=True. Defaults to 0.2.</p> required <code>gpus</code> <code>int</code> <p>Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine.</p> required <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. Defaults to 3.</p> required <code>image_size</code> <code>int</code> <p>Image size (height and width) for the input to the classification model. Defaults to 224.</p> required <code>results_file_format</code> <code>ResultsFormat</code> <p>The format in which to output the predictions. Currently 'csv' and 'megadetector' JSON formats are supported. Default is 'csv'.</p> required <code>results_file_name</code> <code>Path</code> <p>The filename for the output predictions in the save directory. Defaults to 'zamba_predictions.csv' or 'zamba_predictions.json' depending on results_file_format.</p> required <code>model_cache_dir</code> <code>Path</code> <p>Cache directory where downloaded model weights will be saved. If None and no environment variable is set, will use your default cache directory. Defaults to None.</p> required <code>weight_download_region</code> <code>str</code> <p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p> required Source code in <code>zamba/images/config.py</code> <pre><code>class ImageClassificationPredictConfig(ZambaImageConfig):\n    \"\"\"\n    Configuration for using an image classification model for inference.\n\n    Args:\n        data_dir (DirectoryPath): Path to a directory containing images for\n            inference. Defaults to the current working directory.\n        filepaths (FilePath, optional): Path to a CSV containing images for inference, with\n            one row per image in the data_dir. There must be a column called\n            'filepath' (absolute or relative to the data_dir). If None, uses\n            all image files in data_dir. Defaults to None.\n        checkpoint (FilePath, optional): Path to a custom checkpoint file (.ckpt)\n            generated by zamba that can be used to generate predictions. If None,\n            defaults to a pretrained model. Defaults to None.\n        model_name (str, optional): Name of the model to use for inference. Currently\n            supports 'lila.science'. Defaults to 'lila.science'.\n        save (bool): Whether to save out predictions. If False, predictions are\n            not saved. Defaults to True.\n        save_dir (Path, optional): An optional directory in which to save the model\n             predictions and configuration yaml. If no save_dir is specified and save=True,\n             outputs will be written to the current working directory. Defaults to None.\n        overwrite (bool): If True, overwrite outputs in save_dir if they exist.\n            Defaults to False.\n        crop_images (bool): Whether to preprocess images using Megadetector or bounding boxes\n            from labels file. Focuses the model on regions of interest. Defaults to True.\n        detections_threshold (float): Confidence threshold for Megadetector detections.\n            Only applied when crop_images=True. Defaults to 0.2.\n        gpus (int): Number of GPUs to use for inference.\n            Defaults to all of the available GPUs found on the machine.\n        num_workers (int): Number of subprocesses to use for data loading.\n            Defaults to 3.\n        image_size (int, optional): Image size (height and width) for the input to the\n            classification model. Defaults to 224.\n        results_file_format (ResultsFormat): The format in which to output the predictions.\n            Currently 'csv' and 'megadetector' JSON formats are supported. Default is 'csv'.\n        results_file_name (Path, optional): The filename for the output predictions in the\n            save directory. Defaults to 'zamba_predictions.csv' or 'zamba_predictions.json'\n            depending on results_file_format.\n        model_cache_dir (Path, optional): Cache directory where downloaded model weights\n            will be saved. If None and no environment variable is set, will use your\n            default cache directory. Defaults to None.\n        weight_download_region (str): s3 region to download pretrained weights from.\n            Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific).\n            Defaults to \"us\".\n    \"\"\"\n\n    checkpoint: Optional[FilePath] = None\n    model_name: Optional[str] = ImageModelEnum.LILA_SCIENCE.value\n    filepaths: Optional[Union[FilePath, pd.DataFrame]] = None\n    data_dir: DirectoryPath\n    save: bool = True\n    overwrite: bool = False\n    crop_images: bool = True\n    detections_threshold: float = 0.2\n    gpus: int = GPUS_AVAILABLE\n    num_workers: int = 3\n    image_size: Optional[int] = 224\n    results_file_format: ResultsFormat = ResultsFormat.CSV\n    results_file_name: Optional[Path] = Path(\"zamba_predictions.csv\")\n    model_cache_dir: Optional[Path] = None\n    weight_download_region: str = RegionEnum.us.value\n\n    class Config:  # type: ignore\n        arbitrary_types_allowed = True\n\n    _validate_model_cache_dir = validator(\"model_cache_dir\", allow_reuse=True, always=True)(\n        validate_model_cache_dir\n    )\n\n    _get_filepaths = root_validator(allow_reuse=True, pre=False, skip_on_failure=True)(\n        get_image_filepaths\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_save_dir(cls, values):\n        save_dir = values[\"save_dir\"]\n        results_file_name = values[\"results_file_name\"]\n        save = values[\"save\"]\n\n        # if no save_dir but save is True, use current working directory\n        if save_dir is None and save:\n            save_dir = Path.cwd()\n\n        if save_dir is not None:\n            # check if files exist\n            save_path = save_dir / results_file_name\n            if values[\"results_file_format\"] == ResultsFormat.MEGADETECTOR:\n                save_path = save_path.with_suffix(\".json\")\n            if save_path.exists() and not values[\"overwrite\"]:\n                raise ValueError(\n                    f\"{save_path.name} already exists in {save_dir}. If you would like to overwrite, set overwrite=True\"\n                )\n\n            # make a directory if needed\n            save_dir.mkdir(parents=True, exist_ok=True)\n\n            # set save to True if save_dir is set\n            if not save:\n                save = True\n\n        values[\"save_dir\"] = save_dir\n        values[\"save\"] = save\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_filepaths(cls, values):\n        if isinstance(values[\"filepaths\"], pd.DataFrame):\n            files_df = values[\"filepaths\"]\n        else:\n            files_df = pd.DataFrame(pd.read_csv(values[\"filepaths\"]))\n\n        if \"filepath\" not in files_df.columns:\n            raise ValueError(f\"{values['filepath']} must contain a `filepath` column.\")\n        else:\n            files_df = files_df[[\"filepath\"]]\n\n        duplicated = files_df.filepath.duplicated()\n        if duplicated.sum() &gt; 0:\n            logger.warning(\n                f\"Found {duplicated.sum():,} duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\"\n            )\n            files_df = files_df[[\"filepath\"]].drop_duplicates()\n\n        # The filepath column can come in as a str or a Path-like and either absolute\n        # or relative to the data directory. Handle all those cases.\n        filepaths = []\n        for path in files_df[\"filepath\"]:\n            path = Path(path)\n            if not path.is_absolute():\n                # Assume relative to data directory\n                path = values[\"data_dir\"] / path\n            filepaths.append(str(path))\n        files_df[\"filepath\"] = filepaths\n        values[\"filepaths\"] = files_df\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_detections_threshold(cls, values):\n        threshold = values[\"detections_threshold\"]\n\n        if threshold &lt;= 0 or threshold &gt;= 1:\n            raise ValueError(\n                \"Detections threshold value should be greater than zero and less than one.\"\n            )\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_image_size(cls, values):\n        if values[\"image_size\"] &lt;= 0:\n            raise ValueError(\"Image size should be greater than or equal 64\")\n        return values\n\n    _validate_model_name_and_checkpoint = root_validator(allow_reuse=True, skip_on_failure=True)(\n        validate_model_name_and_checkpoint\n    )\n</code></pre>"},{"location":"api-reference/images-config/#zamba.images.config.ImageClassificationTrainingConfig","title":"<code>ImageClassificationTrainingConfig</code>","text":"<p>               Bases: <code>ZambaImageConfig</code></p> <p>Configuration for running image classification training.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Path to directory containing the training images.</p> required <code>labels</code> <code>Union[FilePath, DataFrame]</code> <p>Path to a CSV or JSON file with labels, or a pandas DataFrame. For CSV files, must contain 'filepath' and 'label' columns. For JSON files, must be in COCO or other supported format as specified by labels_format.</p> required <code>labels_format</code> <code>BboxFormat</code> <p>Format for bounding box annotations when labels are provided as JSON. Options are defined in the BboxFormat enum. Defaults to BboxFormat.COCO.</p> required <code>checkpoint</code> <code>FilePath</code> <p>Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to resume training. If None and from_scratch=False, will load a pretrained model. Defaults to None.</p> required <code>model_name</code> <code>str</code> <p>Name of the model to use. Currently supports 'lila.science'. Defaults to 'lila.science'.</p> required <code>name</code> <code>str</code> <p>Classification experiment name used for MLFlow tracking. Defaults to 'image-classification'.</p> required <code>max_epochs</code> <code>int</code> <p>Maximum number of training epochs. Defaults to 100.</p> required <code>lr</code> <code>float</code> <p>Learning rate. If None, will attempt to find a good learning rate. Defaults to None.</p> required <code>image_size</code> <code>int</code> <p>Input image size (height and width) for the model. Defaults to 224.</p> required <code>batch_size</code> <code>int</code> <p>Physical batch size for training. Defaults to 16.</p> required <code>accumulated_batch_size</code> <code>int</code> <p>Virtual batch size for gradient accumulation. Useful to match batch sizes from published papers with constrained GPU memory. If None, uses batch_size. Defaults to None.</p> required <code>early_stopping_patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped. Defaults to 3.</p> required <code>extra_train_augmentations</code> <code>bool</code> <p>Whether to use additional image augmentations. If false, uses simple transforms for camera trap imagery (random perspective shift, random horizontal flip, random rotation). If True, adds complex transforms beyond the basic set (random grayscale, equalize, etc.). Defaults to False.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 2/3 of available CPU cores.</p> required <code>accelerator</code> <code>str</code> <p>PyTorch Lightning accelerator type ('gpu' or 'cpu'). Defaults to 'gpu' if CUDA is available, otherwise 'cpu'.</p> required <code>devices</code> <code>Any</code> <p>Which devices to use for training. Can be int, list of ints, or 'auto'. Defaults to 'auto'.</p> required <code>crop_images</code> <code>bool</code> <p>Whether to preprocess images using Megadetector or bounding boxes from labels. Defaults to True.</p> required <code>detections_threshold</code> <code>float</code> <p>Confidence threshold for Megadetector. Only used when crop_images=True and no bounding boxes are provided in labels. Defaults to 0.2.</p> required <code>checkpoint_path</code> <code>Path</code> <p>Directory where training outputs will be saved. Defaults to current working directory.</p> required <code>weighted_loss</code> <code>bool</code> <p>Whether to use class-weighted loss during training. Helpful for imbalanced datasets. Defaults to False.</p> required <code>mlflow_tracking_uri</code> <code>str</code> <p>URI for MLFlow tracking server. Defaults to './mlruns'.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to train the model from scratch (base weights) instead of using a pretrained checkpoint. Defaults to False.</p> required <code>use_default_model_labels</code> <code>bool</code> <p>Whether to use the full set of default model labels or only the labels in the provided dataset. If set to False, will replace the model head for finetuning and output only the species in the provided labels file. If None, automatically determined based on the labels provided.</p> required <code>scheduler_config</code> <code>Union[str, SchedulerConfig]</code> <p>Learning rate scheduler configuration. If 'default', uses the scheduler from original training. Defaults to 'default'.</p> required <code>split_proportions</code> <code>Dict[str, int]</code> <p>Proportions for train/val/test splits if no split column is provided in labels. Defaults to {'train': 3, 'val': 1, 'test': 1}.</p> required <code>model_cache_dir</code> <code>Path</code> <p>Directory where downloaded model weights will be cached. If None, uses the system's default cache directory. Defaults to None.</p> required <code>cache_dir</code> <code>Path</code> <p>Directory where cropped/processed images will be cached. Defaults to a 'image_cache' subdirectory in the system's cache directory.</p> required <code>weight_download_region</code> <code>str</code> <p>S3 region for downloading pretrained weights. Options are 'us', 'eu', or 'asia'. Defaults to 'us'.</p> required <code>species_in_label_order</code> <code>list</code> <p>Optional list to specify the order of species labels in the model output. Defaults to None.</p> required Source code in <code>zamba/images/config.py</code> <pre><code>class ImageClassificationTrainingConfig(ZambaImageConfig):\n    \"\"\"Configuration for running image classification training.\n\n    Args:\n        data_dir (Path): Path to directory containing the training images.\n        labels (Union[FilePath, pd.DataFrame]): Path to a CSV or JSON file with labels, or a pandas DataFrame.\n            For CSV files, must contain 'filepath' and 'label' columns.\n            For JSON files, must be in COCO or other supported format as specified by labels_format.\n        labels_format (BboxFormat): Format for bounding box annotations when labels are provided as JSON.\n            Options are defined in the BboxFormat enum. Defaults to BboxFormat.COCO.\n        checkpoint (FilePath, optional): Path to a custom checkpoint file (.ckpt) generated by zamba\n            that can be used to resume training. If None and from_scratch=False, will load a pretrained model.\n            Defaults to None.\n        model_name (str, optional): Name of the model to use. Currently supports 'lila.science'.\n            Defaults to 'lila.science'.\n        name (str, optional): Classification experiment name used for MLFlow tracking.\n            Defaults to 'image-classification'.\n        max_epochs (int): Maximum number of training epochs. Defaults to 100.\n        lr (float, optional): Learning rate. If None, will attempt to find a good learning rate.\n            Defaults to None.\n        image_size (int): Input image size (height and width) for the model. Defaults to 224.\n        batch_size (int, optional): Physical batch size for training. Defaults to 16.\n        accumulated_batch_size (int, optional): Virtual batch size for gradient accumulation.\n            Useful to match batch sizes from published papers with constrained GPU memory.\n            If None, uses batch_size. Defaults to None.\n        early_stopping_patience (int): Number of epochs with no improvement after which training\n            will be stopped. Defaults to 3.\n        extra_train_augmentations (bool): Whether to use additional image augmentations.\n            If false, uses simple transforms for camera trap imagery (random perspective shift,\n            random horizontal flip, random rotation).\n            If True, adds complex transforms beyond the basic set (random grayscale, equalize, etc.).\n            Defaults to False.\n        num_workers (int): Number of workers for data loading. Defaults to 2/3 of available CPU cores.\n        accelerator (str): PyTorch Lightning accelerator type ('gpu' or 'cpu').\n            Defaults to 'gpu' if CUDA is available, otherwise 'cpu'.\n        devices (Any): Which devices to use for training. Can be int, list of ints, or 'auto'.\n            Defaults to 'auto'.\n        crop_images (bool): Whether to preprocess images using Megadetector or bounding boxes\n            from labels. Defaults to True.\n        detections_threshold (float): Confidence threshold for Megadetector.\n            Only used when crop_images=True and no bounding boxes are provided in labels.\n            Defaults to 0.2.\n        checkpoint_path (Path): Directory where training outputs will be saved.\n            Defaults to current working directory.\n        weighted_loss (bool): Whether to use class-weighted loss during training.\n            Helpful for imbalanced datasets. Defaults to False.\n        mlflow_tracking_uri (str, optional): URI for MLFlow tracking server.\n            Defaults to './mlruns'.\n        from_scratch (bool): Whether to train the model from scratch (base weights)\n            instead of using a pretrained checkpoint. Defaults to False.\n        use_default_model_labels (bool, optional): Whether to use the full set of default model\n            labels or only the labels in the provided dataset.\n            If set to False, will replace the model head for finetuning and output only\n            the species in the provided labels file.\n            If None, automatically determined based on the labels provided.\n        scheduler_config (Union[str, SchedulerConfig], optional): Learning rate scheduler\n            configuration. If 'default', uses the scheduler from original training.\n            Defaults to 'default'.\n        split_proportions (Dict[str, int], optional): Proportions for train/val/test splits\n            if no split column is provided in labels. Defaults to {'train': 3, 'val': 1, 'test': 1}.\n        model_cache_dir (Path, optional): Directory where downloaded model weights will be cached.\n            If None, uses the system's default cache directory. Defaults to None.\n        cache_dir (Path, optional): Directory where cropped/processed images will be cached.\n            Defaults to a 'image_cache' subdirectory in the system's cache directory.\n        weight_download_region (str): S3 region for downloading pretrained weights.\n            Options are 'us', 'eu', or 'asia'. Defaults to 'us'.\n        species_in_label_order (list, optional): Optional list to specify the order of\n            species labels in the model output. Defaults to None.\n    \"\"\"\n\n    data_dir: Path\n    labels: Union[FilePath, pd.DataFrame]\n    labels_format: BboxInputFormat = BboxInputFormat.COCO\n    checkpoint: Optional[FilePath] = None\n    model_name: Optional[str] = ImageModelEnum.LILA_SCIENCE.value\n    name: Optional[str] = \"image-classification\"\n    max_epochs: int = 100\n    lr: Optional[float] = None  # if None, will find a good learning rate\n    image_size: int = 224\n    batch_size: Optional[int] = 16\n    accumulated_batch_size: Optional[int] = None\n    early_stopping_patience: int = 3\n    extra_train_augmentations: bool = False\n    num_workers: int = int(os.cpu_count() // 1.5)  # default use 2/3 of available cores\n    accelerator: str = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n    devices: Any = \"auto\"\n    crop_images: bool = True\n    detections_threshold: float = 0.2\n    checkpoint_path: Path = Path.cwd()\n    weighted_loss: bool = False\n    mlflow_tracking_uri: Optional[str] = \"./mlruns\"\n    from_scratch: Optional[bool] = False\n    use_default_model_labels: Optional[bool] = None\n    scheduler_config: Optional[Union[str, SchedulerConfig]] = \"default\"\n    split_proportions: Optional[Dict[str, int]] = {\"train\": 3, \"val\": 1, \"test\": 1}\n    model_cache_dir: Optional[Path] = None\n    cache_dir: Optional[Path] = Path(appdirs.user_cache_dir()) / \"zamba\" / \"image_cache\"\n    weight_download_region: str = RegionEnum.us.value\n    species_in_label_order: Optional[list] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    _validate_model_cache_dir = validator(\"model_cache_dir\", allow_reuse=True, always=True)(\n        validate_model_cache_dir\n    )\n\n    @staticmethod\n    def process_json_annotations(labels, labels_format: BboxInputFormat) -&gt; pd.DataFrame:\n        return bbox_json_to_df(labels, bbox_format=labels_format)\n\n    @root_validator(skip_on_failure=True)\n    def process_cache_dir(cls, values):\n        cache_dir = values[\"cache_dir\"]\n        if not cache_dir.exists():\n            cache_dir.mkdir(parents=True)\n            logger.info(\"Cache dir created.\")\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_labels(cls, values):\n        \"\"\"Validate and load labels\"\"\"\n        logger.info(\"Validating labels\")\n\n        if isinstance(values[\"labels\"], pd.DataFrame):\n            pass\n        elif values[\"labels\"].suffix == \".json\":\n            with open(values[\"labels\"], \"r\") as f:\n                values[\"labels\"] = cls.process_json_annotations(\n                    json.load(f), values[\"labels_format\"]\n                )\n        else:\n            values[\"labels\"] = pd.read_csv(values[\"labels\"])\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_devices(cls, values):\n        # per pytorch lightning docs, should be int or list of ints\n        # https://lightning.ai/docs/pytorch/stable/common/trainer.html#devices\n        raw_val = values[\"devices\"]\n        if \",\" in raw_val:\n            values[\"devices\"] = [int(v) for v in raw_val]\n        elif raw_val == \"auto\":\n            pass\n        else:\n            values[\"devices\"] = int(raw_val)\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_data_dir(cls, values):\n        if not os.path.exists(values[\"data_dir\"]):\n            raise ValueError(\"Data dir doesn't exist.\")\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_image_files(cls, values):\n        \"\"\"Validate and load image files.\"\"\"\n        logger.info(\"Validating image files exist\")\n\n        exists = process_map(\n            cls._validate_filepath,\n            (values[\"data_dir\"] / values[\"labels\"].filepath.path).items(),\n            chunksize=max(\n                1, len(values[\"labels\"]) // 1000\n            ),  # chunks can be large; should be fast operation\n            total=len(values[\"labels\"]),\n        )\n\n        file_existence = pd.DataFrame(exists).set_index(0)\n        exists = file_existence[2]\n\n        if not exists.all():\n            missing_files = file_existence[~exists]\n            example_missing = [str(f) for f in missing_files.head(3)[1].values]\n            logger.warning(\n                f\"{(~exists).sum()} files in provided labels file do not exist on disk; ignoring those files. Example: {example_missing}...\"\n            )\n\n        values[\"labels\"] = values[\"labels\"][exists]\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def preprocess_labels(cls, values):\n        \"\"\"One hot encode and add splits.\"\"\"\n        logger.info(\"Preprocessing labels.\")\n        labels = values[\"labels\"]\n\n        # lowercase to facilitate subset checking\n        labels[\"label\"] = labels.label.str.lower()\n\n        # one hot encoding\n        labels = pd.get_dummies(labels.rename(columns={\"label\": \"species\"}), columns=[\"species\"])\n\n        # We validate that all the images exist prior to this, so once this assembles the set of classes,\n        # we should have at least one example of each label and don't need to worry about filtering out classes\n        # with missing examples.\n        species_columns = labels.columns[labels.columns.str.contains(\"species_\")]\n        values[\"species_in_label_order\"] = species_columns.to_list()\n\n        indices = (\n            labels[species_columns].idxmax(axis=1).apply(lambda x: species_columns.get_loc(x))\n        )\n\n        labels[\"label\"] = indices\n\n        # if no \"split\" column, set up train, val, and test split\n        if \"split\" not in labels.columns:\n            make_split(labels, values)\n\n        values[\"labels\"] = labels.reset_index()\n\n        example_species = [\n            species.replace(\"species_\", \"\") for species in values[\"species_in_label_order\"][:3]\n        ]\n        logger.info(\n            f\"Labels preprocessed. {len(values['species_in_label_order'])} species found: {example_species}...\"\n        )\n        return values\n\n    _validate_model_name_and_checkpoint = root_validator(allow_reuse=True, skip_on_failure=True)(\n        validate_model_name_and_checkpoint\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_from_scratch(cls, values):\n        from_scratch = values[\"from_scratch\"]\n        model_checkpoint = values[\"checkpoint\"]\n        if (from_scratch is False or from_scratch is False) and model_checkpoint is None:\n            raise ValueError(\n                \"You must specify checkpoint if you don't want to start training from scratch.\"\n            )\n        return values\n\n    @staticmethod\n    def _validate_filepath(ix_path):\n        ix, path = ix_path\n        path = Path(path)\n        return ix, path, path.exists() and path.stat().st_size &gt; 0\n</code></pre>"},{"location":"api-reference/images-config/#zamba.images.config.ImageClassificationTrainingConfig.preprocess_labels","title":"<code>preprocess_labels(values)</code>","text":"<p>One hot encode and add splits.</p> Source code in <code>zamba/images/config.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef preprocess_labels(cls, values):\n    \"\"\"One hot encode and add splits.\"\"\"\n    logger.info(\"Preprocessing labels.\")\n    labels = values[\"labels\"]\n\n    # lowercase to facilitate subset checking\n    labels[\"label\"] = labels.label.str.lower()\n\n    # one hot encoding\n    labels = pd.get_dummies(labels.rename(columns={\"label\": \"species\"}), columns=[\"species\"])\n\n    # We validate that all the images exist prior to this, so once this assembles the set of classes,\n    # we should have at least one example of each label and don't need to worry about filtering out classes\n    # with missing examples.\n    species_columns = labels.columns[labels.columns.str.contains(\"species_\")]\n    values[\"species_in_label_order\"] = species_columns.to_list()\n\n    indices = (\n        labels[species_columns].idxmax(axis=1).apply(lambda x: species_columns.get_loc(x))\n    )\n\n    labels[\"label\"] = indices\n\n    # if no \"split\" column, set up train, val, and test split\n    if \"split\" not in labels.columns:\n        make_split(labels, values)\n\n    values[\"labels\"] = labels.reset_index()\n\n    example_species = [\n        species.replace(\"species_\", \"\") for species in values[\"species_in_label_order\"][:3]\n    ]\n    logger.info(\n        f\"Labels preprocessed. {len(values['species_in_label_order'])} species found: {example_species}...\"\n    )\n    return values\n</code></pre>"},{"location":"api-reference/images-config/#zamba.images.config.ImageClassificationTrainingConfig.validate_image_files","title":"<code>validate_image_files(values)</code>","text":"<p>Validate and load image files.</p> Source code in <code>zamba/images/config.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef validate_image_files(cls, values):\n    \"\"\"Validate and load image files.\"\"\"\n    logger.info(\"Validating image files exist\")\n\n    exists = process_map(\n        cls._validate_filepath,\n        (values[\"data_dir\"] / values[\"labels\"].filepath.path).items(),\n        chunksize=max(\n            1, len(values[\"labels\"]) // 1000\n        ),  # chunks can be large; should be fast operation\n        total=len(values[\"labels\"]),\n    )\n\n    file_existence = pd.DataFrame(exists).set_index(0)\n    exists = file_existence[2]\n\n    if not exists.all():\n        missing_files = file_existence[~exists]\n        example_missing = [str(f) for f in missing_files.head(3)[1].values]\n        logger.warning(\n            f\"{(~exists).sum()} files in provided labels file do not exist on disk; ignoring those files. Example: {example_missing}...\"\n        )\n\n    values[\"labels\"] = values[\"labels\"][exists]\n\n    return values\n</code></pre>"},{"location":"api-reference/images-config/#zamba.images.config.ImageClassificationTrainingConfig.validate_labels","title":"<code>validate_labels(values)</code>","text":"<p>Validate and load labels</p> Source code in <code>zamba/images/config.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef validate_labels(cls, values):\n    \"\"\"Validate and load labels\"\"\"\n    logger.info(\"Validating labels\")\n\n    if isinstance(values[\"labels\"], pd.DataFrame):\n        pass\n    elif values[\"labels\"].suffix == \".json\":\n        with open(values[\"labels\"], \"r\") as f:\n            values[\"labels\"] = cls.process_json_annotations(\n                json.load(f), values[\"labels_format\"]\n            )\n    else:\n        values[\"labels\"] = pd.read_csv(values[\"labels\"])\n\n    return values\n</code></pre>"},{"location":"api-reference/images-manager/","title":"zamba.images.manager","text":""},{"location":"api-reference/metrics/","title":"zamba.metrics","text":""},{"location":"api-reference/metrics/#zamba.metrics.compute_species_specific_metrics","title":"<code>compute_species_specific_metrics(y_true, y_pred, labels=None)</code>","text":"<p>Computes species-specific accuracy, F1, precision, and recall. Args:     y_true (np.ndarray): An array with shape (samples, species) where each value indicates         the presence of a species in a sample.     y_pred (np.ndarray): An array with shape (samples, species) where each value indicates         the predicted presence of a species in a sample.</p> <p>Yields:</p> Type Description <code>Tuple[str, int, float]</code> <p>str, int, float: The metric name, species label index, and metric value.</p> Source code in <code>zamba/metrics.py</code> <pre><code>def compute_species_specific_metrics(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    labels: Optional[List[str]] = None,\n) -&gt; Generator[Tuple[str, int, float], None, None]:\n    \"\"\"Computes species-specific accuracy, F1, precision, and recall.\n    Args:\n        y_true (np.ndarray): An array with shape (samples, species) where each value indicates\n            the presence of a species in a sample.\n        y_pred (np.ndarray): An array with shape (samples, species) where each value indicates\n            the predicted presence of a species in a sample.\n\n    Yields:\n        str, int, float: The metric name, species label index, and metric value.\n    \"\"\"\n    if labels is None:\n        labels = range(y_true.shape[1])\n\n    elif len(labels) != y_true.shape[1]:\n        raise ValueError(\n            f\"The number of labels ({len(labels)}) must match the number of columns in y_true ({y_true.shape[1]}).\"\n        )\n\n    for index, label in enumerate(labels):\n        yield \"accuracy\", label, accuracy_score(y_true[:, index], y_pred[:, index])\n        yield \"f1\", label, f1_score(y_true[:, index], y_pred[:, index], zero_division=0)\n        yield \"precision\", label, precision_score(\n            y_true[:, index], y_pred[:, index], zero_division=0\n        )\n        yield \"recall\", label, recall_score(y_true[:, index], y_pred[:, index], zero_division=0)\n</code></pre>"},{"location":"api-reference/models-config/","title":"zamba.models.config","text":""},{"location":"api-reference/models-config/#zamba.models.config.BackboneFinetuneConfig","title":"<code>BackboneFinetuneConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration containing parameters to be used for backbone finetuning.</p> <p>Parameters:</p> Name Type Description Default <code>unfreeze_backbone_at_epoch</code> <code>int</code> <p>Epoch at which the backbone will be unfrozen. Defaults to 5.</p> required <code>backbone_initial_ratio_lr</code> <code>float</code> <p>Used to scale down the backbone learning rate compared to rest of model. Defaults to 0.01.</p> required <code>multiplier</code> <code>int or float</code> <p>Multiply the learning rate by a constant value at the end of each epoch. Defaults to 1.</p> required <code>pre_train_bn</code> <code>bool</code> <p>Train batch normalization layers prior to finetuning. False is recommended for slowfast models and True is recommended for time distributed models. Defaults to False.</p> required <code>train_bn</code> <code>bool</code> <p>Make batch normalization trainable. Defaults to False.</p> required <code>verbose</code> <code>bool</code> <p>Display current learning rate for model and backbone. Defaults to True.</p> required Source code in <code>zamba/models/config.py</code> <pre><code>class BackboneFinetuneConfig(ZambaBaseModel):\n    \"\"\"Configuration containing parameters to be used for backbone finetuning.\n\n    Args:\n        unfreeze_backbone_at_epoch (int, optional): Epoch at which the backbone\n            will be unfrozen. Defaults to 5.\n        backbone_initial_ratio_lr (float, optional): Used to scale down the backbone\n            learning rate compared to rest of model. Defaults to 0.01.\n        multiplier (int or float, optional): Multiply the learning rate by a constant\n            value at the end of each epoch. Defaults to 1.\n        pre_train_bn (bool, optional): Train batch normalization layers prior to\n            finetuning. False is recommended for slowfast models and True is recommended\n            for time distributed models. Defaults to False.\n        train_bn (bool, optional): Make batch normalization trainable. Defaults to False.\n        verbose (bool, optional): Display current learning rate for model and backbone.\n            Defaults to True.\n    \"\"\"\n\n    unfreeze_backbone_at_epoch: Optional[int] = 5\n    backbone_initial_ratio_lr: Optional[float] = 0.01\n    multiplier: Optional[Union[int, float]] = 1\n    pre_train_bn: Optional[bool] = False  # freeze batch norm layers prior to finetuning\n    train_bn: Optional[bool] = False  # don't train bn layers in unfrozen finetuning layers\n    verbose: Optional[bool] = True\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration containing parameters to be used for early stopping.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>str</code> <p>Metric to be monitored. Options are \"val_macro_f1\" or \"val_loss\". Defaults to \"val_macro_f1\".</p> required <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped. Defaults to 5.</p> required <code>verbose</code> <code>bool</code> <p>Verbosity mode. Defaults to True.</p> required <code>mode</code> <code>str</code> <p>Options are \"min\" or \"max\". In \"min\" mode, training will stop when the quantity monitored has stopped decreasing and in \"max\" mode it will stop when the quantity monitored has stopped increasing. If None, mode will be inferred from monitor. Defaults to None.</p> required Source code in <code>zamba/models/config.py</code> <pre><code>class EarlyStoppingConfig(ZambaBaseModel):\n    \"\"\"Configuration containing parameters to be used for early stopping.\n\n    Args:\n        monitor (str): Metric to be monitored. Options are \"val_macro_f1\" or\n            \"val_loss\". Defaults to \"val_macro_f1\".\n        patience (int): Number of epochs with no improvement after which training\n            will be stopped. Defaults to 5.\n        verbose (bool): Verbosity mode. Defaults to True.\n        mode (str, optional): Options are \"min\" or \"max\". In \"min\" mode, training\n            will stop when the quantity monitored has stopped decreasing and in\n            \"max\" mode it will stop when the quantity monitored has stopped increasing.\n            If None, mode will be inferred from monitor. Defaults to None.\n    \"\"\"\n\n    monitor: MonitorEnum = \"val_macro_f1\"\n    patience: int = 5\n    verbose: bool = True\n    mode: Optional[str] = None\n\n    @root_validator\n    def validate_mode(cls, values):\n        mode = {\"val_macro_f1\": \"max\", \"val_loss\": \"min\"}[values.get(\"monitor\")]\n        user_mode = values.get(\"mode\")\n        if user_mode is None:\n            values[\"mode\"] = mode\n        elif user_mode != mode:\n            raise ValueError(\n                f\"Provided mode {user_mode} is incorrect for {values.get('monitor')} monitor.\"\n            )\n        return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Contains all configs necessary to use a model for training or inference. Must contain a train_config or a predict_config at a minimum.</p> <p>Parameters:</p> Name Type Description Default <code>video_loader_config</code> <code>VideoLoaderConfig</code> <p>An instantiated VideoLoaderConfig. If None, will use default video loader config for model specified in TrainConfig or PredictConfig.</p> required <code>train_config</code> <code>TrainConfig</code> <p>An instantiated TrainConfig. Defaults to None.</p> required <code>predict_config</code> <code>PredictConfig</code> <p>An instantiated PredictConfig. Defaults to None.</p> required Source code in <code>zamba/models/config.py</code> <pre><code>class ModelConfig(ZambaBaseModel):\n    \"\"\"Contains all configs necessary to use a model for training or inference.\n    Must contain a train_config or a predict_config at a minimum.\n\n    Args:\n        video_loader_config (VideoLoaderConfig, optional): An instantiated VideoLoaderConfig.\n            If None, will use default video loader config for model specified in TrainConfig or\n            PredictConfig.\n        train_config (TrainConfig, optional): An instantiated TrainConfig.\n            Defaults to None.\n        predict_config (PredictConfig, optional): An instantiated PredictConfig.\n            Defaults to None.\n    \"\"\"\n\n    video_loader_config: Optional[VideoLoaderConfig] = None\n    train_config: Optional[TrainConfig] = None\n    predict_config: Optional[PredictConfig] = None\n\n    class Config:\n        json_loads = yaml.safe_load\n\n    @root_validator(skip_on_failure=True)\n    def one_config_must_exist(cls, values):\n        if values[\"train_config\"] is None and values[\"predict_config\"] is None:\n            raise ValueError(\"Must provide either `train_config` or `predict_config`.\")\n        else:\n            return values\n\n    @root_validator(skip_on_failure=True)\n    def get_default_video_loader_config(cls, values):\n        if values[\"video_loader_config\"] is None:\n            model_name = (\n                values[\"train_config\"].model_name\n                if values[\"train_config\"] is not None\n                else values[\"predict_config\"].model_name\n            )\n\n            logger.info(f\"No video loader config specified. Using default for {model_name}.\")\n\n            config_file = MODELS_DIRECTORY / f\"{model_name}/config.yaml\"\n            with config_file.open() as f:\n                config_dict = yaml.safe_load(f)\n\n            values[\"video_loader_config\"] = VideoLoaderConfig(**config_dict[\"video_loader_config\"])\n\n        return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.ModelEnum","title":"<code>ModelEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Shorthand names of models supported by zamba.</p> Source code in <code>zamba/models/config.py</code> <pre><code>class ModelEnum(str, Enum):\n    \"\"\"Shorthand names of models supported by zamba.\"\"\"\n\n    time_distributed = \"time_distributed\"\n    slowfast = \"slowfast\"\n    european = \"european\"\n    blank_nonblank = \"blank_nonblank\"\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.MonitorEnum","title":"<code>MonitorEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Validation metric to monitor for early stopping. Training is stopped when no improvement is observed.</p> Source code in <code>zamba/models/config.py</code> <pre><code>class MonitorEnum(str, Enum):\n    \"\"\"Validation metric to monitor for early stopping. Training is stopped when no\n    improvement is observed.\"\"\"\n\n    val_macro_f1 = \"val_macro_f1\"\n    val_loss = \"val_loss\"\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.PredictConfig","title":"<code>PredictConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration for using a video model for inference.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>DirectoryPath</code> <p>Path to a directory containing videos for inference. Defaults to the current working directory.</p> required <code>filepaths</code> <code>FilePath</code> <p>Path to a CSV containing videos for inference, with one row per video in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all files in data_dir. Defaults to None.</p> required <code>checkpoint</code> <code>FilePath</code> <p>Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to generate predictions. If None, defaults to a pretrained model. Defaults to None.</p> required <code>model_name</code> <code>str</code> <p>Name of the model to use for inference. Options are: time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed.</p> required <code>gpus</code> <code>int</code> <p>Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine.</p> required <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for inference. Defaults to 2.</p> required <code>save</code> <code>bool</code> <p>Whether to save out predictions. If False, predictions are not saved. Defaults to True.</p> required <code>save_dir</code> <code>Path</code> <p>An optional directory in which to save the model  predictions and configuration yaml. If no save_dir is specified and save=True,  outputs will be written to the current working directory. Defaults to None.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite outputs in save_dir if they exist. Defaults to False.</p> required <code>dry_run</code> <code>bool</code> <p>Perform inference on a single batch for testing. Predictions will not be saved. Defaults to False.</p> required <code>proba_threshold</code> <code>float</code> <p>Probability threshold for classification. If specified, binary predictions are returned with 1 being greater than the threshold and 0 being less than or equal to the threshold. If None, return probability scores for each species. Defaults to None.</p> required <code>output_class_names</code> <code>bool</code> <p>Output the species with the highest probability score as a single prediction for each video. If False, return probabilty scores for each species. Defaults to False.</p> required <code>weight_download_region</code> <code>str</code> <p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p> required <code>skip_load_validation</code> <code>bool</code> <p>By default, zamba runs a check to verify that all videos can be loaded and skips files that cannot be loaded. This can be time intensive, depending on how many videos there are. If you are very confident all your videos can be loaded, you can set this to True and skip this check. Defaults to False.</p> required <code>model_cache_dir</code> <code>Path</code> <p>Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, uses your default cache directory. Defaults to None.</p> required Source code in <code>zamba/models/config.py</code> <pre><code>class PredictConfig(ZambaBaseModel):\n    \"\"\"\n    Configuration for using a video model for inference.\n\n    Args:\n        data_dir (DirectoryPath): Path to a directory containing videos for\n            inference. Defaults to the current working directory.\n        filepaths (FilePath, optional): Path to a CSV containing videos for inference, with\n            one row per video in the data_dir. There must be a column called\n            'filepath' (absolute or relative to the data_dir). If None, uses\n            all files in data_dir. Defaults to None.\n        checkpoint (FilePath, optional): Path to a custom checkpoint file (.ckpt)\n            generated by zamba that can be used to generate predictions. If None,\n            defaults to a pretrained model. Defaults to None.\n        model_name (str, optional): Name of the model to use for inference. Options are:\n            time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed.\n        gpus (int): Number of GPUs to use for inference.\n            Defaults to all of the available GPUs found on the machine.\n        num_workers (int): Number of subprocesses to use for data loading. 0 means\n            that the data will be loaded in the main process. The maximum value is\n            the number of CPUs in the system. Defaults to 3.\n        batch_size (int): Batch size to use for inference. Defaults to 2.\n        save (bool): Whether to save out predictions. If False, predictions are\n            not saved. Defaults to True.\n        save_dir (Path, optional): An optional directory in which to save the model\n             predictions and configuration yaml. If no save_dir is specified and save=True,\n             outputs will be written to the current working directory. Defaults to None.\n        overwrite (bool): If True, overwrite outputs in save_dir if they exist.\n            Defaults to False.\n        dry_run (bool): Perform inference on a single batch for testing. Predictions\n            will not be saved. Defaults to False.\n        proba_threshold (float, optional): Probability threshold for classification.\n            If specified, binary predictions are returned with 1 being greater than the\n            threshold and 0 being less than or equal to the threshold. If None, return\n            probability scores for each species. Defaults to None.\n        output_class_names (bool): Output the species with the highest probability\n            score as a single prediction for each video. If False, return probabilty\n            scores for each species. Defaults to False.\n        weight_download_region (str): s3 region to download pretrained weights from.\n            Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific).\n            Defaults to \"us\".\n        skip_load_validation (bool): By default, zamba runs a check to verify that\n            all videos can be loaded and skips files that cannot be loaded. This can\n            be time intensive, depending on how many videos there are. If you are very\n            confident all your videos can be loaded, you can set this to True and skip\n            this check. Defaults to False.\n        model_cache_dir (Path, optional): Cache directory where downloaded model weights\n            will be saved. If None and the MODEL_CACHE_DIR environment variable is\n            not set, uses your default cache directory. Defaults to None.\n    \"\"\"\n\n    data_dir: DirectoryPath = \"\"\n    filepaths: Optional[FilePath] = None\n    checkpoint: Optional[FilePath] = None\n    model_name: Optional[ModelEnum] = ModelEnum.time_distributed.value\n    gpus: int = GPUS_AVAILABLE\n    num_workers: int = 3\n    batch_size: int = 2\n    save: bool = True\n    save_dir: Optional[Path] = None\n    overwrite: bool = False\n    dry_run: bool = False\n    proba_threshold: Optional[float] = None\n    output_class_names: bool = False\n    weight_download_region: RegionEnum = \"us\"\n    skip_load_validation: bool = False\n    model_cache_dir: Optional[Path] = None\n\n    _validate_gpus = validator(\"gpus\", allow_reuse=True, pre=True)(validate_gpus)\n\n    _validate_model_cache_dir = validator(\"model_cache_dir\", allow_reuse=True, always=True)(\n        validate_model_cache_dir\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_dry_run_and_save(cls, values):\n        if values[\"dry_run\"] and (\n            (values[\"save\"] is not False) or (values[\"save_dir\"] is not None)\n        ):\n            logger.warning(\n                \"Cannot save when predicting with dry_run=True. Setting save=False and save_dir=None.\"\n            )\n            values[\"save\"] = False\n            values[\"save_dir\"] = None\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_save_dir(cls, values):\n        save_dir = values[\"save_dir\"]\n        save = values[\"save\"]\n\n        # if no save_dir but save is True, use current working directory\n        if save_dir is None and save:\n            save_dir = Path.cwd()\n\n        if save_dir is not None:\n            # check if files exist\n            if (\n                (save_dir / \"zamba_predictions.csv\").exists()\n                or (save_dir / \"predict_configuration.yaml\").exists()\n            ) and not values[\"overwrite\"]:\n                raise ValueError(\n                    f\"zamba_predictions.csv and/or predict_configuration.yaml already exist in {save_dir}. If you would like to overwrite, set overwrite=True\"\n                )\n\n            # make a directory if needed\n            save_dir.mkdir(parents=True, exist_ok=True)\n\n            # set save to True if save_dir is set\n            if not save:\n                save = True\n\n        values[\"save_dir\"] = save_dir\n        values[\"save\"] = save\n\n        return values\n\n    _validate_model_name_and_checkpoint = root_validator(allow_reuse=True, skip_on_failure=True)(\n        validate_model_name_and_checkpoint\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_proba_threshold(cls, values):\n        if values[\"proba_threshold\"] is not None:\n            if (values[\"proba_threshold\"] &lt;= 0) or (values[\"proba_threshold\"] &gt;= 1):\n                raise ValueError(\n                    \"Setting proba_threshold outside of the range (0, 1) will cause all probabilities to be rounded to the same value.\"\n                )\n\n            if values[\"output_class_names\"] is True:\n                logger.warning(\n                    \"`output_class_names` will be ignored because `proba_threshold` is specified.\"\n                )\n        return values\n\n    _get_filepaths = root_validator(allow_reuse=True, pre=False, skip_on_failure=True)(\n        get_video_filepaths\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_files(cls, values):\n        # if globbing from data directory, already have valid dataframe\n        if isinstance(values[\"filepaths\"], pd.DataFrame):\n            files_df = values[\"filepaths\"]\n        else:\n            # make into dataframe even if only one column for clearer indexing\n            files_df = pd.DataFrame(pd.read_csv(values[\"filepaths\"]))\n\n        if \"filepath\" not in files_df.columns:\n            raise ValueError(f\"{values['filepaths']} must contain a `filepath` column.\")\n        else:\n            files_df = files_df[[\"filepath\"]]\n\n        # can only contain one row per filepath\n        duplicated = files_df.filepath.duplicated()\n        if duplicated.sum() &gt; 0:\n            logger.warning(\n                f\"Found {duplicated.sum():,} duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\"\n            )\n            files_df = files_df[[\"filepath\"]].drop_duplicates()\n\n        values[\"filepaths\"] = check_files_exist_and_load(\n            df=files_df,\n            data_dir=values[\"data_dir\"],\n            skip_load_validation=values[\"skip_load_validation\"],\n        )\n        return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.SchedulerConfig","title":"<code>SchedulerConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration containing parameters for a custom pytorch learning rate scheduler. See https://pytorch.org/docs/stable/optim.html for options.</p> <p>Parameters:</p> Name Type Description Default <code>scheduler</code> <code>str</code> <p>Name of learning rate scheduler to use. See https://pytorch.org/docs/stable/optim.html for options.</p> required <code>scheduler_params</code> <code>dict</code> <p>Parameters passed to learning rate scheduler upon initialization (eg. {\"milestones\": [1], \"gamma\": 0.5, \"verbose\": True}). Defaults to None.</p> required Source code in <code>zamba/models/config.py</code> <pre><code>class SchedulerConfig(ZambaBaseModel):\n    \"\"\"Configuration containing parameters for a custom pytorch learning rate scheduler.\n    See https://pytorch.org/docs/stable/optim.html for options.\n\n    Args:\n        scheduler (str): Name of learning rate scheduler to use. See\n            https://pytorch.org/docs/stable/optim.html for options.\n        scheduler_params (dict, optional): Parameters passed to learning rate\n            scheduler upon initialization (eg. {\"milestones\": [1], \"gamma\": 0.5,\n            \"verbose\": True}). Defaults to None.\n    \"\"\"\n\n    scheduler: Optional[str]\n    scheduler_params: Optional[dict] = None\n\n    @validator(\"scheduler\", always=True)\n    def validate_scheduler(cls, scheduler):\n        if scheduler is None:\n            return None\n\n        elif scheduler not in torch.optim.lr_scheduler.__dict__.keys():\n            raise ValueError(\n                \"Scheduler is not a `torch.optim.lr_scheduler`. \"\n                \"See https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py \"\n                \"for options.\"\n            )\n        else:\n            return scheduler\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.TrainConfig","title":"<code>TrainConfig</code>","text":"<p>               Bases: <code>ZambaBaseModel</code></p> <p>Configuration for training a video model.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>FilePath or pandas DataFrame</code> <p>Path to a CSV or pandas DataFrame containing labels for training, with one row per label. There must be columns called 'filepath' (absolute or relative to the data_dir) and 'label', and optionally columns called 'split' (\"train\", \"val\", or \"holdout\") and 'site'. Labels must be specified to train a model.</p> required <code>data_dir</code> <code>DirectoryPath</code> <p>Path to a directory containing training videos. Defaults to the current working directory.</p> required <code>checkpoint</code> <code>FilePath</code> <p>Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to resume training. If None and from_scratch is False, defaults to a pretrained model. Defaults to None.</p> required <code>scheduler_config</code> <code>SchedulerConfig or str</code> <p>Config for setting up the learning rate scheduler on the model. If \"default\", uses scheduler that was used for training. If None, will not use a scheduler. Defaults to \"default\".</p> required <code>model_name</code> <code>str</code> <p>Name of the model to use for training. Options are: time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed.</p> required <code>dry_run</code> <code>(bool or int, Optional)</code> <p>Run one training and validation batch for one epoch to detect any bugs prior to training the full model. Disables tuners, checkpoint callbacks, loggers, and logger callbacks. Defaults to False.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for training. Defaults to 2.</p> required <code>auto_lr_find</code> <code>bool</code> <p>Use a learning rate finder algorithm when calling trainer.tune() to try to find an optimal initial learning rate. Defaults to False. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution.</p> required <code>backbone_finetune_config</code> <code>BackboneFinetuneConfig</code> <p>Set parameters to finetune a backbone model to align with the current learning rate. Defaults to a BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True).</p> required <code>gpus</code> <code>int</code> <p>Number of GPUs to use during training. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available.</p> required <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3.</p> required <code>max_epochs</code> <code>int</code> <p>Stop training once this number of epochs is reached. Disabled by default (None), which means training continues until early stopping criteria are met.</p> required <code>early_stopping_config</code> <code>EarlyStoppingConfig</code> <p>Configuration for early stopping, which monitors a metric during training and stops training when the metric stops improving. Defaults to EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max').</p> required <code>weight_download_region</code> <code>str</code> <p>s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific). Defaults to \"us\".</p> required <code>split_proportions</code> <code>dict</code> <p>Proportions used to divide data into training, validation, and holdout sets if a \"split\" column is not included in labels. Defaults to {\"train\": 3, \"val\": 1, \"holdout\": 1}.</p> required <code>save_dir</code> <code>Path</code> <p>Path to a directory where training files will be saved. Files include the best model checkpoint (<code>model_name</code>.ckpt), training configuration (configuration.yaml), Tensorboard logs (events.out.tfevents...), test metrics (test_metrics.json), validation metrics (val_metrics.json), and model hyperparameters (hparams.yml). If not specified, files are saved to a folder in the current working directory.</p> required <code>overwrite</code> <code>bool</code> <p>If True, will save outputs in <code>save_dir</code> overwriting if those exist. If False, will create auto-incremented <code>version_n</code> folder in <code>save_dir</code> with model outputs. Defaults to False.</p> required <code>skip_load_validation</code> <code>bool</code> <p>Skip ffprobe check, which verifies that all videos can be loaded and skips files that cannot be loaded. Defaults to False.</p> required <code>from_scratch</code> <code>bool</code> <p>Instantiate the model with base weights. This means starting with ImageNet weights for image-based models (time_distributed, european, and blank_nonblank) and Kinetics weights for video-based models (slowfast). Defaults to False.</p> required <code>use_default_model_labels</code> <code>bool</code> <p>By default, output the full set of default model labels rather than just the species in the labels file. Only applies if the provided labels are a subset of the default model labels. If set to False, will replace the model head for finetuning and output only the species in the provided labels file.</p> required <code>model_cache_dir</code> <code>Path</code> <p>Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, uses your default cache directory. Defaults to None.</p> required Source code in <code>zamba/models/config.py</code> <pre><code>class TrainConfig(ZambaBaseModel):\n    \"\"\"\n    Configuration for training a video model.\n\n    Args:\n        labels (FilePath or pandas DataFrame): Path to a CSV or pandas DataFrame\n            containing labels for training, with one row per label. There must be\n            columns called 'filepath' (absolute or relative to the data_dir) and\n            'label', and optionally columns called 'split' (\"train\", \"val\", or \"holdout\")\n            and 'site'. Labels must be specified to train a model.\n        data_dir (DirectoryPath): Path to a directory containing training\n            videos. Defaults to the current working directory.\n        checkpoint (FilePath, optional): Path to a custom checkpoint file (.ckpt)\n            generated by zamba that can be used to resume training. If None and from_scratch\n            is False, defaults to a pretrained model. Defaults to None.\n        scheduler_config (SchedulerConfig or str, optional): Config for setting up\n            the learning rate scheduler on the model. If \"default\", uses scheduler\n            that was used for training. If None, will not use a scheduler.\n            Defaults to \"default\".\n        model_name (str, optional): Name of the model to use for training. Options are:\n            time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed.\n        dry_run (bool or int, Optional): Run one training and validation batch\n            for one epoch to detect any bugs prior to training the full model.\n            Disables tuners, checkpoint callbacks, loggers, and logger callbacks.\n            Defaults to False.\n        batch_size (int): Batch size to use for training. Defaults to 2.\n        auto_lr_find (bool): Use a learning rate finder algorithm when calling\n            trainer.tune() to try to find an optimal initial learning rate. Defaults to\n            False. The learning rate finder is not guaranteed to find a good learning\n            rate; depending on the dataset, it can select a learning rate that leads to\n            poor model training. Use with caution.\n        backbone_finetune_config (BackboneFinetuneConfig, optional): Set parameters\n            to finetune a backbone model to align with the current learning rate.\n            Defaults to a BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5,\n            backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False,\n            train_bn=False, verbose=True).\n        gpus (int): Number of GPUs to use during training. By default, all of\n            the available GPUs found on the machine will be used. An error will be raised\n            if the number of GPUs specified is more than the number that are available.\n        num_workers (int): Number of subprocesses to use for data loading. 0 means\n            that the data will be loaded in the main process. The maximum value is\n            the number of CPUs in the system. Defaults to 3.\n        max_epochs (int, optional): Stop training once this number of epochs is\n            reached. Disabled by default (None), which means training continues\n            until early stopping criteria are met.\n        early_stopping_config (EarlyStoppingConfig, optional): Configuration for\n            early stopping, which monitors a metric during training and stops training\n            when the metric stops improving. Defaults to EarlyStoppingConfig(monitor='val_macro_f1',\n            patience=5, verbose=True, mode='max').\n        weight_download_region (str): s3 region to download pretrained weights from.\n            Options are \"us\" (United States), \"eu\" (Europe), or \"asia\" (Asia Pacific).\n            Defaults to \"us\".\n        split_proportions (dict): Proportions used to divide data into training,\n            validation, and holdout sets if a \"split\" column is not included in\n            labels. Defaults to {\"train\": 3, \"val\": 1, \"holdout\": 1}.\n        save_dir (Path, optional): Path to a directory where training files\n            will be saved. Files include the best model checkpoint (``model_name``.ckpt),\n            training configuration (configuration.yaml), Tensorboard logs\n            (events.out.tfevents...), test metrics (test_metrics.json), validation\n            metrics (val_metrics.json), and model hyperparameters (hparams.yml).\n            If not specified, files are saved to a folder in the current working directory.\n        overwrite (bool): If True, will save outputs in `save_dir` overwriting if those\n            exist. If False, will create auto-incremented `version_n` folder in `save_dir`\n            with model outputs. Defaults to False.\n        skip_load_validation (bool): Skip ffprobe check, which verifies that all\n            videos can be loaded and skips files that cannot be loaded. Defaults\n            to False.\n        from_scratch (bool): Instantiate the model with base weights. This means\n            starting with ImageNet weights for image-based models (time_distributed,\n            european, and blank_nonblank) and Kinetics weights for video-based models\n            (slowfast). Defaults to False.\n        use_default_model_labels (bool, optional): By default, output the full set of\n            default model labels rather than just the species in the labels file. Only\n            applies if the provided labels are a subset of the default model labels.\n            If set to False, will replace the model head for finetuning and output only\n            the species in the provided labels file.\n        model_cache_dir (Path, optional): Cache directory where downloaded model weights\n            will be saved. If None and the MODEL_CACHE_DIR environment variable is\n            not set, uses your default cache directory. Defaults to None.\n    \"\"\"\n\n    labels: Union[FilePath, pd.DataFrame]\n    data_dir: DirectoryPath = \"\"\n    checkpoint: Optional[FilePath] = None\n    scheduler_config: Optional[Union[str, SchedulerConfig]] = \"default\"\n    model_name: Optional[ModelEnum] = ModelEnum.time_distributed.value\n    dry_run: Union[bool, int] = False\n    batch_size: int = 2\n    auto_lr_find: bool = False\n    backbone_finetune_config: Optional[BackboneFinetuneConfig] = BackboneFinetuneConfig()\n    gpus: int = GPUS_AVAILABLE\n    num_workers: int = 3\n    max_epochs: Optional[int] = None\n    early_stopping_config: Optional[EarlyStoppingConfig] = EarlyStoppingConfig()\n    weight_download_region: RegionEnum = \"us\"\n    split_proportions: Optional[Dict[str, int]] = {\"train\": 3, \"val\": 1, \"holdout\": 1}\n    save_dir: Path = Path.cwd()\n    overwrite: bool = False\n    skip_load_validation: bool = False\n    from_scratch: bool = False\n    use_default_model_labels: Optional[bool] = None\n    model_cache_dir: Optional[Path] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    _validate_gpus = validator(\"gpus\", allow_reuse=True, pre=True)(validate_gpus)\n\n    _validate_model_cache_dir = validator(\"model_cache_dir\", allow_reuse=True, always=True)(\n        validate_model_cache_dir\n    )\n\n    @root_validator(skip_on_failure=True)\n    def validate_from_scratch_and_checkpoint(cls, values):\n        if values[\"from_scratch\"]:\n            if values[\"checkpoint\"] is not None:\n                raise ValueError(\"If from_scratch=True, you cannot specify a checkpoint.\")\n\n            if values[\"model_name\"] is None:\n                raise ValueError(\"If from_scratch=True, model_name cannot be None.\")\n\n        return values\n\n    _validate_model_name_and_checkpoint = root_validator(allow_reuse=True, skip_on_failure=True)(\n        validate_model_name_and_checkpoint\n    )\n\n    @validator(\"scheduler_config\", always=True)\n    def validate_scheduler_config(cls, scheduler_config):\n        if scheduler_config is None:\n            return SchedulerConfig(scheduler=None)\n        elif isinstance(scheduler_config, str) and scheduler_config != \"default\":\n            raise ValueError(\"Scheduler can either be 'default', None, or a SchedulerConfig.\")\n        else:\n            return scheduler_config\n\n    @root_validator(skip_on_failure=True)\n    def turn_off_load_validation_if_dry_run(cls, values):\n        if values[\"dry_run\"] and not values[\"skip_load_validation\"]:\n            logger.info(\"Turning off video loading check since dry_run=True.\")\n            values[\"skip_load_validation\"] = True\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_filepaths_and_labels(cls, values):\n        logger.info(\"Validating labels csv.\")\n        labels = (\n            pd.read_csv(values[\"labels\"])\n            if not isinstance(values[\"labels\"], pd.DataFrame)\n            else values[\"labels\"]\n        )\n\n        if not set([\"label\", \"filepath\"]).issubset(labels.columns):\n            raise ValueError(f\"{values['labels']} must contain `filepath` and `label` columns.\")\n\n        # subset to required and optional\n        cols_to_keep = [c for c in labels.columns if c in [\"filepath\", \"label\", \"site\", \"split\"]]\n        labels = labels[cols_to_keep]\n\n        # validate split column has no partial nulls or invalid values\n        if \"split\" in labels.columns:\n            # if split is entirely null, warn, drop column, and generate splits automatically\n            if labels.split.isnull().all():\n                logger.warning(\n                    \"Split column is entirely null. Will generate splits automatically using `split_proportions`.\"\n                )\n                labels = labels.drop(\"split\", axis=1)\n\n            # error if split column has null values\n            elif labels.split.isnull().any():\n                raise ValueError(\n                    f\"Found {labels.split.isnull().sum()} row(s) with null `split`. Fill in these rows with either `train`, `val`, or `holdout`. Alternatively, do not include a `split` column in your labels and we'll generate splits for you using `split_proportions`.\"\n                )\n\n            # otherwise check that split values are valid\n            elif not set(labels.split).issubset({\"train\", \"val\", \"holdout\"}):\n                raise ValueError(\n                    f\"Found the following invalid values for `split`: {set(labels.split).difference({'train', 'val', 'holdout'})}. `split` can only contain `train`, `val`, or `holdout.`\"\n                )\n\n            elif values[\"split_proportions\"] is not None:\n                logger.warning(\n                    \"Labels contains split column yet split_proportions are also provided. Split column in labels takes precedence.\"\n                )\n                # set to None for clarity in final configuration.yaml\n                values[\"split_proportions\"] = None\n\n        # error if labels are entirely null\n        null_labels = labels.label.isnull()\n        if sum(null_labels) == len(labels):\n            raise ValueError(\"Species cannot be null for all videos.\")\n\n        # skip and warn about any videos without species label\n        elif sum(null_labels) &gt; 0:\n            logger.warning(f\"Found {sum(null_labels)} filepath(s) with no label. Will skip.\")\n            labels = labels[~null_labels]\n\n        # check that all videos exist and can be loaded\n        values[\"labels\"] = check_files_exist_and_load(\n            df=labels,\n            data_dir=values[\"data_dir\"],\n            skip_load_validation=values[\"skip_load_validation\"],\n        )\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def validate_provided_species_and_use_default_model_labels(cls, values):\n        \"\"\"If the model species are the desired output, the labels file must contain\n        a subset of the model species.\n        \"\"\"\n        provided_species = set(values[\"labels\"].label)\n        model_species = set(\n            get_model_species(checkpoint=values[\"checkpoint\"], model_name=values[\"model_name\"])\n        )\n\n        if not provided_species.issubset(model_species):\n            # if labels are not a subset, user cannot set use_default_model_labels to True\n            if values[\"use_default_model_labels\"]:\n                raise ValueError(\n                    \"Conflicting information between `use_default_model_labels=True` and the \"\n                    \"species provided in labels file. \"\n                    \"If you want your model to predict all the zamba species, make sure your \"\n                    \"labels are a subset. The species in the labels file that are not \"\n                    f\"in the model species are {provided_species - model_species}. \"\n                    \"If you want your model to only predict the species in your labels file, \"\n                    \"set `use_default_model_labels` to False.\"\n                )\n\n            else:\n                values[\"use_default_model_labels\"] = False\n\n        # if labels are a subset, default to True if no value provided\n        elif values[\"use_default_model_labels\"] is None:\n            values[\"use_default_model_labels\"] = True\n\n        return values\n\n    @root_validator(skip_on_failure=True)\n    def preprocess_labels(cls, values):\n        \"\"\"One hot encode, add splits, and check for binary case.\n\n        Replaces values['labels'] with modified DataFrame.\n\n        Args:\n            values: dictionary containing 'labels' and other config info\n        \"\"\"\n        logger.info(\"Preprocessing labels into one hot encoded labels with one row per video.\")\n        labels = values[\"labels\"]\n\n        # lowercase to facilitate subset checking\n        labels[\"label\"] = labels.label.str.lower()\n\n        model_species = get_model_species(\n            checkpoint=values[\"checkpoint\"], model_name=values[\"model_name\"]\n        )\n        labels[\"label\"] = pd.Categorical(\n            labels.label, categories=model_species if values[\"use_default_model_labels\"] else None\n        )\n        # one hot encode collapse to one row per video\n        labels = (\n            pd.get_dummies(labels.rename(columns={\"label\": \"species\"}), columns=[\"species\"])\n            .groupby(\"filepath\")\n            .max()\n        )\n\n        # if no \"split\" column, set up train, val, and holdout split\n        if \"split\" not in labels.columns:\n            make_split(labels, values)\n\n        # if there are only two species columns and every video belongs to one of them,\n        # keep only blank label if it exists to allow resuming of blank_nonblank model\n        # otherwise drop the second species column so the problem is treated as a binary classification\n        species_cols = labels.filter(regex=\"species_\").columns\n        sums = labels[species_cols].sum(axis=1)\n\n        if len(species_cols) == 2 and (sums == 1).all():\n            col_to_keep = \"species_blank\" if \"species_blank\" in species_cols else species_cols[0]\n            col_to_drop = [c for c in species_cols if c != col_to_keep]\n\n            logger.warning(\n                f\"Binary case detected so only one species column will be kept. Output will be the binary case of {col_to_keep}.\"\n            )\n            labels = labels.drop(columns=col_to_drop)\n\n        # filepath becomes column instead of index\n        values[\"labels\"] = labels.reset_index()\n        return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.TrainConfig.preprocess_labels","title":"<code>preprocess_labels(values)</code>","text":"<p>One hot encode, add splits, and check for binary case.</p> <p>Replaces values['labels'] with modified DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <p>dictionary containing 'labels' and other config info</p> required Source code in <code>zamba/models/config.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef preprocess_labels(cls, values):\n    \"\"\"One hot encode, add splits, and check for binary case.\n\n    Replaces values['labels'] with modified DataFrame.\n\n    Args:\n        values: dictionary containing 'labels' and other config info\n    \"\"\"\n    logger.info(\"Preprocessing labels into one hot encoded labels with one row per video.\")\n    labels = values[\"labels\"]\n\n    # lowercase to facilitate subset checking\n    labels[\"label\"] = labels.label.str.lower()\n\n    model_species = get_model_species(\n        checkpoint=values[\"checkpoint\"], model_name=values[\"model_name\"]\n    )\n    labels[\"label\"] = pd.Categorical(\n        labels.label, categories=model_species if values[\"use_default_model_labels\"] else None\n    )\n    # one hot encode collapse to one row per video\n    labels = (\n        pd.get_dummies(labels.rename(columns={\"label\": \"species\"}), columns=[\"species\"])\n        .groupby(\"filepath\")\n        .max()\n    )\n\n    # if no \"split\" column, set up train, val, and holdout split\n    if \"split\" not in labels.columns:\n        make_split(labels, values)\n\n    # if there are only two species columns and every video belongs to one of them,\n    # keep only blank label if it exists to allow resuming of blank_nonblank model\n    # otherwise drop the second species column so the problem is treated as a binary classification\n    species_cols = labels.filter(regex=\"species_\").columns\n    sums = labels[species_cols].sum(axis=1)\n\n    if len(species_cols) == 2 and (sums == 1).all():\n        col_to_keep = \"species_blank\" if \"species_blank\" in species_cols else species_cols[0]\n        col_to_drop = [c for c in species_cols if c != col_to_keep]\n\n        logger.warning(\n            f\"Binary case detected so only one species column will be kept. Output will be the binary case of {col_to_keep}.\"\n        )\n        labels = labels.drop(columns=col_to_drop)\n\n    # filepath becomes column instead of index\n    values[\"labels\"] = labels.reset_index()\n    return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.TrainConfig.validate_provided_species_and_use_default_model_labels","title":"<code>validate_provided_species_and_use_default_model_labels(values)</code>","text":"<p>If the model species are the desired output, the labels file must contain a subset of the model species.</p> Source code in <code>zamba/models/config.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef validate_provided_species_and_use_default_model_labels(cls, values):\n    \"\"\"If the model species are the desired output, the labels file must contain\n    a subset of the model species.\n    \"\"\"\n    provided_species = set(values[\"labels\"].label)\n    model_species = set(\n        get_model_species(checkpoint=values[\"checkpoint\"], model_name=values[\"model_name\"])\n    )\n\n    if not provided_species.issubset(model_species):\n        # if labels are not a subset, user cannot set use_default_model_labels to True\n        if values[\"use_default_model_labels\"]:\n            raise ValueError(\n                \"Conflicting information between `use_default_model_labels=True` and the \"\n                \"species provided in labels file. \"\n                \"If you want your model to predict all the zamba species, make sure your \"\n                \"labels are a subset. The species in the labels file that are not \"\n                f\"in the model species are {provided_species - model_species}. \"\n                \"If you want your model to only predict the species in your labels file, \"\n                \"set `use_default_model_labels` to False.\"\n            )\n\n        else:\n            values[\"use_default_model_labels\"] = False\n\n    # if labels are a subset, default to True if no value provided\n    elif values[\"use_default_model_labels\"] is None:\n        values[\"use_default_model_labels\"] = True\n\n    return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.ZambaBaseModel","title":"<code>ZambaBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Set defaults for all models that inherit from the pydantic base model.</p> Source code in <code>zamba/models/config.py</code> <pre><code>class ZambaBaseModel(BaseModel):\n    \"\"\"Set defaults for all models that inherit from the pydantic base model.\"\"\"\n\n    class Config:\n        extra = \"forbid\"\n        use_enum_values = True\n        validate_assignment = True\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.check_files_exist_and_load","title":"<code>check_files_exist_and_load(df, data_dir, skip_load_validation)</code>","text":"<p>Check whether files in file list exist and can be loaded with ffmpeg. Warn and skip files that don't exist or can't be loaded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with a \"filepath\" column</p> required <code>data_dir</code> <code>Path</code> <p>Data folder to prepend if filepath is not an absolute path.</p> required <code>skip_load_validation</code> <code>bool</code> <p>Skip ffprobe check that verifies all videos can be loaded.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with valid and loadable videos.</p> Source code in <code>zamba/models/config.py</code> <pre><code>def check_files_exist_and_load(\n    df: pd.DataFrame, data_dir: DirectoryPath, skip_load_validation: bool\n):\n    \"\"\"Check whether files in file list exist and can be loaded with ffmpeg.\n    Warn and skip files that don't exist or can't be loaded.\n\n    Args:\n        df (pd.DataFrame): DataFrame with a \"filepath\" column\n        data_dir (Path): Data folder to prepend if filepath is not an\n            absolute path.\n        skip_load_validation (bool): Skip ffprobe check that verifies all videos\n            can be loaded.\n\n    Returns:\n        pd.DataFrame: DataFrame with valid and loadable videos.\n    \"\"\"\n    # update filepath column to prepend data_dir\n    df[\"filepath\"] = str(data_dir) / df.filepath.path\n\n    # we can have multiple rows per file with labels so limit just to one row per file for these checks\n    files_df = df[[\"filepath\"]].drop_duplicates()\n\n    # check for missing files\n    logger.info(f\"Checking all {len(files_df):,} filepaths exist. Trying fast file checking...\")\n\n    # try to check files in parallel\n    paths = files_df[\"filepath\"].apply(Path)\n    exists = pqdm(paths, Path.exists, n_jobs=16)\n    exists = np.array(exists)\n\n    # if fast checking fails, fall back to slow checking\n    # if an I/O error is in `exists`, the array has dtype `object`\n    if exists.dtype != bool:\n        logger.info(\n            \"Fast file checking failed. Running slower check, which can take 30 seconds per thousand files.\"\n        )\n        exists = files_df[\"filepath\"].path.exists()\n\n    # select the missing files\n    invalid_files = files_df[~exists]\n\n    # if no files exist\n    if len(invalid_files) == len(files_df):\n        raise ValueError(\n            f\"None of the video filepaths exist. Are you sure they're specified correctly? Here's an example invalid path: {invalid_files.filepath.values[0]}. Either specify absolute filepaths in the csv or provide filepaths relative to `data_dir`.\"\n        )\n\n    # if at least some files exist\n    elif len(invalid_files) &gt; 0:\n        logger.debug(\n            f\"The following files could not be found: {'/n'.join(invalid_files.filepath.values.tolist())}\"\n        )\n        logger.warning(\n            f\"Skipping {len(invalid_files)} file(s) that could not be found. For example, {invalid_files.filepath.values[0]}.\"\n        )\n        # remove invalid files to prep for ffprobe check on remaining\n        files_df = files_df[~files_df.filepath.isin(invalid_files.filepath)]\n\n    bad_load = []\n    if not skip_load_validation:\n        logger.info(\n            \"Checking that all videos can be loaded. If you're very confident all your videos can be loaded, you can skip this with `skip_load_validation`, but it's not recommended.\"\n        )\n\n        # ffprobe check\n        for f in tqdm(files_df.filepath):\n            try:\n                ffmpeg.probe(str(f))\n            except ffmpeg.Error as exc:\n                logger.debug(ZambaFfmpegException(exc.stderr))\n                bad_load.append(f)\n\n        if len(bad_load) &gt; 0:\n            logger.warning(\n                f\"Skipping {len(bad_load)} file(s) that could not be loaded with ffmpeg.\"\n            )\n\n    df = df[\n        (~df.filepath.isin(bad_load)) &amp; (~df.filepath.isin(invalid_files.filepath))\n    ].reset_index(drop=True)\n\n    return df\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.get_filepaths","title":"<code>get_filepaths(values, suffix_whitelist)</code>","text":"<p>If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column <code>filepath</code> contains files with valid suffixes.</p> Source code in <code>zamba/models/config.py</code> <pre><code>def get_filepaths(values, suffix_whitelist):\n    \"\"\"If no file list is passed, get all files in data directory. Warn if there\n    are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath`\n    contains files with valid suffixes.\n    \"\"\"\n    if values[\"filepaths\"] is None:\n        logger.info(f\"Getting files in {values['data_dir']}.\")\n        files = []\n        new_suffixes = []\n\n        # iterate over all files in data directory\n        for f in Path(values[\"data_dir\"]).rglob(\"*\"):\n            if f.is_file():\n                # keep just files with supported suffixes\n                if f.suffix.lower() in suffix_whitelist:\n                    files.append(f.resolve())\n                else:\n                    new_suffixes.append(f.suffix.lower())\n\n        if len(new_suffixes) &gt; 0:\n            logger.warning(\n                f\"Ignoring {len(new_suffixes)} file(s) with suffixes {set(new_suffixes)}. To include, specify all suffixes with a VIDEO_SUFFIXES or IMAGE_SUFFIXES environment variable.\"\n            )\n\n        if len(files) == 0:\n            error_msg = f\"No relevant files found in {values['data_dir']}.\"\n            if len(set(new_suffixes) &amp; set(IMAGE_SUFFIXES)) &gt; 0:\n                error_msg += \" Image files *were* found. Use a command starting with `zamba image` to work with images rather than videos.\"\n            raise ValueError(error_msg)\n\n        logger.info(f\"Found {len(files):,} media files in {values['data_dir']}.\")\n        values[\"filepaths\"] = pd.DataFrame(files, columns=[\"filepath\"])\n    return values\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.make_split","title":"<code>make_split(labels, values)</code>","text":"<p>Add a split column to <code>labels</code>.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>DataFrame with one row per video</p> required <code>values</code> <p>dictionary with config info</p> required Source code in <code>zamba/models/config.py</code> <pre><code>def make_split(labels, values):\n    \"\"\"Add a split column to `labels`.\n\n    Args:\n        labels: DataFrame with one row per video\n        values: dictionary with config info\n    \"\"\"\n    logger.info(\n        f\"Dividing media files into train, val, and holdout sets using the following split proportions: {values['split_proportions']}.\"\n    )\n\n    # use site info if we have it\n    if \"site\" in labels.columns:\n        logger.info(\"Using provided 'site' column to do a site-specific split\")\n        labels[\"split\"] = create_site_specific_splits(\n            labels[\"site\"], proportions=values[\"split_proportions\"]\n        )\n    else:\n        # otherwise randomly allocate\n        logger.info(\n            \"No 'site' column found so media files for each species will be randomly allocated across splits using provided split proportions.\"\n        )\n\n        expected_splits = [k for k, v in values[\"split_proportions\"].items() if v &gt; 0]\n        random.seed(SPLIT_SEED)\n\n        # check we have at least as many videos per species as we have splits\n        # labels are OHE at this point\n        num_videos_per_species = labels.filter(regex=\"species_\").sum().to_dict()\n        too_few = {\n            k.split(\"species_\", 1)[1]: v\n            for k, v in num_videos_per_species.items()\n            if 0 &lt; v &lt; len(expected_splits)\n        }\n\n        if len(too_few) &gt; 0:\n            raise ValueError(\n                f\"Not all species have enough media files to allocate into the following splits: {', '.join(expected_splits)}. A minimum of {len(expected_splits)} media files per label is required. Found the following counts: {too_few}. Either remove these labels or add more images/videos.\"\n            )\n\n        for c in labels.filter(regex=\"species_\").columns:\n            species_df = labels[labels[c] &gt; 0]\n\n            if len(species_df):\n                # within each species, seed splits by putting one video in each set and then allocate videos based on split proportions\n                labels.loc[species_df.index, \"split\"] = expected_splits + random.choices(\n                    list(values[\"split_proportions\"].keys()),\n                    weights=list(values[\"split_proportions\"].values()),\n                    k=len(species_df) - len(expected_splits),\n                )\n\n        logger.info(f\"{labels.split.value_counts()}\")\n\n    # write splits.csv\n    filename = values[\"save_dir\"] / \"splits.csv\"\n    logger.info(f\"Writing out split information to {filename}.\")\n\n    # create the directory to save if we need to\n    values[\"save_dir\"].mkdir(parents=True, exist_ok=True)\n\n    labels.reset_index()[[\"filepath\", \"split\"]].drop_duplicates().to_csv(filename, index=False)\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.validate_gpus","title":"<code>validate_gpus(gpus)</code>","text":"<p>Ensure the number of GPUs requested is equal to or less than the number of GPUs available on the machine.</p> Source code in <code>zamba/models/config.py</code> <pre><code>def validate_gpus(gpus: int):\n    \"\"\"Ensure the number of GPUs requested is equal to or less than the number of GPUs\n    available on the machine.\"\"\"\n    if gpus &gt; GPUS_AVAILABLE:\n        raise ValueError(f\"Found only {GPUS_AVAILABLE} GPU(s). Cannot use {gpus}.\")\n    else:\n        return gpus\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.validate_model_cache_dir","title":"<code>validate_model_cache_dir(model_cache_dir)</code>","text":"<p>Set up cache directory for downloading model weight. Order of priority is: config argument, environment variable, or user's default cache dir.</p> Source code in <code>zamba/models/config.py</code> <pre><code>def validate_model_cache_dir(model_cache_dir: Optional[Path]):\n    \"\"\"Set up cache directory for downloading model weight. Order of priority is:\n    config argument, environment variable, or user's default cache dir.\n    \"\"\"\n    if model_cache_dir is None:\n        model_cache_dir = get_model_cache_dir()\n\n    model_cache_dir = Path(model_cache_dir)\n    model_cache_dir.mkdir(parents=True, exist_ok=True)\n    return model_cache_dir\n</code></pre>"},{"location":"api-reference/models-config/#zamba.models.config.validate_model_name_and_checkpoint","title":"<code>validate_model_name_and_checkpoint(cls, values)</code>","text":"<p>Ensures a checkpoint file or model name is provided. If a model name is provided, looks up the corresponding public checkpoint file from the official configs. Download the checkpoint if it does not yet exist.</p> Source code in <code>zamba/models/config.py</code> <pre><code>def validate_model_name_and_checkpoint(cls, values):\n    \"\"\"Ensures a checkpoint file or model name is provided. If a model name is provided,\n    looks up the corresponding public checkpoint file from the official configs.\n    Download the checkpoint if it does not yet exist.\n    \"\"\"\n    checkpoint = values.get(\"checkpoint\")\n    model_name = values.get(\"model_name\")\n\n    # must specify either checkpoint or model name\n    if checkpoint is None and model_name is None:\n        raise ValueError(\"Must provide either model_name or checkpoint path.\")\n\n    # checkpoint supercedes model\n    elif checkpoint is not None and model_name is not None:\n        logger.info(f\"Using checkpoint file: {checkpoint}.\")\n        # get model name from checkpoint so it can be used for the video loader config\n        hparams = get_checkpoint_hparams(checkpoint)\n\n        try:\n            values[\"model_name\"] = available_models[hparams[\"model_class\"]]._default_model_name\n        except (AttributeError, KeyError):\n            model_name = f\"{model_name}-{checkpoint.stem}\"\n\n    elif checkpoint is None and model_name is not None:\n        if not values.get(\"from_scratch\"):\n            # get public weights file from official models config\n            values[\"checkpoint\"] = get_model_checkpoint_filename(model_name)\n\n            # if cached version exists, use that\n            cached_path = Path(values[\"model_cache_dir\"]) / values[\"checkpoint\"]\n            if cached_path.exists():\n                values[\"checkpoint\"] = cached_path\n\n            # download if checkpoint doesn't exist\n            if not values[\"checkpoint\"].exists():\n                logger.info(\n                    f\"Downloading weights for model '{model_name}' to {values['model_cache_dir']}.\"\n                )\n                values[\"checkpoint\"] = download_weights(\n                    filename=str(values[\"checkpoint\"]),\n                    weight_region=values[\"weight_download_region\"],\n                    destination_dir=values[\"model_cache_dir\"],\n                )\n\n    return values\n</code></pre>"},{"location":"api-reference/models-efficientnet_models/","title":"zamba.models.efficientnet_models","text":""},{"location":"api-reference/models-model_manager/","title":"zamba.models.model_manager","text":""},{"location":"api-reference/models-model_manager/#zamba.models.model_manager.ModelManager","title":"<code>ModelManager</code>","text":"<p>               Bases: <code>object</code></p> <p>Mediates loading, configuration, and logic of model calls.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelConfig</code> <p>Instantiated ModelConfig.</p> required Source code in <code>zamba/models/model_manager.py</code> <pre><code>class ModelManager(object):\n    \"\"\"Mediates loading, configuration, and logic of model calls.\n\n    Args:\n        config (ModelConfig): Instantiated ModelConfig.\n    \"\"\"\n\n    def __init__(self, config: ModelConfig):\n        self.config = config\n\n    @classmethod\n    def from_yaml(cls, config):\n        if not isinstance(config, ModelConfig):\n            config = ModelConfig.parse_file(config)\n        return cls(config)\n\n    def train(self):\n        train_model(\n            train_config=self.config.train_config,\n            video_loader_config=self.config.video_loader_config,\n        )\n\n    def predict(self):\n        predict_model(\n            predict_config=self.config.predict_config,\n            video_loader_config=self.config.video_loader_config,\n        )\n</code></pre>"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager.instantiate_model","title":"<code>instantiate_model(checkpoint, labels=None, scheduler_config=None, from_scratch=None, model_name=None, use_default_model_labels=None, species=None)</code>","text":"<p>Instantiates the model from a checkpoint and detects whether the model head should be replaced. The model head is replaced if labels contain species that are not on the model or use_default_model_labels=False.</p> <p>Supports model instantiation for the following cases: - train from scratch (from_scratch=True) - finetune with new species (from_scratch=False, labels contains different species than model) - finetune with a subset of zamba species and output only the species in the labels file (use_default_model_labels=False) - finetune with a subset of zamba species but output all zamba species (use_default_model_labels=True) - predict using pretrained model (labels=None)</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>path</code> <p>Path to a checkpoint on disk.</p> required <code>labels</code> <code>DataFrame</code> <p>Dataframe where filepath is the index and columns are one hot encoded species.</p> <code>None</code> <code>scheduler_config</code> <code>SchedulerConfig</code> <p>SchedulerConfig to use for training or finetuning. Only used if labels is not None.</p> <code>None</code> <code>from_scratch</code> <code>bool</code> <p>Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None.</p> <code>None</code> <code>model_name</code> <code>ModelEnum</code> <p>Model name used to look up default hparams used for that model. Only relevant if training from scratch.</p> <code>None</code> <code>use_default_model_labels</code> <code>bool</code> <p>Whether to output the full set of default model labels rather than just the species in the labels file. Only used if labels is not None.</p> <code>None</code> <code>species</code> <code>list</code> <p>List of species in label order. If None, read from labels file.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ZambaVideoClassificationLightningModule</code> <code>ZambaVideoClassificationLightningModule</code> <p>Instantiated model</p> Source code in <code>zamba/models/model_manager.py</code> <pre><code>def instantiate_model(\n    checkpoint: os.PathLike,\n    labels: Optional[pd.DataFrame] = None,\n    scheduler_config: Optional[SchedulerConfig] = None,\n    from_scratch: Optional[bool] = None,\n    model_name: Optional[ModelEnum] = None,\n    use_default_model_labels: Optional[bool] = None,\n    species: Optional[list] = None,\n) -&gt; ZambaVideoClassificationLightningModule:\n    \"\"\"Instantiates the model from a checkpoint and detects whether the model head should be replaced.\n    The model head is replaced if labels contain species that are not on the model or use_default_model_labels=False.\n\n    Supports model instantiation for the following cases:\n    - train from scratch (from_scratch=True)\n    - finetune with new species (from_scratch=False, labels contains different species than model)\n    - finetune with a subset of zamba species and output only the species in the labels file (use_default_model_labels=False)\n    - finetune with a subset of zamba species but output all zamba species (use_default_model_labels=True)\n    - predict using pretrained model (labels=None)\n\n    Args:\n        checkpoint (path): Path to a checkpoint on disk.\n        labels (pd.DataFrame, optional): Dataframe where filepath is the index and columns are one hot encoded species.\n        scheduler_config (SchedulerConfig, optional): SchedulerConfig to use for training or finetuning.\n            Only used if labels is not None.\n        from_scratch (bool, optional): Whether to instantiate the model with base weights. This means starting\n            from the imagenet weights for image based models and the Kinetics weights for video models.\n           Only used if labels is not None.\n        model_name (ModelEnum, optional): Model name used to look up default hparams used for that model.\n            Only relevant if training from scratch.\n        use_default_model_labels (bool, optional): Whether to output the full set of default model labels rather than\n            just the species in the labels file. Only used if labels is not None.\n        species (list, optional): List of species in label order. If None, read from labels file.\n\n    Returns:\n        ZambaVideoClassificationLightningModule: Instantiated model\n    \"\"\"\n    if from_scratch:\n        hparams = get_default_hparams(model_name)\n    else:\n        hparams = get_checkpoint_hparams(checkpoint)\n\n    model_class = available_models[hparams[\"model_class\"]]\n    logger.info(f\"Instantiating model: {model_class.__name__}\")\n\n    # predicting\n    if labels is None:\n        logger.info(\"Loading from checkpoint.\")\n        model = model_class.from_disk(path=checkpoint, **hparams)\n        return model\n\n    # get species from labels file\n    if species is None:\n        species = labels.filter(regex=r\"^species_\").columns.tolist()\n        species = [s.split(\"species_\", 1)[1] for s in species]\n\n    # train from scratch\n    if from_scratch:\n        logger.info(\"Training from scratch.\")\n\n        # default would use scheduler used for pretrained model\n        if scheduler_config != \"default\":\n            hparams.update(scheduler_config.dict())\n\n        hparams.update({\"species\": species})\n        model = model_class(**hparams)\n        log_schedulers(model)\n        return model\n\n    # determine if finetuning or resuming training\n\n    # check if species in label file are a subset of pretrained model species\n    is_subset = set(species).issubset(set(hparams[\"species\"]))\n\n    if is_subset:\n        if use_default_model_labels:\n            return resume_training(\n                scheduler_config=scheduler_config,\n                hparams=hparams,\n                model_class=model_class,\n                checkpoint=checkpoint,\n            )\n\n        else:\n            logger.info(\n                \"Limiting only to species in labels file. Replacing model head and finetuning.\"\n            )\n            return replace_head(\n                scheduler_config=scheduler_config,\n                hparams=hparams,\n                species=species,\n                model_class=model_class,\n                checkpoint=checkpoint,\n            )\n\n    # without a subset, you will always get a new head\n    # the config validation prohibits setting use_default_model_labels to True without a subset\n    else:\n        logger.info(\n            \"Provided species do not fully overlap with Zamba species. Replacing model head and finetuning.\"\n        )\n        return replace_head(\n            scheduler_config=scheduler_config,\n            hparams=hparams,\n            species=species,\n            model_class=model_class,\n            checkpoint=checkpoint,\n        )\n</code></pre>"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager.predict_model","title":"<code>predict_model(predict_config, video_loader_config=None)</code>","text":"<p>Predicts from a model and writes out predictions to a csv.</p> <p>Parameters:</p> Name Type Description Default <code>predict_config</code> <code>PredictConfig</code> <p>Pydantic config for performing inference.</p> required <code>video_loader_config</code> <code>VideoLoaderConfig</code> <p>Pydantic config for preprocessing videos. If None, will use default for model specified in PredictConfig.</p> <code>None</code> Source code in <code>zamba/models/model_manager.py</code> <pre><code>def predict_model(\n    predict_config: PredictConfig,\n    video_loader_config: VideoLoaderConfig = None,\n):\n    \"\"\"Predicts from a model and writes out predictions to a csv.\n\n    Args:\n        predict_config (PredictConfig): Pydantic config for performing inference.\n        video_loader_config (VideoLoaderConfig, optional): Pydantic config for preprocessing videos.\n            If None, will use default for model specified in PredictConfig.\n    \"\"\"\n    # get default VLC for model if not specified\n    if video_loader_config is None:\n        video_loader_config = ModelConfig(\n            predict_config=predict_config, video_loader_config=video_loader_config\n        ).video_loader_config\n\n    # set up model\n    model = instantiate_model(\n        checkpoint=predict_config.checkpoint,\n    )\n\n    data_module = ZambaVideoDataModule(\n        video_loader_config=video_loader_config,\n        transform=MODEL_MAPPING[model.__class__.__name__][\"transform\"],\n        predict_metadata=predict_config.filepaths,\n        batch_size=predict_config.batch_size,\n        num_workers=predict_config.num_workers,\n    )\n\n    validate_species(model, data_module)\n\n    if video_loader_config.cache_dir is None:\n        logger.info(\"No cache dir is specified. Videos will not be cached.\")\n    else:\n        logger.info(f\"Videos will be cached to {video_loader_config.cache_dir}.\")\n\n    accelerator, devices = configure_accelerator_and_devices_from_gpus(predict_config.gpus)\n\n    trainer = pl.Trainer(\n        accelerator=accelerator,\n        devices=devices,\n        logger=False,\n        fast_dev_run=predict_config.dry_run,\n    )\n\n    configuration = {\n        \"model_class\": model.model_class,\n        \"species\": model.species,\n        \"predict_config\": json.loads(predict_config.json(exclude={\"filepaths\"})),\n        \"inference_start_time\": datetime.utcnow().isoformat(),\n        \"video_loader_config\": json.loads(video_loader_config.json()),\n    }\n\n    if predict_config.save is not False:\n        config_path = predict_config.save_dir / \"predict_configuration.yaml\"\n        logger.info(f\"Writing out full configuration to {config_path}.\")\n        with config_path.open(\"w\") as fp:\n            yaml.dump(configuration, fp)\n\n    dataloader = data_module.predict_dataloader()\n    logger.info(\"Starting prediction...\")\n    probas = trainer.predict(model=model, dataloaders=dataloader)\n\n    df = pd.DataFrame(\n        np.vstack(probas), columns=model.species, index=dataloader.dataset.original_indices\n    )\n\n    # change output format if specified\n    if predict_config.proba_threshold is not None:\n        df = (df &gt; predict_config.proba_threshold).astype(int)\n\n    elif predict_config.output_class_names:\n        df = df.idxmax(axis=1)\n\n    else:  # round to a useful number of places\n        df = df.round(5)\n\n    if predict_config.save is not False:\n        preds_path = predict_config.save_dir / \"zamba_predictions.csv\"\n        logger.info(f\"Saving out predictions to {preds_path}.\")\n        with preds_path.open(\"w\") as fp:\n            df.to_csv(fp, index=True)\n\n    return df\n</code></pre>"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager.train_model","title":"<code>train_model(train_config, video_loader_config=None)</code>","text":"<p>Trains a model.</p> <p>Parameters:</p> Name Type Description Default <code>train_config</code> <code>TrainConfig</code> <p>Pydantic config for training.</p> required <code>video_loader_config</code> <code>VideoLoaderConfig</code> <p>Pydantic config for preprocessing videos. If None, will use default for model specified in TrainConfig.</p> <code>None</code> Source code in <code>zamba/models/model_manager.py</code> <pre><code>def train_model(\n    train_config: TrainConfig,\n    video_loader_config: Optional[VideoLoaderConfig] = None,\n):\n    \"\"\"Trains a model.\n\n    Args:\n        train_config (TrainConfig): Pydantic config for training.\n        video_loader_config (VideoLoaderConfig, optional): Pydantic config for preprocessing videos.\n            If None, will use default for model specified in TrainConfig.\n    \"\"\"\n    # get default VLC for model if not specified\n    if video_loader_config is None:\n        video_loader_config = ModelConfig(\n            train_config=train_config, video_loader_config=video_loader_config\n        ).video_loader_config\n\n    # set up model\n    model = instantiate_model(\n        checkpoint=train_config.checkpoint,\n        labels=train_config.labels,\n        scheduler_config=train_config.scheduler_config,\n        from_scratch=train_config.from_scratch,\n        model_name=train_config.model_name,\n        use_default_model_labels=train_config.use_default_model_labels,\n    )\n\n    data_module = ZambaVideoDataModule(\n        video_loader_config=video_loader_config,\n        transform=MODEL_MAPPING[model.__class__.__name__][\"transform\"],\n        train_metadata=train_config.labels,\n        batch_size=train_config.batch_size,\n        num_workers=train_config.num_workers,\n    )\n\n    validate_species(model, data_module)\n\n    train_config.save_dir.mkdir(parents=True, exist_ok=True)\n\n    # add folder version_n that auto increments if we are not overwriting\n    tensorboard_version = train_config.save_dir.name if train_config.overwrite else None\n    tensorboard_save_dir = (\n        train_config.save_dir.parent if train_config.overwrite else train_config.save_dir\n    )\n\n    tensorboard_logger = TensorBoardLogger(\n        save_dir=tensorboard_save_dir,\n        name=None,\n        version=tensorboard_version,\n        default_hp_metric=False,\n    )\n\n    logging_and_save_dir = (\n        tensorboard_logger.log_dir if not train_config.overwrite else train_config.save_dir\n    )\n\n    model_checkpoint = ModelCheckpoint(\n        dirpath=logging_and_save_dir,\n        filename=train_config.model_name,\n        monitor=(\n            train_config.early_stopping_config.monitor\n            if train_config.early_stopping_config is not None\n            else None\n        ),\n        mode=(\n            train_config.early_stopping_config.mode\n            if train_config.early_stopping_config is not None\n            else \"min\"\n        ),\n    )\n\n    callbacks = [model_checkpoint]\n\n    if train_config.early_stopping_config is not None:\n        callbacks.append(EarlyStopping(**train_config.early_stopping_config.dict()))\n\n    if train_config.backbone_finetune_config is not None:\n        callbacks.append(BackboneFinetuning(**train_config.backbone_finetune_config.dict()))\n\n    accelerator, devices = configure_accelerator_and_devices_from_gpus(train_config.gpus)\n\n    trainer = pl.Trainer(\n        accelerator=accelerator,\n        devices=devices,\n        max_epochs=train_config.max_epochs,\n        logger=tensorboard_logger,\n        callbacks=callbacks,\n        fast_dev_run=train_config.dry_run,\n        strategy=(\n            DDPStrategy(find_unused_parameters=False)\n            if (data_module.multiprocessing_context is not None) and (train_config.gpus &gt; 1)\n            else \"auto\"\n        ),\n    )\n\n    if video_loader_config.cache_dir is None:\n        logger.info(\"No cache dir is specified. Videos will not be cached.\")\n    else:\n        logger.info(f\"Videos will be cached to {video_loader_config.cache_dir}.\")\n\n    if train_config.auto_lr_find:\n        logger.info(\"Finding best learning rate.\")\n        tuner = Tuner(trainer)\n        tuner.lr_find(model=model, datamodule=data_module)\n\n    try:\n        git_hash = git.Repo(search_parent_directories=True).head.object.hexsha\n    except git.exc.InvalidGitRepositoryError:\n        git_hash = None\n\n    configuration = {\n        \"git_hash\": git_hash,\n        \"model_class\": model.model_class,\n        \"species\": model.species,\n        \"starting_learning_rate\": model.lr,\n        \"train_config\": json.loads(train_config.json(exclude={\"labels\"})),\n        \"training_start_time\": datetime.utcnow().isoformat(),\n        \"video_loader_config\": json.loads(video_loader_config.json()),\n    }\n\n    if not train_config.dry_run:\n        config_path = Path(logging_and_save_dir) / \"train_configuration.yaml\"\n        config_path.parent.mkdir(exist_ok=True, parents=True)\n        logger.info(f\"Writing out full configuration to {config_path}.\")\n        with config_path.open(\"w\") as fp:\n            yaml.dump(configuration, fp)\n\n    logger.info(\"Starting training...\")\n    trainer.fit(model, data_module)\n\n    if not train_config.dry_run:\n        if trainer.datamodule.test_dataloader() is not None:\n            logger.info(\"Calculating metrics on holdout set.\")\n            test_metrics = trainer.test(\n                dataloaders=trainer.datamodule.test_dataloader(), ckpt_path=\"best\"\n            )[0]\n            with (Path(logging_and_save_dir) / \"test_metrics.json\").open(\"w\") as fp:\n                json.dump(test_metrics, fp, indent=2)\n\n        if trainer.datamodule.val_dataloader() is not None:\n            logger.info(\"Calculating metrics on validation set.\")\n            val_metrics = trainer.validate(\n                dataloaders=trainer.datamodule.val_dataloader(), ckpt_path=\"best\"\n            )[0]\n            with (Path(logging_and_save_dir) / \"val_metrics.json\").open(\"w\") as fp:\n                json.dump(val_metrics, fp, indent=2)\n\n    return trainer\n</code></pre>"},{"location":"api-reference/models-slowfast_models/","title":"zamba.models.slowfast_models","text":""},{"location":"api-reference/models-slowfast_models/#zamba.models.slowfast_models.SlowFast","title":"<code>SlowFast</code>","text":"<p>               Bases: <code>ZambaVideoClassificationLightningModule</code></p> <p>Pretrained SlowFast model for fine-tuning with the following architecture:</p> <p>Input -&gt; SlowFast Base (including trainable Backbone) -&gt; Res Basic Head -&gt; Output</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>Module</code> <p>When scheduling the backbone to train with the <code>BackboneFinetune</code> callback, this indicates the trainable part of the base.</p> <code>base</code> <code>Module</code> <p>The entire model prior to the head.</p> <code>head</code> <code>Module</code> <p>The trainable head.</p> <code>_backbone_output_dim</code> <code>int</code> <p>Dimensionality of the backbone output (and head input).</p> Source code in <code>zamba/models/slowfast_models.py</code> <pre><code>@register_model\nclass SlowFast(ZambaVideoClassificationLightningModule):\n    \"\"\"Pretrained SlowFast model for fine-tuning with the following architecture:\n\n    Input -&gt; SlowFast Base (including trainable Backbone) -&gt; Res Basic Head -&gt; Output\n\n    Attributes:\n        backbone (torch.nn.Module): When scheduling the backbone to train with the\n            `BackboneFinetune` callback, this indicates the trainable part of the base.\n        base (torch.nn.Module): The entire model prior to the head.\n        head (torch.nn.Module): The trainable head.\n        _backbone_output_dim (int): Dimensionality of the backbone output (and head input).\n    \"\"\"\n\n    _default_model_name = \"slowfast\"  # used to look up default configuration for checkpoints\n\n    def __init__(\n        self,\n        backbone_mode: str = \"train\",\n        post_backbone_dropout: Optional[float] = None,\n        output_with_global_average: bool = True,\n        head_dropout_rate: Optional[float] = None,\n        head_hidden_layer_sizes: Optional[Tuple[int]] = None,\n        finetune_from: Optional[Union[os.PathLike, str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Initializes the SlowFast model.\n\n        Args:\n            backbone_mode (str): If \"eval\", treat the backbone as a feature extractor\n                and set to evaluation mode in all forward passes.\n            post_backbone_dropout (float, optional): Dropout that operates on the output of the\n                backbone + pool (before the fully-connected layer in the head).\n            output_with_global_average (bool): If True, apply an adaptive average pooling\n                operation after the fully-connected layer in the head.\n            head_dropout_rate (float, optional): Optional dropout rate applied after backbone and\n                between projection layers in the head.\n            head_hidden_layer_sizes (tuple of int): If not None, the size of hidden layers in the\n                head multilayer perceptron.\n            finetune_from (pathlike or str, optional): If not None, load an existing model from\n                the path and resume training from an existing model.\n        \"\"\"\n        super().__init__(**kwargs)\n\n        if finetune_from is None:\n            self.initialize_from_torchub()\n        else:\n            model = self.from_disk(finetune_from)\n            self._backbone_output_dim = model.head.proj.in_features\n            self.backbone = model.backbone\n            self.base = model.base\n\n        for param in self.base.parameters():\n            param.requires_grad = False\n\n        head = ResNetBasicHead(\n            proj=build_multilayer_perceptron(\n                self._backbone_output_dim,\n                head_hidden_layer_sizes,\n                self.num_classes,\n                activation=torch.nn.ReLU,\n                dropout=head_dropout_rate,\n                output_activation=None,\n            ),\n            activation=None,\n            pool=None,\n            dropout=(\n                None if post_backbone_dropout is None else torch.nn.Dropout(post_backbone_dropout)\n            ),\n            output_pool=torch.nn.AdaptiveAvgPool3d(1),\n        )\n\n        self.backbone_mode = backbone_mode\n        self.head = head\n\n        self.save_hyperparameters(\n            \"backbone_mode\",\n            \"head_dropout_rate\",\n            \"head_hidden_layer_sizes\",\n            \"output_with_global_average\",\n            \"post_backbone_dropout\",\n        )\n\n    def initialize_from_torchub(self):\n        \"\"\"Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule\n        by removing the head and setting the backbone and base.\"\"\"\n\n        # workaround for pytorch bug\n        torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n        base = torch.hub.load(\n            \"facebookresearch/pytorchvideo:0.1.3\", model=\"slowfast_r50\", pretrained=True\n        )\n        self._backbone_output_dim = base.blocks[-1].proj.in_features\n\n        base.blocks = base.blocks[:-1]  # Remove the pre-trained head\n\n        # self.backbone attribute lets `BackboneFinetune` freeze and unfreeze that module\n        self.backbone = base.blocks[-2:]\n        self.base = base\n\n    def forward(self, x, *args, **kwargs):\n        if self.backbone_mode == \"eval\":\n            self.base.eval()\n\n        x = self.base(x)\n        return self.head(x)\n</code></pre>"},{"location":"api-reference/models-slowfast_models/#zamba.models.slowfast_models.SlowFast.__init__","title":"<code>__init__(backbone_mode='train', post_backbone_dropout=None, output_with_global_average=True, head_dropout_rate=None, head_hidden_layer_sizes=None, finetune_from=None, **kwargs)</code>","text":"<p>Initializes the SlowFast model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone_mode</code> <code>str</code> <p>If \"eval\", treat the backbone as a feature extractor and set to evaluation mode in all forward passes.</p> <code>'train'</code> <code>post_backbone_dropout</code> <code>float</code> <p>Dropout that operates on the output of the backbone + pool (before the fully-connected layer in the head).</p> <code>None</code> <code>output_with_global_average</code> <code>bool</code> <p>If True, apply an adaptive average pooling operation after the fully-connected layer in the head.</p> <code>True</code> <code>head_dropout_rate</code> <code>float</code> <p>Optional dropout rate applied after backbone and between projection layers in the head.</p> <code>None</code> <code>head_hidden_layer_sizes</code> <code>tuple of int</code> <p>If not None, the size of hidden layers in the head multilayer perceptron.</p> <code>None</code> <code>finetune_from</code> <code>pathlike or str</code> <p>If not None, load an existing model from the path and resume training from an existing model.</p> <code>None</code> Source code in <code>zamba/models/slowfast_models.py</code> <pre><code>def __init__(\n    self,\n    backbone_mode: str = \"train\",\n    post_backbone_dropout: Optional[float] = None,\n    output_with_global_average: bool = True,\n    head_dropout_rate: Optional[float] = None,\n    head_hidden_layer_sizes: Optional[Tuple[int]] = None,\n    finetune_from: Optional[Union[os.PathLike, str]] = None,\n    **kwargs,\n):\n    \"\"\"Initializes the SlowFast model.\n\n    Args:\n        backbone_mode (str): If \"eval\", treat the backbone as a feature extractor\n            and set to evaluation mode in all forward passes.\n        post_backbone_dropout (float, optional): Dropout that operates on the output of the\n            backbone + pool (before the fully-connected layer in the head).\n        output_with_global_average (bool): If True, apply an adaptive average pooling\n            operation after the fully-connected layer in the head.\n        head_dropout_rate (float, optional): Optional dropout rate applied after backbone and\n            between projection layers in the head.\n        head_hidden_layer_sizes (tuple of int): If not None, the size of hidden layers in the\n            head multilayer perceptron.\n        finetune_from (pathlike or str, optional): If not None, load an existing model from\n            the path and resume training from an existing model.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    if finetune_from is None:\n        self.initialize_from_torchub()\n    else:\n        model = self.from_disk(finetune_from)\n        self._backbone_output_dim = model.head.proj.in_features\n        self.backbone = model.backbone\n        self.base = model.base\n\n    for param in self.base.parameters():\n        param.requires_grad = False\n\n    head = ResNetBasicHead(\n        proj=build_multilayer_perceptron(\n            self._backbone_output_dim,\n            head_hidden_layer_sizes,\n            self.num_classes,\n            activation=torch.nn.ReLU,\n            dropout=head_dropout_rate,\n            output_activation=None,\n        ),\n        activation=None,\n        pool=None,\n        dropout=(\n            None if post_backbone_dropout is None else torch.nn.Dropout(post_backbone_dropout)\n        ),\n        output_pool=torch.nn.AdaptiveAvgPool3d(1),\n    )\n\n    self.backbone_mode = backbone_mode\n    self.head = head\n\n    self.save_hyperparameters(\n        \"backbone_mode\",\n        \"head_dropout_rate\",\n        \"head_hidden_layer_sizes\",\n        \"output_with_global_average\",\n        \"post_backbone_dropout\",\n    )\n</code></pre>"},{"location":"api-reference/models-slowfast_models/#zamba.models.slowfast_models.SlowFast.initialize_from_torchub","title":"<code>initialize_from_torchub()</code>","text":"<p>Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule by removing the head and setting the backbone and base.</p> Source code in <code>zamba/models/slowfast_models.py</code> <pre><code>def initialize_from_torchub(self):\n    \"\"\"Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule\n    by removing the head and setting the backbone and base.\"\"\"\n\n    # workaround for pytorch bug\n    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n    base = torch.hub.load(\n        \"facebookresearch/pytorchvideo:0.1.3\", model=\"slowfast_r50\", pretrained=True\n    )\n    self._backbone_output_dim = base.blocks[-1].proj.in_features\n\n    base.blocks = base.blocks[:-1]  # Remove the pre-trained head\n\n    # self.backbone attribute lets `BackboneFinetune` freeze and unfreeze that module\n    self.backbone = base.blocks[-2:]\n    self.base = base\n</code></pre>"},{"location":"api-reference/models-utils/","title":"zamba.models.utils","text":""},{"location":"api-reference/models-utils/#zamba.models.utils.configure_accelerator_and_devices_from_gpus","title":"<code>configure_accelerator_and_devices_from_gpus(gpus)</code>","text":"<p>Derive accelerator and number of devices for pl.Trainer from user-specified number of gpus.</p> Source code in <code>zamba/models/utils.py</code> <pre><code>def configure_accelerator_and_devices_from_gpus(gpus):\n    \"\"\"Derive accelerator and number of devices for pl.Trainer from user-specified number of gpus.\"\"\"\n    if gpus &gt; 0:\n        accelerator = \"gpu\"\n        devices = gpus\n    else:\n        accelerator = \"cpu\"\n        devices = \"auto\"\n    return accelerator, devices\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/","title":"zamba.object_detection.yolox.megadetector_lite_yolox","text":""},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.FillModeEnum","title":"<code>FillModeEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for frame filtering fill modes</p> <p>Attributes:</p> Name Type Description <code>repeat</code> <p>Randomly resample qualifying frames to get to n_frames</p> <code>score_sorted</code> <p>Take up to n_frames in sort order (even if some have zero probability)</p> <code>weighted_euclidean</code> <p>Sample the remaining frames weighted by their euclidean distance in time to the frames over the threshold</p> <code>weighted_prob</code> <p>Sample the remaining frames weighted by their predicted probability</p> Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>class FillModeEnum(str, Enum):\n    \"\"\"Enum for frame filtering fill modes\n\n    Attributes:\n        repeat: Randomly resample qualifying frames to get to n_frames\n        score_sorted: Take up to n_frames in sort order (even if some have zero probability)\n        weighted_euclidean: Sample the remaining frames weighted by their euclidean distance in\n            time to the frames over the threshold\n        weighted_prob: Sample the remaining frames weighted by their predicted probability\n    \"\"\"\n\n    repeat = \"repeat\"\n    score_sorted = \"score_sorted\"\n    weighted_euclidean = \"weighted_euclidean\"\n    weighted_prob = \"weighted_prob\"\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.MegadetectorLiteYoloX","title":"<code>MegadetectorLiteYoloX</code>","text":"Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>class MegadetectorLiteYoloX:\n    def __init__(\n        self,\n        path: os.PathLike = LOCAL_MD_LITE_MODEL,\n        kwargs: os.PathLike = LOCAL_MD_LITE_MODEL_KWARGS,\n        config: Optional[Union[MegadetectorLiteYoloXConfig, dict]] = None,\n    ):\n        \"\"\"MegadetectorLite based on YOLOX.\n\n        Args:\n            path (pathlike): Path to trained YoloX model checkpoint (.pth extension)\n            config (MegadetectorLiteYoloXConfig): YoloX configuration\n        \"\"\"\n        if config is None:\n            config = MegadetectorLiteYoloXConfig()\n        elif isinstance(config, dict):\n            config = MegadetectorLiteYoloXConfig.parse_obj(config)\n\n        yolox = YoloXModel.load(\n            checkpoint=path,\n            model_kwargs_path=kwargs,\n        )\n\n        ckpt = torch.load(yolox.args.ckpt, weights_only=False, map_location=config.device)\n        model = yolox.exp.get_model()\n        model.load_state_dict(ckpt[\"model\"])\n        model = model.eval().to(config.device)\n\n        self.model = model\n        self.yolox = yolox\n        self.config = config\n        self.num_classes = yolox.exp.num_classes\n\n    @staticmethod\n    def scale_and_pad_array(\n        image_array: np.ndarray, output_width: int, output_height: int\n    ) -&gt; np.ndarray:\n        return np.array(\n            ImageOps.pad(\n                Image.fromarray(image_array),\n                (output_width, output_height),\n                method=Image.BICUBIC,\n                color=None,\n                centering=(0, 0),\n            )\n        )\n\n    def _preprocess(self, frame: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Process an image for the model, including scaling/padding the image, transposing from\n        (height, width, channel) to (channel, height, width) and casting to float.\n        \"\"\"\n        arr = np.ascontiguousarray(\n            self.scale_and_pad_array(frame, self.config.image_width, self.config.image_height),\n            dtype=np.float32,\n        )\n        return np.moveaxis(arr, 2, 0)\n\n    def _preprocess_video(self, video: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Process a video for the model, including resizing the frames in the video, transposing\n        from (batch, height, width, channel) to (batch, channel, height, width) and casting to float.\n        \"\"\"\n        resized_video = np.zeros(\n            (video.shape[0], video.shape[3], self.config.image_height, self.config.image_width),\n            dtype=np.float32,\n        )\n        for frame_idx in range(video.shape[0]):\n            resized_video[frame_idx] = self._preprocess(video[frame_idx])\n        return resized_video\n\n    def detect_video(self, video_arr: np.ndarray, pbar: bool = False):\n        \"\"\"Runs object detection on an video.\n\n        Args:\n            video_arr (np.ndarray): An video array with dimensions (frames, height, width, channels).\n            pbar (int): Whether to show progress bar. Defaults to False.\n\n        Returns:\n            list: A list containing detections and score for each frame. Each tuple contains two arrays:\n                the first is an array of bounding box detections with dimensions (object, 4) where\n                object is the number of objects detected and the other 4 dimension are\n                (x1, y1, x2, y1). The second is an array of object detection confidence scores of\n                length (object) where object is the number of objects detected.\n        \"\"\"\n\n        pbar = tqdm if pbar else lambda x: x\n\n        # batch of frames\n        batch_size = self.config.frame_batch_size\n\n        video_outputs = []\n        with torch.no_grad():\n            for i in range(0, len(video_arr), batch_size):\n                a = video_arr[i : i + batch_size]\n\n                outputs = self.model(\n                    torch.from_numpy(self._preprocess_video(a)).to(self.config.device)\n                )\n                outputs = postprocess(\n                    outputs, self.num_classes, self.config.confidence, self.config.nms_threshold\n                )\n                video_outputs.extend(outputs)\n\n        detections = []\n        for o in pbar(video_outputs):\n            detections.append(\n                self._process_frame_output(\n                    o, original_height=video_arr.shape[1], original_width=video_arr.shape[2]\n                )\n            )\n\n        return detections\n\n    def detect_image(self, img_arr: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Runs object detection on an image.\n\n        Args:\n            img_arr (np.ndarray): An image array with dimensions (height, width, channels).\n\n        Returns:\n            np.ndarray: An array of bounding box detections with dimensions (object, 4) where\n                object is the number of objects detected and the other 4 dimension are\n                (x1, y1, x2, y2).\n\n            np.ndarray: An array of object detection confidence scores of length (object) where\n                object is the number of objects detected.\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.model(\n                torch.from_numpy(self._preprocess(img_arr)).unsqueeze(0).to(self.config.device)\n            )\n            output = postprocess(\n                outputs, self.num_classes, self.config.confidence, self.config.nms_threshold\n            )\n\n        return self._process_frame_output(output[0], img_arr.shape[0], img_arr.shape[1])\n\n    def _process_frame_output(self, output, original_height, original_width):\n        if output is None:\n            return np.array([]), np.array([])\n        else:\n            detections = pd.DataFrame(\n                output.cpu().numpy(),\n                columns=[\"x1\", \"y1\", \"x2\", \"y2\", \"score1\", \"score2\", \"class_num\"],\n            ).assign(score=lambda row: row.score1 * row.score2)\n\n            # Transform bounding box to be in terms of the original image dimensions\n            ratio = min(\n                self.config.image_width / original_width,\n                self.config.image_height / original_height,\n            )\n            detections[[\"x1\", \"y1\", \"x2\", \"y2\"]] /= ratio\n\n            # Express bounding boxes in terms of proportions of original image dimensions\n            detections[[\"x1\", \"x2\"]] /= original_width\n            detections[[\"y1\", \"y2\"]] /= original_height\n\n            return detections[[\"x1\", \"y1\", \"x2\", \"y2\"]].values, detections.score.values\n\n    def filter_frames(\n        self, frames: np.ndarray, detections: List[Tuple[float, float, float, float]]\n    ) -&gt; np.ndarray:\n        \"\"\"Filter video frames using megadetector lite.\n\n        Which frames are returned depends on the fill_mode and how many frames are above the\n        confidence threshold. If more than n_frames are above the threshold, the top n_frames are\n        returned. Otherwise add to those over threshold based on fill_mode. If none of these\n        conditions are met, returns all frames above the threshold.\n\n        Args:\n            frames (np.ndarray): Array of video frames to filter with dimensions (frames, height,\n                width, channels)\n            detections (list of tuples): List of detection results for each frame. Each element is\n                a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection\n                 probabilities, both as float\n\n        Returns:\n            np.ndarray: An array of video frames of length n_frames or shorter\n        \"\"\"\n\n        frame_scores = pd.Series(\n            [(np.max(score) if (len(score) &gt; 0) else 0) for _, score in detections]\n        ).sort_values(\n            ascending=False\n        )  # reduce to one score per frame\n\n        selected_indices = frame_scores.loc[frame_scores &gt; self.config.confidence].index\n\n        if self.config.n_frames is None:\n            # no minimum n_frames provided, just select all the frames with scores &gt; threshold\n            pass\n\n        elif len(selected_indices) &gt;= self.config.n_frames:\n            # num. frames with scores &gt; threshold is greater than the requested number of frames\n            selected_indices = (\n                frame_scores[selected_indices]\n                .sort_values(ascending=False)\n                .iloc[: self.config.n_frames]\n                .index\n            )\n\n        elif len(selected_indices) &lt; self.config.n_frames:\n            # num. frames with scores &gt; threshold is less than the requested number of frames\n            # repeat frames that are above threshold to get to n_frames\n            rng = np.random.RandomState(self.config.seed)\n\n            if self.config.fill_mode == \"repeat\":\n                repeated_indices = rng.choice(\n                    selected_indices,\n                    self.config.n_frames - len(selected_indices),\n                    replace=True,\n                )\n                selected_indices = np.concatenate((selected_indices, repeated_indices))\n\n            # take frames in sorted order up to n_frames, even if score is zero\n            elif self.config.fill_mode == \"score_sorted\":\n                selected_indices = (\n                    frame_scores.sort_values(ascending=False).iloc[: self.config.n_frames].index\n                )\n\n            # sample up to n_frames, prefer points closer to frames with detection\n            elif self.config.fill_mode == \"weighted_euclidean\":\n                sample_from = frame_scores.loc[~frame_scores.index.isin(selected_indices)].index\n                # take one over euclidean distance to all points with detection\n                weights = [1 / np.linalg.norm(selected_indices - sample) for sample in sample_from]\n                # normalize weights\n                weights /= np.sum(weights)\n                sampled = rng.choice(\n                    sample_from,\n                    self.config.n_frames - len(selected_indices),\n                    replace=False,\n                    p=weights,\n                )\n\n                selected_indices = np.concatenate((selected_indices, sampled))\n\n            # sample up to n_frames, weight by predicted probability - only if some frames have nonzero prob\n            elif (self.config.fill_mode == \"weighted_prob\") and (len(selected_indices) &gt; 0):\n                sample_from = frame_scores.loc[~frame_scores.index.isin(selected_indices)].index\n                weights = frame_scores[sample_from] / np.sum(frame_scores[sample_from])\n                sampled = rng.choice(\n                    sample_from,\n                    self.config.n_frames - len(selected_indices),\n                    replace=False,\n                    p=weights,\n                )\n\n                selected_indices = np.concatenate((selected_indices, sampled))\n\n        # sort the selected images back into their original order\n        if self.config.sort_by_time:\n            selected_indices = sorted(selected_indices)\n\n        return frames[selected_indices]\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.MegadetectorLiteYoloX.__init__","title":"<code>__init__(path=LOCAL_MD_LITE_MODEL, kwargs=LOCAL_MD_LITE_MODEL_KWARGS, config=None)</code>","text":"<p>MegadetectorLite based on YOLOX.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>pathlike</code> <p>Path to trained YoloX model checkpoint (.pth extension)</p> <code>LOCAL_MD_LITE_MODEL</code> <code>config</code> <code>MegadetectorLiteYoloXConfig</code> <p>YoloX configuration</p> <code>None</code> Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>def __init__(\n    self,\n    path: os.PathLike = LOCAL_MD_LITE_MODEL,\n    kwargs: os.PathLike = LOCAL_MD_LITE_MODEL_KWARGS,\n    config: Optional[Union[MegadetectorLiteYoloXConfig, dict]] = None,\n):\n    \"\"\"MegadetectorLite based on YOLOX.\n\n    Args:\n        path (pathlike): Path to trained YoloX model checkpoint (.pth extension)\n        config (MegadetectorLiteYoloXConfig): YoloX configuration\n    \"\"\"\n    if config is None:\n        config = MegadetectorLiteYoloXConfig()\n    elif isinstance(config, dict):\n        config = MegadetectorLiteYoloXConfig.parse_obj(config)\n\n    yolox = YoloXModel.load(\n        checkpoint=path,\n        model_kwargs_path=kwargs,\n    )\n\n    ckpt = torch.load(yolox.args.ckpt, weights_only=False, map_location=config.device)\n    model = yolox.exp.get_model()\n    model.load_state_dict(ckpt[\"model\"])\n    model = model.eval().to(config.device)\n\n    self.model = model\n    self.yolox = yolox\n    self.config = config\n    self.num_classes = yolox.exp.num_classes\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.MegadetectorLiteYoloX.detect_image","title":"<code>detect_image(img_arr)</code>","text":"<p>Runs object detection on an image.</p> <p>Parameters:</p> Name Type Description Default <code>img_arr</code> <code>ndarray</code> <p>An image array with dimensions (height, width, channels).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y2).</p> <code>ndarray</code> <p>np.ndarray: An array of object detection confidence scores of length (object) where object is the number of objects detected.</p> Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>def detect_image(self, img_arr: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Runs object detection on an image.\n\n    Args:\n        img_arr (np.ndarray): An image array with dimensions (height, width, channels).\n\n    Returns:\n        np.ndarray: An array of bounding box detections with dimensions (object, 4) where\n            object is the number of objects detected and the other 4 dimension are\n            (x1, y1, x2, y2).\n\n        np.ndarray: An array of object detection confidence scores of length (object) where\n            object is the number of objects detected.\n    \"\"\"\n    with torch.no_grad():\n        outputs = self.model(\n            torch.from_numpy(self._preprocess(img_arr)).unsqueeze(0).to(self.config.device)\n        )\n        output = postprocess(\n            outputs, self.num_classes, self.config.confidence, self.config.nms_threshold\n        )\n\n    return self._process_frame_output(output[0], img_arr.shape[0], img_arr.shape[1])\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.MegadetectorLiteYoloX.detect_video","title":"<code>detect_video(video_arr, pbar=False)</code>","text":"<p>Runs object detection on an video.</p> <p>Parameters:</p> Name Type Description Default <code>video_arr</code> <code>ndarray</code> <p>An video array with dimensions (frames, height, width, channels).</p> required <code>pbar</code> <code>int</code> <p>Whether to show progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing detections and score for each frame. Each tuple contains two arrays: the first is an array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). The second is an array of object detection confidence scores of length (object) where object is the number of objects detected.</p> Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>def detect_video(self, video_arr: np.ndarray, pbar: bool = False):\n    \"\"\"Runs object detection on an video.\n\n    Args:\n        video_arr (np.ndarray): An video array with dimensions (frames, height, width, channels).\n        pbar (int): Whether to show progress bar. Defaults to False.\n\n    Returns:\n        list: A list containing detections and score for each frame. Each tuple contains two arrays:\n            the first is an array of bounding box detections with dimensions (object, 4) where\n            object is the number of objects detected and the other 4 dimension are\n            (x1, y1, x2, y1). The second is an array of object detection confidence scores of\n            length (object) where object is the number of objects detected.\n    \"\"\"\n\n    pbar = tqdm if pbar else lambda x: x\n\n    # batch of frames\n    batch_size = self.config.frame_batch_size\n\n    video_outputs = []\n    with torch.no_grad():\n        for i in range(0, len(video_arr), batch_size):\n            a = video_arr[i : i + batch_size]\n\n            outputs = self.model(\n                torch.from_numpy(self._preprocess_video(a)).to(self.config.device)\n            )\n            outputs = postprocess(\n                outputs, self.num_classes, self.config.confidence, self.config.nms_threshold\n            )\n            video_outputs.extend(outputs)\n\n    detections = []\n    for o in pbar(video_outputs):\n        detections.append(\n            self._process_frame_output(\n                o, original_height=video_arr.shape[1], original_width=video_arr.shape[2]\n            )\n        )\n\n    return detections\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.MegadetectorLiteYoloX.filter_frames","title":"<code>filter_frames(frames, detections)</code>","text":"<p>Filter video frames using megadetector lite.</p> <p>Which frames are returned depends on the fill_mode and how many frames are above the confidence threshold. If more than n_frames are above the threshold, the top n_frames are returned. Otherwise add to those over threshold based on fill_mode. If none of these conditions are met, returns all frames above the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>ndarray</code> <p>Array of video frames to filter with dimensions (frames, height, width, channels)</p> required <code>detections</code> <code>list of tuples</code> <p>List of detection results for each frame. Each element is a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection  probabilities, both as float</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of video frames of length n_frames or shorter</p> Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>def filter_frames(\n    self, frames: np.ndarray, detections: List[Tuple[float, float, float, float]]\n) -&gt; np.ndarray:\n    \"\"\"Filter video frames using megadetector lite.\n\n    Which frames are returned depends on the fill_mode and how many frames are above the\n    confidence threshold. If more than n_frames are above the threshold, the top n_frames are\n    returned. Otherwise add to those over threshold based on fill_mode. If none of these\n    conditions are met, returns all frames above the threshold.\n\n    Args:\n        frames (np.ndarray): Array of video frames to filter with dimensions (frames, height,\n            width, channels)\n        detections (list of tuples): List of detection results for each frame. Each element is\n            a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection\n             probabilities, both as float\n\n    Returns:\n        np.ndarray: An array of video frames of length n_frames or shorter\n    \"\"\"\n\n    frame_scores = pd.Series(\n        [(np.max(score) if (len(score) &gt; 0) else 0) for _, score in detections]\n    ).sort_values(\n        ascending=False\n    )  # reduce to one score per frame\n\n    selected_indices = frame_scores.loc[frame_scores &gt; self.config.confidence].index\n\n    if self.config.n_frames is None:\n        # no minimum n_frames provided, just select all the frames with scores &gt; threshold\n        pass\n\n    elif len(selected_indices) &gt;= self.config.n_frames:\n        # num. frames with scores &gt; threshold is greater than the requested number of frames\n        selected_indices = (\n            frame_scores[selected_indices]\n            .sort_values(ascending=False)\n            .iloc[: self.config.n_frames]\n            .index\n        )\n\n    elif len(selected_indices) &lt; self.config.n_frames:\n        # num. frames with scores &gt; threshold is less than the requested number of frames\n        # repeat frames that are above threshold to get to n_frames\n        rng = np.random.RandomState(self.config.seed)\n\n        if self.config.fill_mode == \"repeat\":\n            repeated_indices = rng.choice(\n                selected_indices,\n                self.config.n_frames - len(selected_indices),\n                replace=True,\n            )\n            selected_indices = np.concatenate((selected_indices, repeated_indices))\n\n        # take frames in sorted order up to n_frames, even if score is zero\n        elif self.config.fill_mode == \"score_sorted\":\n            selected_indices = (\n                frame_scores.sort_values(ascending=False).iloc[: self.config.n_frames].index\n            )\n\n        # sample up to n_frames, prefer points closer to frames with detection\n        elif self.config.fill_mode == \"weighted_euclidean\":\n            sample_from = frame_scores.loc[~frame_scores.index.isin(selected_indices)].index\n            # take one over euclidean distance to all points with detection\n            weights = [1 / np.linalg.norm(selected_indices - sample) for sample in sample_from]\n            # normalize weights\n            weights /= np.sum(weights)\n            sampled = rng.choice(\n                sample_from,\n                self.config.n_frames - len(selected_indices),\n                replace=False,\n                p=weights,\n            )\n\n            selected_indices = np.concatenate((selected_indices, sampled))\n\n        # sample up to n_frames, weight by predicted probability - only if some frames have nonzero prob\n        elif (self.config.fill_mode == \"weighted_prob\") and (len(selected_indices) &gt; 0):\n            sample_from = frame_scores.loc[~frame_scores.index.isin(selected_indices)].index\n            weights = frame_scores[sample_from] / np.sum(frame_scores[sample_from])\n            sampled = rng.choice(\n                sample_from,\n                self.config.n_frames - len(selected_indices),\n                replace=False,\n                p=weights,\n            )\n\n            selected_indices = np.concatenate((selected_indices, sampled))\n\n    # sort the selected images back into their original order\n    if self.config.sort_by_time:\n        selected_indices = sorted(selected_indices)\n\n    return frames[selected_indices]\n</code></pre>"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox.MegadetectorLiteYoloXConfig","title":"<code>MegadetectorLiteYoloXConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a MegadetectorLiteYoloX frame selection model</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>float</code> <p>Only consider object detections with this confidence or greater</p> <code>nms_threshold</code> <code>float</code> <p>Non-maximum suppression is a method for filtering many bounding boxes around the same object to a single bounding box. This is a constant that determines how much to suppress similar bounding boxes.</p> <code>image_width</code> <code>int</code> <p>Scale image to this width before sending to object detection model.</p> <code>image_height</code> <code>int</code> <p>Scale image to this height before sending to object detection model.</p> <code>device</code> <code>str</code> <p>Where to run the object detection model, \"cpu\" or \"cuda\".</p> <code>frame_batch_size</code> <code>int</code> <p>Number of frames to predict on at once.</p> <code>n_frames</code> <code>int</code> <p>Max number of frames to return. If None returns all frames above the threshold. Defaults to None.</p> <code>fill_mode</code> <code>str</code> <p>Mode for upsampling if the number of frames above the threshold is less than n_frames. Defaults to \"repeat\".</p> <code>sort_by_time</code> <code>bool</code> <p>Whether to sort the selected frames by time (original order) before returning. If False, returns frames sorted by score (descending). Defaults to True.</p> <code>seed</code> <code>int</code> <p>Random state for random number generator. Defaults to 55.</p> Source code in <code>zamba/object_detection/yolox/megadetector_lite_yolox.py</code> <pre><code>class MegadetectorLiteYoloXConfig(BaseModel):\n    \"\"\"Configuration for a MegadetectorLiteYoloX frame selection model\n\n    Attributes:\n        confidence (float): Only consider object detections with this confidence or greater\n        nms_threshold (float): Non-maximum suppression is a method for filtering many bounding\n            boxes around the same object to a single bounding box. This is a constant that\n            determines how much to suppress similar bounding boxes.\n        image_width (int): Scale image to this width before sending to object detection model.\n        image_height (int): Scale image to this height before sending to object detection model.\n        device (str): Where to run the object detection model, \"cpu\" or \"cuda\".\n        frame_batch_size (int): Number of frames to predict on at once.\n        n_frames (int, optional): Max number of frames to return. If None returns all frames above\n            the threshold. Defaults to None.\n        fill_mode (str, optional): Mode for upsampling if the number of frames above the threshold\n            is less than n_frames. Defaults to \"repeat\".\n        sort_by_time (bool, optional): Whether to sort the selected frames by time (original order)\n            before returning. If False, returns frames sorted by score (descending). Defaults to\n            True.\n        seed (int, optional): Random state for random number generator. Defaults to 55.\n    \"\"\"\n\n    confidence: float = 0.25\n    nms_threshold: float = 0.45\n    image_width: int = 640\n    image_height: int = 640\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    frame_batch_size: int = 24\n    n_frames: Optional[int] = None\n    fill_mode: Optional[FillModeEnum] = FillModeEnum.score_sorted\n    sort_by_time: bool = True\n    seed: Optional[int] = 55\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"api-reference/pytorch-dataloaders/","title":"zamba.pytorch.dataloaders","text":""},{"location":"api-reference/pytorch-dataloaders/#zamba.pytorch.dataloaders.get_datasets","title":"<code>get_datasets(train_metadata=None, predict_metadata=None, transform=None, video_loader_config=None)</code>","text":"<p>Gets training and/or prediction datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_metadata</code> <code>pathlike</code> <p>Path to a CSV or DataFrame with columns: - filepath: path to a video, relative to <code>video_dir</code> - label:, label of the species that appears in the video - split (optional): If provided, \"train\", \"val\", or \"holdout\" indicating which dataset split the video will be included in. If not provided, and a \"site\" column exists, generate a site-specific split. Otherwise, generate a random split using <code>split_proportions</code>. - site (optional): If no \"split\" column, generate a site-specific split using the values in this column.</p> <code>None</code> <code>predict_metadata</code> <code>pathlike</code> <p>Path to a CSV or DataFrame with a \"filepath\" column.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[FfmpegZambaVideoDataset]</code> <p>A tuple of (train_dataset, val_dataset, test_dataset, predict_dataset) where each dataset</p> <code>Optional[FfmpegZambaVideoDataset]</code> <p>can be None if not specified.</p> Source code in <code>zamba/pytorch/dataloaders.py</code> <pre><code>def get_datasets(\n    train_metadata: Optional[pd.DataFrame] = None,\n    predict_metadata: Optional[pd.DataFrame] = None,\n    transform: Optional[torchvision.transforms.transforms.Compose] = None,\n    video_loader_config: Optional[VideoLoaderConfig] = None,\n) -&gt; Tuple[\n    Optional[\"FfmpegZambaVideoDataset\"],\n    Optional[\"FfmpegZambaVideoDataset\"],\n    Optional[\"FfmpegZambaVideoDataset\"],\n    Optional[\"FfmpegZambaVideoDataset\"],\n]:\n    \"\"\"Gets training and/or prediction datasets.\n\n    Args:\n        train_metadata (pathlike, optional): Path to a CSV or DataFrame with columns:\n          - filepath: path to a video, relative to `video_dir`\n          - label:, label of the species that appears in the video\n          - split (optional): If provided, \"train\", \"val\", or \"holdout\" indicating which dataset\n            split the video will be included in. If not provided, and a \"site\" column exists,\n            generate a site-specific split. Otherwise, generate a random split using\n            `split_proportions`.\n          - site (optional): If no \"split\" column, generate a site-specific split using the values\n            in this column.\n        predict_metadata (pathlike, optional): Path to a CSV or DataFrame with a \"filepath\" column.\n        transform (torchvision.transforms.transforms.Compose, optional)\n        video_loader_config (VideoLoaderConfig, optional)\n\n    Returns:\n        A tuple of (train_dataset, val_dataset, test_dataset, predict_dataset) where each dataset\n        can be None if not specified.\n    \"\"\"\n    if predict_metadata is not None:\n        # enable filtering the same way on all datasets\n        predict_metadata[\"species_\"] = 0\n\n    def subset_metadata_or_none(\n        metadata: Optional[pd.DataFrame] = None, subset: Optional[str] = None\n    ) -&gt; Optional[pd.DataFrame]:\n        if metadata is None:\n            return None\n        else:\n            metadata_subset = metadata.loc[metadata.split == subset] if subset else metadata\n            if len(metadata_subset) &gt; 0:\n                return FfmpegZambaVideoDataset(\n                    annotations=metadata_subset.set_index(\"filepath\").filter(regex=\"species\"),\n                    transform=transform,\n                    video_loader_config=video_loader_config,\n                )\n            else:\n                return None\n\n    train_dataset = subset_metadata_or_none(train_metadata, \"train\")\n    val_dataset = subset_metadata_or_none(train_metadata, \"val\")\n    test_dataset = subset_metadata_or_none(train_metadata, \"holdout\")\n    predict_dataset = subset_metadata_or_none(predict_metadata)\n\n    return train_dataset, val_dataset, test_dataset, predict_dataset\n</code></pre>"},{"location":"api-reference/pytorch-finetuning/","title":"zamba.pytorch.finetuning","text":""},{"location":"api-reference/pytorch-finetuning/#zamba.pytorch.finetuning.BackboneFinetuning","title":"<code>BackboneFinetuning</code>","text":"<p>               Bases: <code>BackboneFinetuning</code></p> <p>Derived from PTL's built-in <code>BackboneFinetuning</code>, but during the backbone freeze phase, choose whether to freeze batch norm layers, even if <code>train_bn</code> is True (i.e., even if we train them during the backbone unfreeze phase).</p> <p>Finetune a backbone model based on a learning rate user-defined scheduling. When the backbone learning rate reaches the current model learning rate and <code>should_align</code> is set to True, it will align with it for the rest of the training.</p> <p>Args:</p> <pre><code>unfreeze_backbone_at_epoch: Epoch at which the backbone will be unfreezed.\n\nlambda_func: Scheduling function for increasing backbone learning rate.\n\nbackbone_initial_ratio_lr:\n    Used to scale down the backbone learning rate compared to rest of model\n\nbackbone_initial_lr: Optional, Inital learning rate for the backbone.\n    By default, we will use current_learning /  backbone_initial_ratio_lr\n\nshould_align: Wheter to align with current learning rate when backbone learning\n    reaches it.\n\ninitial_denom_lr: When unfreezing the backbone, the intial learning rate will\n    current_learning_rate /  initial_denom_lr.\n\ntrain_bn: Wheter to make Batch Normalization trainable.\n\nverbose: Display current learning rate for model and backbone\n\nround: Precision for displaying learning rate\n</code></pre> <p>Example::</p> <pre><code>&gt;&gt;&gt; from pytorch_lightning import Trainer\n&gt;&gt;&gt; from pytorch_lightning.callbacks import BackboneFinetuning\n&gt;&gt;&gt; multiplicative = lambda epoch: 1.5\n&gt;&gt;&gt; backbone_finetuning = BackboneFinetuning(200, multiplicative)\n&gt;&gt;&gt; trainer = Trainer(callbacks=[backbone_finetuning])\n</code></pre> Source code in <code>zamba/pytorch/finetuning.py</code> <pre><code>class BackboneFinetuning(pl.callbacks.finetuning.BackboneFinetuning):\n    r\"\"\"\n\n    Derived from PTL's built-in ``BackboneFinetuning``, but during the backbone freeze phase,\n    choose whether to freeze batch norm layers, even if ``train_bn`` is True (i.e., even if we train them\n    during the backbone unfreeze phase).\n\n    Finetune a backbone model based on a learning rate user-defined scheduling.\n    When the backbone learning rate reaches the current model learning rate\n    and ``should_align`` is set to True, it will align with it for the rest of the training.\n\n    Args:\n\n        unfreeze_backbone_at_epoch: Epoch at which the backbone will be unfreezed.\n\n        lambda_func: Scheduling function for increasing backbone learning rate.\n\n        backbone_initial_ratio_lr:\n            Used to scale down the backbone learning rate compared to rest of model\n\n        backbone_initial_lr: Optional, Inital learning rate for the backbone.\n            By default, we will use current_learning /  backbone_initial_ratio_lr\n\n        should_align: Wheter to align with current learning rate when backbone learning\n            reaches it.\n\n        initial_denom_lr: When unfreezing the backbone, the intial learning rate will\n            current_learning_rate /  initial_denom_lr.\n\n        train_bn: Wheter to make Batch Normalization trainable.\n\n        verbose: Display current learning rate for model and backbone\n\n        round: Precision for displaying learning rate\n\n    Example::\n\n        &gt;&gt;&gt; from pytorch_lightning import Trainer\n        &gt;&gt;&gt; from pytorch_lightning.callbacks import BackboneFinetuning\n        &gt;&gt;&gt; multiplicative = lambda epoch: 1.5\n        &gt;&gt;&gt; backbone_finetuning = BackboneFinetuning(200, multiplicative)\n        &gt;&gt;&gt; trainer = Trainer(callbacks=[backbone_finetuning])\n\n    \"\"\"\n\n    def __init__(\n        self, *args, multiplier: Optional[float] = 1, pre_train_bn: bool = False, **kwargs\n    ):\n        if multiplier is not None:\n            kwargs[\"lambda_func\"] = multiplier_factory(multiplier)\n        super().__init__(*args, **kwargs)\n        # choose whether to train batch norm layers prior to finetuning phase\n        self.pre_train_bn = pre_train_bn\n\n    def freeze_before_training(self, pl_module: \"pl.LightningModule\"):\n        self.freeze(pl_module.backbone, train_bn=self.pre_train_bn)\n</code></pre>"},{"location":"api-reference/pytorch-finetuning/#zamba.pytorch.finetuning.multiplier_factory","title":"<code>multiplier_factory(rate)</code>","text":"<p>Returns a function that returns a constant value for use in computing a constant learning rate multiplier.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <code>float</code> <p>Constant multiplier.</p> required Source code in <code>zamba/pytorch/finetuning.py</code> <pre><code>def multiplier_factory(rate: float):\n    \"\"\"Returns a function that returns a constant value for use in computing a constant learning\n    rate multiplier.\n\n    Args:\n        rate (float): Constant multiplier.\n    \"\"\"\n\n    def multiplier(*args, **kwargs):\n        return rate\n\n    return multiplier\n</code></pre>"},{"location":"api-reference/pytorch-layers/","title":"zamba.pytorch.layers","text":""},{"location":"api-reference/pytorch-layers/#zamba.pytorch.layers.TimeDistributed","title":"<code>TimeDistributed</code>","text":"<p>               Bases: <code>Module</code></p> <p>Applies <code>module</code> over <code>tdim</code> identically for each step, use <code>low_mem</code> to compute one at a time.</p> <p>NOTE: vendored (with minor adaptations) from fastai: https://github.com/fastai/fastai/blob/4b0785254fdece1a44859956b6e54eedb167a97e/fastai/layers.py#L510-L544</p> Updates <ul> <li>super.init() in init</li> <li>assign attributes in init</li> <li>inherit from torch.nn.Module rather than fastai.Module</li> </ul> Source code in <code>zamba/pytorch/layers.py</code> <pre><code>class TimeDistributed(torch.nn.Module):\n    \"\"\"Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time.\n\n    NOTE: vendored (with minor adaptations) from fastai:\n    https://github.com/fastai/fastai/blob/4b0785254fdece1a44859956b6e54eedb167a97e/fastai/layers.py#L510-L544\n\n    Updates:\n     - super.__init__() in init\n     - assign attributes in init\n     - inherit from torch.nn.Module rather than fastai.Module\n    \"\"\"\n\n    def __init__(self, module, low_mem=False, tdim=1):\n        super().__init__()\n        self.low_mem = low_mem\n        self.tdim = tdim\n        self.module = module\n\n    def forward(self, *tensors, **kwargs):\n        \"input x with shape:(bs,seq_len,channels,width,height)\"\n        if self.low_mem or self.tdim != 1:\n            return self.low_mem_forward(*tensors, **kwargs)\n        else:\n            # only support tdim=1\n            inp_shape = tensors[0].shape\n            bs, seq_len = inp_shape[0], inp_shape[1]\n            out = self.module(*[x.view(bs * seq_len, *x.shape[2:]) for x in tensors], **kwargs)\n        return self.format_output(out, bs, seq_len)\n\n    def low_mem_forward(self, *tensors, **kwargs):\n        \"input x with shape:(bs,seq_len,channels,width,height)\"\n        seq_len = tensors[0].shape[self.tdim]\n        args_split = [torch.unbind(x, dim=self.tdim) for x in tensors]\n        out = []\n        for i in range(seq_len):\n            out.append(self.module(*[args[i] for args in args_split]), **kwargs)\n        if isinstance(out[0], tuple):\n            return _stack_tups(out, stack_dim=self.tdim)\n        return torch.stack(out, dim=self.tdim)\n\n    def format_output(self, out, bs, seq_len):\n        \"unstack from batchsize outputs\"\n        if isinstance(out, tuple):\n            return tuple(out_i.view(bs, seq_len, *out_i.shape[1:]) for out_i in out)\n        return out.view(bs, seq_len, *out.shape[1:])\n\n    def __repr__(self):\n        return f\"TimeDistributed({self.module})\"\n</code></pre>"},{"location":"api-reference/pytorch-layers/#zamba.pytorch.layers.TimeDistributed.format_output","title":"<code>format_output(out, bs, seq_len)</code>","text":"<p>unstack from batchsize outputs</p> Source code in <code>zamba/pytorch/layers.py</code> <pre><code>def format_output(self, out, bs, seq_len):\n    \"unstack from batchsize outputs\"\n    if isinstance(out, tuple):\n        return tuple(out_i.view(bs, seq_len, *out_i.shape[1:]) for out_i in out)\n    return out.view(bs, seq_len, *out.shape[1:])\n</code></pre>"},{"location":"api-reference/pytorch-layers/#zamba.pytorch.layers.TimeDistributed.forward","title":"<code>forward(*tensors, **kwargs)</code>","text":"<p>input x with shape:(bs,seq_len,channels,width,height)</p> Source code in <code>zamba/pytorch/layers.py</code> <pre><code>def forward(self, *tensors, **kwargs):\n    \"input x with shape:(bs,seq_len,channels,width,height)\"\n    if self.low_mem or self.tdim != 1:\n        return self.low_mem_forward(*tensors, **kwargs)\n    else:\n        # only support tdim=1\n        inp_shape = tensors[0].shape\n        bs, seq_len = inp_shape[0], inp_shape[1]\n        out = self.module(*[x.view(bs * seq_len, *x.shape[2:]) for x in tensors], **kwargs)\n    return self.format_output(out, bs, seq_len)\n</code></pre>"},{"location":"api-reference/pytorch-layers/#zamba.pytorch.layers.TimeDistributed.low_mem_forward","title":"<code>low_mem_forward(*tensors, **kwargs)</code>","text":"<p>input x with shape:(bs,seq_len,channels,width,height)</p> Source code in <code>zamba/pytorch/layers.py</code> <pre><code>def low_mem_forward(self, *tensors, **kwargs):\n    \"input x with shape:(bs,seq_len,channels,width,height)\"\n    seq_len = tensors[0].shape[self.tdim]\n    args_split = [torch.unbind(x, dim=self.tdim) for x in tensors]\n    out = []\n    for i in range(seq_len):\n        out.append(self.module(*[args[i] for args in args_split]), **kwargs)\n    if isinstance(out[0], tuple):\n        return _stack_tups(out, stack_dim=self.tdim)\n    return torch.stack(out, dim=self.tdim)\n</code></pre>"},{"location":"api-reference/pytorch-transforms/","title":"zamba.pytorch.transforms","text":""},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.ConvertHWCtoCHW","title":"<code>ConvertHWCtoCHW</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convert tensor from (0:H, 1:W, 2:C) to (2:C, 0:H, 1:W)</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>class ConvertHWCtoCHW(torch.nn.Module):\n    \"\"\"Convert tensor from (0:H, 1:W, 2:C) to (2:C, 0:H, 1:W)\"\"\"\n\n    def forward(self, vid: torch.Tensor) -&gt; torch.Tensor:\n        return vid.permute(2, 0, 1)\n</code></pre>"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.ConvertTCHWtoCTHW","title":"<code>ConvertTCHWtoCTHW</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convert tensor from (T, C, H, W) to (C, T, H, W)</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>class ConvertTCHWtoCTHW(torch.nn.Module):\n    \"\"\"Convert tensor from (T, C, H, W) to (C, T, H, W)\"\"\"\n\n    def forward(self, vid: torch.Tensor) -&gt; torch.Tensor:\n        return vid.permute(1, 0, 2, 3)\n</code></pre>"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.ConvertTHWCtoCTHW","title":"<code>ConvertTHWCtoCTHW</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convert tensor from (0:T, 1:H, 2:W, 3:C) to (3:C, 0:T, 1:H, 2:W)</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>class ConvertTHWCtoCTHW(torch.nn.Module):\n    \"\"\"Convert tensor from (0:T, 1:H, 2:W, 3:C) to (3:C, 0:T, 1:H, 2:W)\"\"\"\n\n    def forward(self, vid: torch.Tensor) -&gt; torch.Tensor:\n        return vid.permute(3, 0, 1, 2)\n</code></pre>"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.ConvertTHWCtoTCHW","title":"<code>ConvertTHWCtoTCHW</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convert tensor from (T, H, W, C) to (T, C, H, W)</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>class ConvertTHWCtoTCHW(torch.nn.Module):\n    \"\"\"Convert tensor from (T, H, W, C) to (T, C, H, W)\"\"\"\n\n    def forward(self, vid: torch.Tensor) -&gt; torch.Tensor:\n        return vid.permute(0, 3, 1, 2)\n</code></pre>"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.PackSlowFastPathways","title":"<code>PackSlowFastPathways</code>","text":"<p>               Bases: <code>Module</code></p> <p>Creates the slow and fast pathway inputs for the slowfast model.</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>class PackSlowFastPathways(torch.nn.Module):\n    \"\"\"Creates the slow and fast pathway inputs for the slowfast model.\"\"\"\n\n    def __init__(self, alpha: int = 4):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, frames: torch.Tensor):\n        fast_pathway = frames\n        # Perform temporal sampling from the fast pathway.\n        slow_pathway = torch.index_select(\n            frames,\n            1,\n            torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // self.alpha).long(),\n        )\n        frame_list = [slow_pathway, fast_pathway]\n        return frame_list\n</code></pre>"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.PadDimensions","title":"<code>PadDimensions</code>","text":"<p>               Bases: <code>Module</code></p> <p>Pads a tensor to ensure a fixed output dimension for a give axis.</p> <p>Attributes:</p> Name Type Description <code>dimension_sizes</code> <p>A tuple of int or None the same length as the number of dimensions in the input tensor. If int, pad that dimension to at least that size. If None, do not pad.</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>class PadDimensions(torch.nn.Module):\n    \"\"\"Pads a tensor to ensure a fixed output dimension for a give axis.\n\n    Attributes:\n        dimension_sizes: A tuple of int or None the same length as the number of dimensions in the\n            input tensor. If int, pad that dimension to at least that size. If None, do not pad.\n    \"\"\"\n\n    def __init__(self, dimension_sizes: Tuple[Optional[int]]):\n        super().__init__()\n        self.dimension_sizes = dimension_sizes\n\n    @staticmethod\n    def compute_left_and_right_pad(original_size: int, padded_size: int) -&gt; Tuple[int, int]:\n        \"\"\"Computes left and right pad size.\n\n        Args:\n            original_size (list, int): The original tensor size\n            padded_size (list, int): The desired tensor size\n\n        Returns:\n           Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1\n        \"\"\"\n        if original_size &gt;= padded_size:\n            return 0, 0\n        pad = padded_size - original_size\n        quotient, remainder = divmod(pad, 2)\n        return quotient, quotient + remainder\n\n    def forward(self, vid: torch.Tensor) -&gt; torch.Tensor:\n        padding = tuple(\n            itertools.chain.from_iterable(\n                (\n                    (0, 0)\n                    if padded_size is None\n                    else self.compute_left_and_right_pad(original_size, padded_size)\n                )\n                for original_size, padded_size in zip(vid.shape, self.dimension_sizes)\n            )\n        )\n        return torch.nn.functional.pad(vid, padding[::-1])\n</code></pre>"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms.PadDimensions.compute_left_and_right_pad","title":"<code>compute_left_and_right_pad(original_size, padded_size)</code>  <code>staticmethod</code>","text":"<p>Computes left and right pad size.</p> <p>Parameters:</p> Name Type Description Default <code>original_size</code> <code>(list, int)</code> <p>The original tensor size</p> required <code>padded_size</code> <code>(list, int)</code> <p>The desired tensor size</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1</p> Source code in <code>zamba/pytorch/transforms.py</code> <pre><code>@staticmethod\ndef compute_left_and_right_pad(original_size: int, padded_size: int) -&gt; Tuple[int, int]:\n    \"\"\"Computes left and right pad size.\n\n    Args:\n        original_size (list, int): The original tensor size\n        padded_size (list, int): The desired tensor size\n\n    Returns:\n       Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1\n    \"\"\"\n    if original_size &gt;= padded_size:\n        return 0, 0\n    pad = padded_size - original_size\n    quotient, remainder = divmod(pad, 2)\n    return quotient, quotient + remainder\n</code></pre>"},{"location":"api-reference/pytorch-utils/","title":"zamba.pytorch.utils","text":""},{"location":"api-reference/pytorch-utils/#zamba.pytorch.utils.build_multilayer_perceptron","title":"<code>build_multilayer_perceptron(input_size, hidden_layer_sizes, output_size, activation=torch.nn.ReLU, dropout=None, output_dropout=None, output_activation=None)</code>","text":"<p>Builds a multilayer perceptron.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Size of first input layer.</p> required <code>hidden_layer_sizes</code> <code>tuple of int</code> <p>If provided, size of hidden layers.</p> required <code>output_size</code> <code>int</code> <p>Size of the last output layer.</p> required <code>activation</code> <code>Module</code> <p>Activation layer between each pair of layers.</p> <code>ReLU</code> <code>dropout</code> <code>float</code> <p>If provided, insert dropout layers with the following dropout rate in between each pair of layers.</p> <code>None</code> <code>output_dropout</code> <code>float</code> <p>If provided, insert a dropout layer with the following dropout rate before the output.</p> <code>None</code> <code>output_activation</code> <code>Module</code> <p>Activation layer after the final layer.</p> <code>None</code> <p>Returns:     torch.nn.Sequential</p> Source code in <code>zamba/pytorch/utils.py</code> <pre><code>def build_multilayer_perceptron(\n    input_size: int,\n    hidden_layer_sizes: Optional[Tuple[int]],\n    output_size: int,\n    activation: Optional[torch.nn.Module] = torch.nn.ReLU,\n    dropout: Optional[float] = None,\n    output_dropout: Optional[float] = None,\n    output_activation: Optional[torch.nn.Module] = None,\n) -&gt; torch.nn.Sequential:\n    \"\"\"Builds a multilayer perceptron.\n\n    Args:\n        input_size (int): Size of first input layer.\n        hidden_layer_sizes (tuple of int, optional): If provided, size of hidden layers.\n        output_size (int): Size of the last output layer.\n        activation (torch.nn.Module, optional): Activation layer between each pair of layers.\n        dropout (float, optional): If provided, insert dropout layers with the following dropout\n            rate in between each pair of layers.\n        output_dropout (float, optional): If provided, insert a dropout layer with the following\n            dropout rate before the output.\n        output_activation (torch.nn.Module, optional): Activation layer after the final layer.\n    Returns:\n        torch.nn.Sequential\n    \"\"\"\n    if (hidden_layer_sizes is None) or len(hidden_layer_sizes) == 0:\n        return torch.nn.Linear(input_size, output_size)\n\n    layers = [torch.nn.Linear(input_size, hidden_layer_sizes[0])]\n    if activation is not None:\n        layers.append(activation())\n\n    if (dropout is not None) and (dropout &gt; 0):\n        layers.append(torch.nn.Dropout(dropout))\n\n    for in_size, out_size in zip(hidden_layer_sizes[:-1], hidden_layer_sizes[1:]):\n        layers.append(torch.nn.Linear(in_size, out_size))\n        if activation is not None:\n            layers.append(activation())\n\n        if (dropout is not None) and (dropout &gt; 0):\n            layers.append(torch.nn.Dropout(dropout))\n\n    layers.append(torch.nn.Linear(hidden_layer_sizes[-1], output_size))\n\n    if (output_dropout is not None) and (output_dropout &gt; 0):\n        layers.append(torch.nn.Dropout(dropout))\n\n    if output_activation is not None:\n        layers.append(output_activation())\n\n    return torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"api-reference/pytorch_lightning-utils/","title":"zamba.pytorch_lightning.video_modules","text":""},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.video_modules.ZambaVideoClassificationLightningModule","title":"<code>ZambaVideoClassificationLightningModule</code>","text":"<p>               Bases: <code>ZambaClassificationLightningModule</code></p> Source code in <code>zamba/pytorch_lightning/video_modules.py</code> <pre><code>class ZambaVideoClassificationLightningModule(ZambaClassificationLightningModule):\n    def on_train_start(self):\n        metrics = {\"val_macro_f1\": {}}\n\n        if self.num_classes &gt; 2:\n            metrics.update(\n                {f\"val_top_{k}_accuracy\": {} for k in DEFAULT_TOP_K if k &lt; self.num_classes}\n            )\n        else:\n            metrics.update({\"val_accuracy\": {}})\n\n        # write hparams to hparams.yaml file, log metrics to tb hparams tab\n        self.logger.log_hyperparams(self.hparams, metrics)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"train_loss\", loss.detach())\n        self.training_step_outputs.append(loss)\n        return loss\n\n    def _val_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"val_loss\", loss.detach())\n\n        y_proba = torch.sigmoid(y_hat.cpu()).numpy()\n        return {\n            \"y_true\": y.cpu().numpy().astype(int),\n            \"y_pred\": y_proba.round().astype(int),\n            \"y_proba\": y_proba,\n        }\n\n    def validation_step(self, batch, batch_idx):\n        output = self._val_step(batch, batch_idx)\n        self.validation_step_outputs.append(output)\n        return output\n\n    def test_step(self, batch, batch_idx):\n        output = self._val_step(batch, batch_idx)\n        self.test_step_outputs.append(output)\n        return output\n\n    @staticmethod\n    def aggregate_step_outputs(\n        outputs: Dict[str, np.ndarray],\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        y_true = np.vstack([output[\"y_true\"] for output in outputs])\n        y_pred = np.vstack([output[\"y_pred\"] for output in outputs])\n        y_proba = np.vstack([output[\"y_proba\"] for output in outputs])\n\n        return y_true, y_pred, y_proba\n\n    def compute_and_log_metrics(\n        self, y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray, subset: str\n    ):\n        self.log(\n            f\"{subset}_macro_f1\",\n            f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        )\n\n        # if only two classes, skip top_k accuracy since not enough classes\n        if self.num_classes &gt; 2:\n            for k in DEFAULT_TOP_K:\n                if k &lt; self.num_classes:\n                    self.log(\n                        f\"{subset}_top_{k}_accuracy\",\n                        top_k_accuracy_score(\n                            y_true.argmax(\n                                axis=1\n                            ),  # top k accuracy only supports single label case\n                            y_proba,\n                            labels=np.arange(y_proba.shape[1]),\n                            k=k,\n                        ),\n                    )\n        else:\n            self.log(f\"{subset}_accuracy\", accuracy_score(y_true, y_pred))\n\n        for metric_name, label, metric in compute_species_specific_metrics(\n            y_true, y_pred, self.species\n        ):\n            self.log(f\"species/{subset}_{metric_name}/{label}\", metric)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K\n        metrics.\n\n        Args:\n            outputs (List[dict]): list of output dictionaries from each validation step\n                containing y_pred and y_true.\n        \"\"\"\n        y_true, y_pred, y_proba = self.aggregate_step_outputs(self.validation_step_outputs)\n        self.compute_and_log_metrics(y_true, y_pred, y_proba, subset=\"val\")\n        self.validation_step_outputs.clear()  # free memory\n\n    def on_test_epoch_end(self):\n        y_true, y_pred, y_proba = self.aggregate_step_outputs(self.test_step_outputs)\n        self.compute_and_log_metrics(y_true, y_pred, y_proba, subset=\"test\")\n        self.test_step_outputs.clear()  # free memory\n\n    def predict_step(self, batch, batch_idx, dataloader_idx: Optional[int] = None):\n        x, y = batch\n        y_hat = self(x)\n        pred = torch.sigmoid(y_hat).cpu().numpy()\n        return pred\n</code></pre>"},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.video_modules.ZambaVideoClassificationLightningModule.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>List[dict]</code> <p>list of output dictionaries from each validation step containing y_pred and y_true.</p> required Source code in <code>zamba/pytorch_lightning/video_modules.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K\n    metrics.\n\n    Args:\n        outputs (List[dict]): list of output dictionaries from each validation step\n            containing y_pred and y_true.\n    \"\"\"\n    y_true, y_pred, y_proba = self.aggregate_step_outputs(self.validation_step_outputs)\n    self.compute_and_log_metrics(y_true, y_pred, y_proba, subset=\"val\")\n    self.validation_step_outputs.clear()  # free memory\n</code></pre>"},{"location":"api-reference/pytorch_lightning-video_modules/","title":"zamba.pytorch_lightning.video_modules","text":""},{"location":"api-reference/pytorch_lightning-video_modules/#zamba.pytorch_lightning.video_modules.ZambaVideoClassificationLightningModule","title":"<code>ZambaVideoClassificationLightningModule</code>","text":"<p>               Bases: <code>ZambaClassificationLightningModule</code></p> Source code in <code>zamba/pytorch_lightning/video_modules.py</code> <pre><code>class ZambaVideoClassificationLightningModule(ZambaClassificationLightningModule):\n    def on_train_start(self):\n        metrics = {\"val_macro_f1\": {}}\n\n        if self.num_classes &gt; 2:\n            metrics.update(\n                {f\"val_top_{k}_accuracy\": {} for k in DEFAULT_TOP_K if k &lt; self.num_classes}\n            )\n        else:\n            metrics.update({\"val_accuracy\": {}})\n\n        # write hparams to hparams.yaml file, log metrics to tb hparams tab\n        self.logger.log_hyperparams(self.hparams, metrics)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"train_loss\", loss.detach())\n        self.training_step_outputs.append(loss)\n        return loss\n\n    def _val_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"val_loss\", loss.detach())\n\n        y_proba = torch.sigmoid(y_hat.cpu()).numpy()\n        return {\n            \"y_true\": y.cpu().numpy().astype(int),\n            \"y_pred\": y_proba.round().astype(int),\n            \"y_proba\": y_proba,\n        }\n\n    def validation_step(self, batch, batch_idx):\n        output = self._val_step(batch, batch_idx)\n        self.validation_step_outputs.append(output)\n        return output\n\n    def test_step(self, batch, batch_idx):\n        output = self._val_step(batch, batch_idx)\n        self.test_step_outputs.append(output)\n        return output\n\n    @staticmethod\n    def aggregate_step_outputs(\n        outputs: Dict[str, np.ndarray],\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        y_true = np.vstack([output[\"y_true\"] for output in outputs])\n        y_pred = np.vstack([output[\"y_pred\"] for output in outputs])\n        y_proba = np.vstack([output[\"y_proba\"] for output in outputs])\n\n        return y_true, y_pred, y_proba\n\n    def compute_and_log_metrics(\n        self, y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray, subset: str\n    ):\n        self.log(\n            f\"{subset}_macro_f1\",\n            f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        )\n\n        # if only two classes, skip top_k accuracy since not enough classes\n        if self.num_classes &gt; 2:\n            for k in DEFAULT_TOP_K:\n                if k &lt; self.num_classes:\n                    self.log(\n                        f\"{subset}_top_{k}_accuracy\",\n                        top_k_accuracy_score(\n                            y_true.argmax(\n                                axis=1\n                            ),  # top k accuracy only supports single label case\n                            y_proba,\n                            labels=np.arange(y_proba.shape[1]),\n                            k=k,\n                        ),\n                    )\n        else:\n            self.log(f\"{subset}_accuracy\", accuracy_score(y_true, y_pred))\n\n        for metric_name, label, metric in compute_species_specific_metrics(\n            y_true, y_pred, self.species\n        ):\n            self.log(f\"species/{subset}_{metric_name}/{label}\", metric)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K\n        metrics.\n\n        Args:\n            outputs (List[dict]): list of output dictionaries from each validation step\n                containing y_pred and y_true.\n        \"\"\"\n        y_true, y_pred, y_proba = self.aggregate_step_outputs(self.validation_step_outputs)\n        self.compute_and_log_metrics(y_true, y_pred, y_proba, subset=\"val\")\n        self.validation_step_outputs.clear()  # free memory\n\n    def on_test_epoch_end(self):\n        y_true, y_pred, y_proba = self.aggregate_step_outputs(self.test_step_outputs)\n        self.compute_and_log_metrics(y_true, y_pred, y_proba, subset=\"test\")\n        self.test_step_outputs.clear()  # free memory\n\n    def predict_step(self, batch, batch_idx, dataloader_idx: Optional[int] = None):\n        x, y = batch\n        y_hat = self(x)\n        pred = torch.sigmoid(y_hat).cpu().numpy()\n        return pred\n</code></pre>"},{"location":"api-reference/pytorch_lightning-video_modules/#zamba.pytorch_lightning.video_modules.ZambaVideoClassificationLightningModule.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>List[dict]</code> <p>list of output dictionaries from each validation step containing y_pred and y_true.</p> required Source code in <code>zamba/pytorch_lightning/video_modules.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K\n    metrics.\n\n    Args:\n        outputs (List[dict]): list of output dictionaries from each validation step\n            containing y_pred and y_true.\n    \"\"\"\n    y_true, y_pred, y_proba = self.aggregate_step_outputs(self.validation_step_outputs)\n    self.compute_and_log_metrics(y_true, y_pred, y_proba, subset=\"val\")\n    self.validation_step_outputs.clear()  # free memory\n</code></pre>"},{"location":"api-reference/settings/","title":"zamba.settings","text":""},{"location":"changelog/","title":"<code>zamba</code> changelog","text":""},{"location":"changelog/#v261-2025-03-18","title":"v.2.6.1 (2025-03-18)","text":"<ul> <li>Fix bug that prevented loading checkpoints when model name was provided (PR #359)</li> </ul>"},{"location":"changelog/#v260-2025-03-14","title":"v.2.6.0 (2025-03-14)","text":"<ul> <li>Added support for training and classifying on images (PR #349)</li> </ul>"},{"location":"changelog/#v250-2024-09-27","title":"v.2.5.0 (2024-09-27)","text":"<ul> <li>Removed support for Python 3.8 and 3.9 plus other requirement updates (PR #337, PR #335). New minimum python version is 3.11.</li> </ul>"},{"location":"changelog/#v241-2024-04-20","title":"v2.4.1 (2024-04-20)","text":"<ul> <li>Bug fixes for docs</li> </ul>"},{"location":"changelog/#v240-2024-04-19","title":"v2.4.0 (2024-04-19)","text":"<ul> <li>Adds experimental image support (PR #314)</li> <li>Clarifies installation instructions for Linux and Windows operating systems (PR #299)</li> </ul>"},{"location":"changelog/#v232-2023-07-17","title":"v2.3.2 (2023-07-17)","text":"<ul> <li>Pin Pydantic to less than v2.0 (PR #277)</li> </ul>"},{"location":"changelog/#v231-2023-05-12","title":"v2.3.1 (2023-05-12)","text":"<ul> <li>Code updates for PyTorch Lightning v2.0.0 (PR #266, PR #272)</li> <li>Switch to pyproject.toml-based build and other requirement updates (PR #254, PR #255, PR #260, PR #262, PR #268)</li> </ul>"},{"location":"changelog/#v230-2022-12-01","title":"v2.3.0 (2022-12-01)","text":""},{"location":"changelog/#model-release","title":"Model release","text":"<ul> <li>Adds a depth estimation module for predicting the distance between animals and the camera (PR #247). This model comes from one of the winning solutions in the Deep Chimpact: Depth Estimation for Wildlife Conservation machine learning challenge hosted by DrivenData.</li> </ul>"},{"location":"changelog/#v224-2022-11-10","title":"v2.2.4 (2022-11-10)","text":"<ul> <li>Do not cache videos if the <code>VIDEO_CACHE_DIR</code> environment variable is an empty string or zero (PR #245)</li> </ul>"},{"location":"changelog/#v223-2022-11-01","title":"v2.2.3 (2022-11-01)","text":"<ul> <li>Fixes Lightning deprecation of DDPPlugin (PR #244)</li> </ul>"},{"location":"changelog/#v222-2022-10-04","title":"v2.2.2 (2022-10-04)","text":"<ul> <li>Adds a page to the docs summarizing the performance of the African species classification model on a holdout set (PR #235)</li> </ul>"},{"location":"changelog/#v221-2022-09-27","title":"v2.2.1 (2022-09-27)","text":"<ul> <li>Turn off showing local variables in Typer's exception and error handling (PR #237)</li> <li>Fixes bug where the column order was incorrect for training models when the provided labels are a subset of the model's default labels (PR #236)</li> </ul>"},{"location":"changelog/#v220-2022-09-26","title":"v2.2.0 (2022-09-26)","text":""},{"location":"changelog/#model-releases-and-new-features","title":"Model releases and new features","text":"<ul> <li>The default <code>time_distributed</code> model (African species classification) has been retrained on over 250,000 videos. This 16x increase in training data significantly improves accuracy. This new version replaces the previous one. (PR #226, PR #232)</li> <li>A new default model option is added: <code>blank_nonblank</code>. This model only does blank detection. This binary model can be trained and finetuned in the same way as the species classification models. This model was trained on both African and European data, totaling over 263,000 training videos. (PR #228)</li> <li>Detect if a user is training in a binary model and preprocess the labels accordingly (PR #215)</li> </ul>"},{"location":"changelog/#bug-fixes-and-improvements","title":"Bug fixes and improvements","text":"<ul> <li>Add a validator to ensure that using a model\u2019s default labels is only possible when the species in the provided labels file are a subset of those (PR #229)</li> <li>Refactor the logic in <code>instantiate_model</code> for clarity (PR #229)</li> <li>Use pqdm to check for missing files in parallel (PR #224)</li> <li>Set <code>model_name</code> based on the provided checkpoint so that user-trained models use the appropriate video loader config (PR #221)</li> <li>Leave <code>data_dir</code> as a relative path (PR #219)</li> <li>Ensure hparams yaml files get included in the source distribution (PR #210)</li> <li>Hold back setuptools so mkdocstrings works (PR #207)</li> <li>Factor out <code>get_cached_array_path</code> (PR #202)</li> </ul>"},{"location":"changelog/#v210-2022-07-15","title":"v2.1.0 (2022-07-15)","text":"<ul> <li>Retrains the time distributed species classification model using the updated MegadetectorLite frame selection (PR #199)</li> <li>Replaces the MegadetectorLite frame selection model with an improved model trained on significantly more data (PR #195)</li> </ul>"},{"location":"changelog/#v204-2022-06-17","title":"v2.0.4 (2022-06-17)","text":"<ul> <li>Pins <code>thop</code> to an earlier version (PR #191)</li> <li>Fixes caching so a previously downloaded checkpoint file actually gets used (PR #190, PR #194)</li> <li>Removes a lightning deprecation warning for DDP (PR #187)</li> <li>Ignores extra columns in the user-provided labels or filepaths csv (PR #186)</li> </ul>"},{"location":"changelog/#v203-2022-05-06","title":"v2.0.3 (2022-05-06)","text":"<p>Releasing to pick up #179.</p> <ul> <li>PR #179 removes the DensePose extra from the default dev requirements and tests. Docs are updated to clarify how to install and run tests for DensePose.</li> </ul>"},{"location":"changelog/#v202-2021-12-21","title":"v2.0.2 (2021-12-21)","text":"<p>Releasing to pick up #172.</p> <ul> <li>PR #172 fixes bug where video loading that uses the YoloX model (all of the built in models) resulted in videos not being able to load.</li> </ul>"},{"location":"changelog/#v201-2021-12-15","title":"v2.0.1 (2021-12-15)","text":"<p>Releasing to pick up #167 and #169.</p> <ul> <li>PR #169 fixes error in splitting data into train/test/val when only a few videos.</li> <li>PR #167 refactors yolox into an <code>object_detection</code> module</li> </ul> <p>Other documentation fixes also included.</p>"},{"location":"changelog/#v200-2021-10-22","title":"v2.0.0 (2021-10-22)","text":""},{"location":"changelog/#previous-model-machine-learning-competition","title":"Previous model: Machine learning competition","text":"<p>The algorithms used by <code>zamba</code> v1 were based on the winning solution from the Pri-matrix Factorization machine learning competition, hosted by DrivenData. Data for the competition was provided by the Chimp&amp;See project and manually labeled by volunteers. The competition had over 300 participants and over 450 submissions throughout the three month challenge. The v1 algorithm was adapted from the winning competition submission, with some aspects changed during development to improve performance.</p> <p>The core algorithm in <code>zamba</code> v1 was a stacked ensemble which consisted of a first layer of models that were then combined into a final prediction in a second layer. The first level of the stack consisted of 5 <code>keras</code> deep learning models, whose individual predictions were combined in the second level of the stack to form the final prediction.</p> <p>In v2, the stacked ensemble algorithm from v1 is replaced with three more powerful single-model options: <code>time_distributed</code>, <code>slowfast</code>, and <code>european</code>. The new models utilize state-of-the-art image and video classification architectures, and are able to outperform the much more computationally intensive stacked ensemble model.</p>"},{"location":"changelog/#new-geographies-and-species","title":"New geographies and species","text":"<p><code>zamba</code> v2 incorporates data from Western Europe (Germany). The new data is packaged in the pretrained <code>european</code> model, which can predict 11 common European species not present in <code>zamba</code> v1.</p> <p><code>zamba</code> v2 also incorporates new training data from 15 countries in central and west Africa, and adds 12 additional species to the pretrained African models.</p>"},{"location":"changelog/#retraining-flexibility","title":"Retraining flexibility","text":"<p>Model training is made available <code>zamba</code> v2, so users can finetune a pretrained model using their own data to improve performance for a specific ecology or set of sites. <code>zamba</code> v2 also allows users to retrain a model on completely new species labels.</p>"},{"location":"contribute/","title":"Help make <code>zamba</code> better","text":"<p><code>zamba</code> is an open source project, which means you can help make it better!</p>"},{"location":"contribute/#develop-the-github-repository","title":"Develop the GitHub repository","text":"<p>To get involved, check out the GitHub code repository. There you can find open issues with comments and links to help you along.</p> <p><code>zamba</code> uses continuous integration and test-driven development to ensure that we always have a working project. So what are you waiting for? <code>git</code> going!</p>"},{"location":"contribute/#installation-for-development","title":"Installation for development","text":"<p>To install <code>zamba</code> for development, you need to clone the git repository and then install the cloned version of the library for local development.</p> <p>To install for development: <pre><code>$ git clone https://github.com/drivendataorg/zamba.git\n$ cd zamba\n$ pip install -r requirements-dev.txt\n</code></pre></p> <p>If your contribution is to the DensePose model, you will need to install the additional dependencies with: <pre><code>$ pip install -e .[densepose]\n</code></pre></p>"},{"location":"contribute/#running-the-zamba-test-suite","title":"Running the <code>zamba</code> test suite","text":"<p>The included <code>Makefile</code> contains code that uses pytest to run all tests in <code>zamba/tests</code>.</p> <p>The command is (from the project root):</p> <pre><code>$ make tests\n</code></pre> <p>For DensePose related tests, the command is: <pre><code>$ make densepose-tests\n</code></pre></p>"},{"location":"contribute/#submit-additional-training-videos","title":"Submit additional training videos","text":"<p>If you have additional labeled videos that may be useful for improving the basic models that ship with <code>zamba</code>, we'd love to hear from you! You can get in touch at info@drivendata.org</p>"},{"location":"models/densepose/","title":"DensePose","text":""},{"location":"models/densepose/#background","title":"Background","text":"<p>DensePose (Neverova et al, 2021) is a model published by Facebook AI Research that can be used to get segmentations for animals that appear in videos. The model was trained on the following animals, but often works for other species as well: bear, cat, cow, dog, elephant, giraffe, horse, sheep, zebra. Here's an example of the segmentation output for a frame:</p> <p></p> <p>Additionally, the model provides mapping of the segmentation output to specific anatomy for chimpanzees. This can be helpful for determining the orientation of chimpanzees in videos and for understanding their behaviors. Here is an example of what that output looks like:</p> <p></p> <p>For more information on the algorithms and outputs of the DensePose model, see the Facebook DensePose Github Repository.</p>"},{"location":"models/densepose/#outputs","title":"Outputs","text":"<p>The Zamba package supports running DensePose on videos to generate three types of outputs:</p> <ul> <li>A <code>.json</code> file with details of segmentations per video frame.</li> <li>A <code>.mp4</code> file where the original video has the segmentation rendered on top of animal so that the output can be visually inspected.</li> <li>A <code>.csv</code> that contains the height and width of the bounding box around each chimpanzee, the frame number and timestamp of the observation, and the percentage of pixels in the bounding box that correspond with each anatomical part. This is specified by adding <code>--output-type chimp_anatomy</code>.</li> </ul> <p>Running the DensePose model is fairly computationally intensive. It is recommended to run the model at a relatively low framerate (e.g., 1 frame per second) to generate outputs for a video. JSON output files can also be quite large because they contain the full embedding. These are not written out by default.</p>"},{"location":"models/densepose/#installation","title":"Installation","text":"<p>In order to use the DensePose model, you must have PyTorch already installed on your system. Then you must install the <code>densepose</code> extra:</p> <pre><code>pip install torch\npip install \"https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz#egg=zamba[densepose]\"\n</code></pre>"},{"location":"models/densepose/#running-densepose","title":"Running DensePose","text":"<p>Once that is done, here's how to run the DensePose model:</p> CLI <pre><code># create a segmentation output video for each input video in PATH_TO_VIDEOS\nzamba densepose --data-dir PATH_TO_VIDEOS --render-output\n</code></pre> Python <pre><code>from zamba.models.densepose import DensePoseConfig\ndensepose_conf = DensePoseConfig(data_dir=\"PATH_TO_VIDEOS\", render_output=True)\ndensepose_conf.run_model()\n</code></pre>"},{"location":"models/densepose/#getting-help","title":"Getting help","text":"<p>To see all of the available options, run <code>zamba densepose --help</code>.</p> <pre><code>$ zamba densepose --help\nUsage: zamba densepose [OPTIONS]\n\n  Run densepose algorithm on videos.\n\n  If an argument is specified in both the command line and in a yaml file, the\n  command line input will take precedence.\n\nOptions:\n  --data-dir PATH                 Path to video or image file or folder\n                                  containing images/videos.\n  --filepaths PATH                Path to csv containing `filepath` column\n                                  with videos.\n  --save-dir PATH                 An optional directory for saving the output.\n                                  Defaults to the current working directory.\n  --config PATH                   Specify options using yaml configuration\n                                  file instead of through command line\n                                  options.\n  --fps FLOAT                     Number of frames per second to process.\n                                  Defaults to 1.0 (1 frame per second).\n                                  [default: 1.0]\n  --output-type [segmentation|chimp_anatomy]\n                                  If 'chimp_anatomy' will apply anatomy model\n                                  from densepose to the rendering and create a\n                                  CSV with the anatomy visible in each frame.\n                                  If 'segmentation', will just output the\n                                  segmented area where an animal is\n                                  identified, which works for more species\n                                  than chimpanzees.  [default: chimp_anatomy]\n  --render-output / --no-render-output\n                                  If True, generate an output image or video\n                                  with either the segmentation or anatomy\n                                  rendered depending on the `output_type` that\n                                  is chosen.  [default: no-render-output]\n  --weight-download-region [us|eu|asia]\n                                  Server region for downloading weights.\n  --cache-dir PATH                Path to directory for model weights.\n                                  Alternatively, specify with environment\n                                  variable `ZAMBA_CACHE_DIR`. If not\n                                  specified, user's cache directory is used.\n  -y, --yes                       Skip confirmation of configuration and\n                                  proceed right to prediction.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"models/depth/","title":"Depth estimation","text":""},{"location":"models/depth/#background","title":"Background","text":"<p>Our depth estimation model is useful for predicting the distance an animal is from the camera, which is an input into models used to estimate animal abundance.</p> <p>The depth model comes from one of the winners of the Deep Chimpact: Depth Estimation for Wildlife Conservation machine learning challenge hosted by DrivenData. The goal of this challenge was to use machine learning and advances in monocular (single-lens) depth estimation techniques to automatically estimate the distance between a camera trap and an animal contained in its video footage. The challenge drew on a unique labeled dataset from research teams from the Max Planck Institute for Evolutionary Anthropology (MPI-EVA) and the Wild Chimpanzee Foundation (WCF).</p> <p>The species in the training dataset included bushbucks, chimpanzees, duikers, elephants, leopards, and monkeys. Videos were from Ta\u00ef National Park in C\u00f4te d'Ivoire and Moyen-Bafing National Park in the Republic of Guinea.</p> <p>The Zamba package supports running the depth estimation model on videos. Under the hood, it does the following:</p> <ul> <li>Resamples the video to one frame per second</li> <li>Runs the MegadetectorLite model on each frame to find frames with animals in them</li> <li>Estimates depth for each detected animal in the frame</li> <li>Outputs a csv with predictions</li> </ul>"},{"location":"models/depth/#output-format","title":"Output format","text":"<p>The output of the depth estimation model is a csv with the following columns:</p> <ul> <li><code>filepath</code>: video name</li> <li><code>time</code>: seconds from the start of the video</li> <li><code>distance</code>: distance between detected animal and the camera</li> </ul> <p>There will be multiple rows per timestamp if there are multiple animals detected in the frame. Due to current limitations of the algorithm, the distance for all animals in the frame will be the same. If there is no animal in the frame, the distance will be null.</p> <p>For example, the first few rows of the <code>depth_predictions.csv</code> might look like this:</p> <pre><code>filepath,time,distance\nvideo_1.avi,0,7.4\nvideo_1.avi,0,7.4\nvideo_1.avi,1,\nvideo_1.avi,2,\nvideo_1.avi,3,\n</code></pre>"},{"location":"models/depth/#installation","title":"Installation","text":"<p>The depth estimation is included by default. If you've already installed zamba, there's nothing more you need to do.</p>"},{"location":"models/depth/#running-depth-estimation","title":"Running depth estimation","text":"<p>Here's how to run the depth estimation model.</p> CLI <pre><code># output a csv with depth predictions for each frame in the videos in PATH_TO_VIDEOS\nzamba depth --data-dir PATH_TO_VIDEOS\n</code></pre> Python <pre><code>from zamba.models.depth_estimation import DepthEstimationConfig\ndepth_conf = DepthEstimationConfig(data_dir=\"PATH_TO_VIDEOS\")\ndepth_conf.run_model()\n</code></pre>"},{"location":"models/depth/#debugging","title":"Debugging","text":"<p>Unlike in the species classification models, selected frames are stored in memory rather than cached to disk. If you run out of memory, try predicting on a smaller number of videos. If you hit a GPU memory error, try reducing the number of workers or the batch size.</p>"},{"location":"models/depth/#getting-help","title":"Getting help","text":"<p>To see all of the available options, run <code>zamba depth --help</code>.</p> <pre><code>$ zamba depth --help\n\n Usage: zamba depth [OPTIONS]\n\n Estimate animal distance at each second in the video.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --filepaths                       PATH          Path to csv containing `filepath` column  \u2502\n\u2502                                                 with videos.                              \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --data-dir                        PATH          Path to folder containing videos.         \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --save-to                         PATH          An optional directory or csv path for     \u2502\n\u2502                                                 saving the output. Defaults to            \u2502\n\u2502                                                 `depth_predictions.csv` in the working    \u2502\n\u2502                                                 directory.                                \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --overwrite               -o                    Overwrite output csv if it exists.        \u2502\n\u2502 --batch-size                      INTEGER       Batch size to use for inference.          \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --num-workers                     INTEGER       Number of subprocesses to use for data    \u2502\n\u2502                                                 loading.                                  \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --gpus                            INTEGER       Number of GPUs to use for inference. If   \u2502\n\u2502                                                 not specifiied, will use all GPUs found   \u2502\n\u2502                                                 on machine.                               \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --model-cache-dir                 PATH          Path to directory for downloading model   \u2502\n\u2502                                                 weights. Alternatively, specify with      \u2502\n\u2502                                                 environment variable `MODEL_CACHE_DIR`.   \u2502\n\u2502                                                 If not specified, user's cache directory  \u2502\n\u2502                                                 is used.                                  \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --weight-download-region          [us|eu|asia]  Server region for downloading weights.    \u2502\n\u2502                                                 [default: None]                           \u2502\n\u2502 --yes                     -y                    Skip confirmation of configuration and    \u2502\n\u2502                                                 proceed right to prediction.              \u2502\n\u2502 --help                                          Show this message and exit.               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"models/species-detection/","title":"Species detection","text":"<p>The classification algorithms in <code>zamba</code> are designed to identify species of animals that appear in camera trap images and videos. The pretrained models that ship with the <code>zamba</code> package are: <code>blank_nonblank</code>, <code>time_distributed</code>, <code>slowfast</code>, and <code>european</code>. For more details of each, read on!</p>"},{"location":"models/species-detection/#model-summary","title":"Model summary","text":""},{"location":"models/species-detection/#video-models","title":"Video models","text":"Model Geography Relative strengths Architecture Number of training videos <code>blank_nonblank</code> Central Africa, West Africa, and Western Europe Just blank detection, without species classification  Image-based <code>TimeDistributedEfficientNet</code> ~263,000 <code>time_distributed</code> Central and West Africa Recommended species classification model for jungle ecologies Image-based <code>TimeDistributedEfficientNet</code> ~250,000 <code>slowfast</code> Central and West Africa Potentially better than <code>time_distributed</code> at small species detection Video-native <code>SlowFast</code> ~15,000 <code>european</code> Western Europe Trained on non-jungle ecologies Finetuned <code>time_distributed</code>model ~13,000 <p>The models trained on the largest datasets took a couple weeks to train on a single GPU machine. Some models will be updated in the future, and you can always check the changelog to see if there have been updates.</p> <p>All models support training, fine-tuning, and inference. For fine-tuning, we recommend using the <code>time_distributed</code> model as the starting point.</p>"},{"location":"models/species-detection/#image-models","title":"Image models","text":"Model Geography Relative strengths Architecture Number of training videos <code>lila.science</code> Global based on datasets from lila.science Good base model for common global species. ConvNextV2 backbone 15 million annotations from 7 million images <p>All models support training, fine-tuning, and inference.</p>"},{"location":"models/species-detection/#species-classes","title":"Species detection","text":""},{"location":"models/species-detection/#what-species-can-zamba-detect","title":"What species can <code>zamba</code> detect?","text":"<p>The <code>blank_nonblank</code> model is trained to do blank detection, without the species classification. It only outputs the probability that the video is <code>blank</code>, meaning that it does not contain an animal.</p> <p>The <code>time_distributed</code> and <code>slowfast</code> models are both trained to identify 32 common species from Central and West Africa. The output labels in these models are:</p> <ul> <li><code>aardvark</code></li> <li><code>antelope_duiker</code></li> <li><code>badger</code></li> <li><code>bat</code></li> <li><code>bird</code></li> <li><code>blank</code></li> <li><code>cattle</code></li> <li><code>cheetah</code></li> <li><code>chimpanzee_bonobo</code></li> <li><code>civet_genet</code></li> <li><code>elephant</code></li> <li><code>equid</code></li> <li><code>forest_buffalo</code></li> <li><code>fox</code></li> <li><code>giraffe</code></li> <li><code>gorilla</code></li> <li><code>hare_rabbit</code></li> <li><code>hippopotamus</code></li> <li><code>hog</code></li> <li><code>human</code></li> <li><code>hyena</code></li> <li><code>large_flightless_bird</code></li> <li><code>leopard</code></li> <li><code>lion</code></li> <li><code>mongoose</code></li> <li><code>monkey_prosimian</code></li> <li><code>pangolin</code></li> <li><code>porcupine</code></li> <li><code>reptile</code></li> <li><code>rodent</code></li> <li><code>small_cat</code></li> <li><code>wild_dog_jackal</code></li> </ul> <p>The <code>european</code> model is trained to identify 11 common species in Western Europe. The possible class labels are:</p> <ul> <li><code>bird</code></li> <li><code>blank</code></li> <li><code>domestic_cat</code></li> <li><code>european_badger</code></li> <li><code>european_beaver</code></li> <li><code>european_hare</code></li> <li><code>european_roe_deer</code></li> <li><code>north_american_raccoon</code></li> <li><code>red_fox</code></li> <li><code>weasel</code></li> <li><code>wild_boar</code></li> </ul> <p>The <code>lila.science</code> model is trained to identify many species and groups from Lila.science's dataset, which contains over 15 million annotations from 7 million images. The 178 classes are:</p> <ul> <li><code>acinonyx_jubatus</code></li> <li><code>aepyceros_melampus</code></li> <li><code>alcelaphus_buselaphus</code></li> <li><code>alces_alces</code></li> <li><code>animalia</code></li> <li><code>anseriform_bird</code></li> <li><code>antilocapra_americana</code></li> <li><code>artamid_corvid_icterid_bird</code></li> <li><code>aves</code></li> <li><code>bos_taurus</code></li> <li><code>bucerotid_ramphastid_bird</code></li> <li><code>burhinid_otidid_bird</code></li> <li><code>callosciurine_squirrels</code></li> <li><code>camelus_dromedarius</code></li> <li><code>canidae</code></li> <li><code>canis_familiaris</code></li> <li><code>canis_latrans</code></li> <li><code>canis_lupus</code></li> <li><code>capra_goat</code></li> <li><code>capreolinae_deer</code></li> <li><code>capricornis_sumatraensis</code></li> <li><code>caprimulgiform_bird</code></li> <li><code>caracal_caracal</code></li> <li><code>catopuma_temminckii</code></li> <li><code>caviidae_dasyproctidae</code></li> <li><code>cebid_monkey</code></li> <li><code>cephalophini_neotragini_oreotragini</code></li> <li><code>cephalophus_silvicultor</code></li> <li><code>cercocebus_macaca_sp</code></li> <li><code>cercopithecine_monkey</code></li> <li><code>cerdocyon_thous</code></li> <li><code>certhiid_furnariid_picid_bird</code></li> <li><code>cervid_deer</code></li> <li><code>cervini_deer</code></li> <li><code>charadriiform_bird</code></li> <li><code>chinchillidae</code></li> <li><code>chiropteran_mammal</code></li> <li><code>cingulata</code></li> <li><code>colobine_monkey</code></li> <li><code>columbimorph_bird</code></li> <li><code>connochaetes_gnou</code></li> <li><code>connochaetes_taurinus</code></li> <li><code>coraciiform_galbuliform_trogoniform_bird</code></li> <li><code>crocuta_crocuta</code></li> <li><code>cuniculidae</code></li> <li><code>damaliscus_lunatus</code></li> <li><code>damaliscus_pygargus</code></li> <li><code>didelphimorph_marsupial</code></li> <li><code>didelphis</code></li> <li><code>eira_pekania</code></li> <li><code>elephantidae</code></li> <li><code>equus_africanus</code></li> <li><code>equus_asinus</code></li> <li><code>equus_caballus</code></li> <li><code>equus_ferus</code></li> <li><code>erethizontidae_hystricidae</code></li> <li><code>estrildid_fringillid_passerid_bird</code></li> <li><code>eulipotyphla</code></li> <li><code>euplerinae</code></li> <li><code>euungulata</code></li> <li><code>felidae</code></li> <li><code>felis</code></li> <li><code>formicariid_grallariid_pittid_bird</code></li> <li><code>galidiinae</code></li> <li><code>galliform_bird</code></li> <li><code>gazelles</code></li> <li><code>genetta</code></li> <li><code>giraffa_camelopardalis</code></li> <li><code>grallariid_pittid_bird</code></li> <li><code>gruiform_bird</code></li> <li><code>hemigaline_civet</code></li> <li><code>herpailurus_yagouaroundi</code></li> <li><code>herpestidae</code></li> <li><code>herptile</code></li> <li><code>hippopotamus_amphibius</code></li> <li><code>hippotragini</code></li> <li><code>hyaena_hyaena</code></li> <li><code>hystricomorph_rats</code></li> <li><code>ictonychinae</code></li> <li><code>invertebrate</code></li> <li><code>lagomorpha</code></li> <li><code>larid_bird</code></li> <li><code>leiotrichid_mimid_bird</code></li> <li><code>leopardus</code></li> <li><code>leptailurus_serval</code></li> <li><code>litocranius</code></li> <li><code>lupulella</code></li> <li><code>lutrinae</code></li> <li><code>lycalopex_urocyon_vulpes</code></li> <li><code>lycaon_pictus</code></li> <li><code>lynx_rufus</code></li> <li><code>macroscelididae</code></li> <li><code>madoqua</code></li> <li><code>mammalia</code></li> <li><code>manidae</code></li> <li><code>marmota</code></li> <li><code>martes</code></li> <li><code>mazama_deer</code></li> <li><code>melinae_mellivorinae_taxidiinae</code></li> <li><code>meliphagid_nectariniid_trochilid_bird</code></li> <li><code>melogale</code></li> <li><code>mephitidae</code></li> <li><code>moschiola_meminna</code></li> <li><code>motacillid_muscicapid_petroicid_prunellid_bird</code></li> <li><code>muntiacini_deer</code></li> <li><code>muroid_mammal</code></li> <li><code>mustelinae</code></li> <li><code>myrmecophaga</code></li> <li><code>nandiniidae_viverridae</code></li> <li><code>nasua</code></li> <li><code>neofelis</code></li> <li><code>nilgiritragus_hylocrius</code></li> <li><code>non_didelphis_opossum</code></li> <li><code>notamacropus</code></li> <li><code>orycteropus</code></li> <li><code>other_antilopini</code></li> <li><code>other_bovini</code></li> <li><code>other_canid</code></li> <li><code>other_haplorhini</code></li> <li><code>other_passeriform_bird</code></li> <li><code>otidimorph_bird</code></li> <li><code>otocyon_megalotis</code></li> <li><code>ovis_sheep</code></li> <li><code>paleognath_bird</code></li> <li><code>pan_troglodytes</code></li> <li><code>panthera_leo</code></li> <li><code>panthera_onca</code></li> <li><code>panthera_pardus</code></li> <li><code>panthera_tigris</code></li> <li><code>papio_sp</code></li> <li><code>paradoxurine_civet</code></li> <li><code>parahyaena_brunnea</code></li> <li><code>pardofelis_marmorata</code></li> <li><code>passerellid_emberizid_bird</code></li> <li><code>pelecanimorph_like_bird</code></li> <li><code>phacochoerus_africanus</code></li> <li><code>prionailurus_bengalensis</code></li> <li><code>prionodontidae</code></li> <li><code>procaviidae</code></li> <li><code>procellariiform_bird</code></li> <li><code>procyon</code></li> <li><code>proteles_cristatus</code></li> <li><code>psittaciform_bird</code></li> <li><code>psophiid_bird</code></li> <li><code>puma_concolor</code></li> <li><code>raptors</code></li> <li><code>ratufa_bicolor</code></li> <li><code>reduncini</code></li> <li><code>rhinocerotidae</code></li> <li><code>rhipidurid_stenostirid_bird</code></li> <li><code>rodentia</code></li> <li><code>rupicapra_rupicapra</code></li> <li><code>sciuridae</code></li> <li><code>sciurine_squirrels</code></li> <li><code>seal_or_sea_lion</code></li> <li><code>spheniscid_bird</code></li> <li><code>squamate</code></li> <li><code>strepsirrhine_primate</code></li> <li><code>strigid_tytonid_bird</code></li> <li><code>suid_pig</code></li> <li><code>tamandua</code></li> <li><code>tapiridae</code></li> <li><code>tayassuid_peccary</code></li> <li><code>tenrecid_mammal</code></li> <li><code>testudine</code></li> <li><code>tinamid_phasianid_bird</code></li> <li><code>tragelaphus</code></li> <li><code>tragelaphus_oryx</code></li> <li><code>tragulus_mouse_deer</code></li> <li><code>trichosurus</code></li> <li><code>tupaia</code></li> <li><code>turdid_bird</code></li> <li><code>ursidae</code></li> <li><code>vicugna_pacos</code></li> <li><code>viverrine_civet</code></li> <li><code>vultures</code></li> <li><code>xerine_squirrels</code></li> <li><code>zebras</code></li> </ul> <p></p>"},{"location":"models/species-detection/#blank_nonblank-model","title":"<code>blank_nonblank</code> model","text":""},{"location":"models/species-detection/#architecture","title":"Architecture","text":"<p>The <code>blank_nonblank</code> uses the same architecture as <code>time_distributed</code> model, but there is only one output class as this is a binary classification problem.</p>"},{"location":"models/species-detection/#default-configuration","title":"Default configuration","text":"<p>The full default configuration is available on Github.</p> <p>The <code>blank_nonblank</code> model uses the same default configuration as the <code>time_distributed</code> model. For the frame selection, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then the classification model is run on only the 16 frames with the highest predicted probability of detection.</p>"},{"location":"models/species-detection/#training-data","title":"Training data","text":"<p>The <code>blank_nonblank</code> model was trained on all the data used for the the <code>time_distributed</code> and <code>european</code> models.</p> <p></p>"},{"location":"models/species-detection/#time_distributed-model","title":"<code>time_distributed</code> model","text":""},{"location":"models/species-detection/#architecture_1","title":"Architecture","text":"<p>The <code>time_distributed</code> model was built by re-training a well-known image classification architecture called EfficientNetV2 (Tan, M., &amp; Le, Q., 2019) to identify the species in our camera trap videos. EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. The model is wrapped in a <code>TimeDistributed</code> layer which enables a single prediction per video.</p> <p></p>"},{"location":"models/species-detection/#training-data_1","title":"Training data","text":"<p>The <code>time_distributed</code> model was trained using data collected and annotated by trained ecologists from Cameroon, Central African Republic, Democratic Republic of the Congo, Gabon, Guinea, Liberia, Mozambique, Nigeria, Republic of the Congo, Senegal, Tanzania, and Uganda, as well as citizen scientists on the Chimp&amp;See platform.</p> <p>The data included camera trap videos from:</p> Country Location Cameroon Campo Ma'an National Park Korup National Park Central African Republic Dzanga-Sangha Protected Area C\u00f4te d'Ivoire Como\u00e9 National Park Guiroutou Ta\u00ef National Park Democratic Republic of the Congo Bili-Uele Protect Area Salonga National Park Gabon Loango National Park Lop\u00e9 National Park Guinea Bakoun Classified Forest Moyen-Bafing National Park Liberia East Nimba Nature Reserve Grebo-Krahn National Park Sapo National Park Mozambique Gorongosa National Park Nigeria Gashaka-Gumti National Park Republic of the Congo Conkouati-Douli National Park Nouabale-Ndoki National Park Senegal Kayan Tanzania Grumeti Game Reserve Ugalla River National Park Uganda Budongo Forest Reserve Bwindi Forest National Park Ngogo and Kibale National Park <p></p>"},{"location":"models/species-detection/#default-configuration_1","title":"Default configuration","text":"<p>The full default configuration is available on Github.</p> <p>By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then <code>time_distributed</code> is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels following frame selection.</p> <p>The default video loading configuration for <code>time_distributed</code> is: <pre><code>video_loader_config:\n  model_input_height: 240\n  model_input_width: 426\n  crop_bottom_pixels: 50\n  fps: 4\n  total_frames: 16\n  ensure_total_frames: true\n  megadetector_lite_config:\n    confidence: 0.25\n    fill_mode: score_sorted\n    n_frames: 16\n    frame_batch_size: 24\n    image_height: 640\n    image_width: 640\n</code></pre></p> <p>You can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file. The only requirement for the <code>time_distributed</code> model is that the video loader must return 16 frames.</p> <p></p>"},{"location":"models/species-detection/#slowfast-model","title":"<code>slowfast</code> model","text":""},{"location":"models/species-detection/#architecture_2","title":"Architecture","text":"<p>The <code>slowfast</code> model was built by re-training a video classification backbone called SlowFast (Feichtenhofer, C., Fan, H., Malik, J., &amp; He, K., 2019). SlowFast refers to the two model pathways involved: one that operates at a low frame rate to capture spatial semantics, and one that operates at a high frame rate to capture motion over time.</p> Source: Feichtenhofer, C., Fan, H., Malik, J., &amp; He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 6202-6211).  <p>Unlike <code>time_distributed</code>, <code>slowfast</code> is video native. This means it takes into account the relationship between frames in a video, rather than running independently on each frame.</p>"},{"location":"models/species-detection/#training-data_2","title":"Training data","text":"<p>The <code>slowfast</code> model was trained on a subset of the data used for the <code>time_distributed</code> model.</p>"},{"location":"models/species-detection/#default-configuration_2","title":"Default configuration","text":"<p>The full default configuration is available on Github.</p> <p>By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then <code>slowfast</code> is run on only the 32 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels.</p> <p>The full default video loading configuration is:</p> <pre><code>video_loader_config:\n  model_input_height: 240\n  model_input_width: 426\n  crop_bottom_pixels: 50\n  fps: 8\n  total_frames: 32\n  ensure_total_frames: true\n  megadetector_lite_config:\n    confidence: 0.25\n    fill_mode: score_sorted\n    n_frames: 32\n    image_height: 416\n    image_width: 416\n</code></pre> <p>You can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file. The two requirements for the <code>slowfast</code> model are that:</p> <ul> <li>the video loader must return 32 frames</li> <li>videos inputted into the model must be at least 200 x 200 pixels</li> </ul> <p></p>"},{"location":"models/species-detection/#european-model","title":"<code>european</code> model","text":""},{"location":"models/species-detection/#architecture_3","title":"Architecture","text":"<p>The <code>european</code> model starts from the a previous version of the <code>time_distributed</code> model, and then replaces and trains the final output layer to predict European species.</p> <p></p>"},{"location":"models/species-detection/#training-data_3","title":"Training data","text":"<p>The <code>european</code> model is finetuned with data collected and annotated by partners at the German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig and The Max Planck Institute for Evolutionary Anthropology. The finetuning data included camera trap videos from Hintenteiche bei Biesenbrow, Germany.</p>"},{"location":"models/species-detection/#default-configuration_3","title":"Default configuration","text":"<p>The full default configuration is available on Github.</p> <p>The <code>european</code> model uses the same default configuration as the <code>time_distributed</code> model.</p> <p>As with all models, you can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file. The only requirement for the <code>european</code> model is that the video loader must return 16 frames.</p> <p></p>"},{"location":"models/species-detection/#megadetectorlite","title":"MegadetectorLite","text":"<p>Frame selection for video models is critical as it would be infeasible to train neural networks on all the frames in a video. For all the species detection models that ship with <code>zamba</code>, the default frame selection method is an efficient object detection model called MegadetectorLite that determines the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection are passed to the model.</p> <p>MegadetectorLite combines two open-source models:</p> <ul> <li>Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos.</li> <li>YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector.</li> </ul> <p>While highly accurate, Megadetector is too computationally intensive to run on every frame. MegadetectorLite was created by training a YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training.</p> <p>MegadetectorLite can be imported into Python code and used directly since it has convenient methods for <code>detect_image</code> and <code>detect_video</code>. See the API documentation for more details.</p> <p></p>"},{"location":"models/species-detection/#lilascience-model","title":"<code>lila.science</code> model","text":""},{"location":"models/species-detection/#architecture_4","title":"Architecture","text":"<p>The <code>lila.science</code> model is a global model with a ConvNextV2 base size (87.7M parameters) backbone accepting 224x224 images as input.</p> <p></p>"},{"location":"models/species-detection/#training-data_4","title":"Training data","text":"<p>Lila.science dataset, which contains over 15 million annotations from 7 million images. The model was trained on cropped images of just the bounding box around an animal.</p> <p>Data came from the following lila.science datasets:</p> Dataset Geography Count of original images Count of cropped annotations Caltech Camera TrapsBeery et al., 2018 Southwestern United States 59,205 96,724 Channel Islands Camera TrapsThe Nature Conservancy, 2021 California, United States 125,369 239,472 Desert Lion Camera TrapsDesert Lion Conservation Project, 2024 Namibia 61,910 185,475 ENA24-detectionYousif et al., 2019 Eastern North America 8,652 11,092 Idaho Camera TrapsIdaho Department of Fish and Game, 2021 Idaho, United States 338,706 1,072,912 Island Conservation Camera TrapsIsland Conservation, 2020 7 islands around the world 44,007 79,660 Missouri Camera TrapsZhang et al., 2016 Missouri, United States 946 955 North American Camera Trap ImagesTabak et al., 2018 United States 2,705,394 7,426,839 New Zealand TrailcamsNew Zealand Trailcams, 2024 New Zealand 2,109,592 2,794,859 Orinoquia Camera TrapsV\u00e9lez et al., 2022 Colombia 80,307 103,856 Snapshot Safari 2024 ExpansionPardo et al., 2021 Africa (multiple countries) 836,522 1,949,366 Snapshot Safari CamdebooPardo et al., 2021 South Africa 15,299 26,379 Snapshot Safari EnonkishuPardo et al., 2021 Kenya 9,049 37,252 Snapshot Safari KarooPardo et al., 2021 South Africa 5,764 8,426 Snapshot Safari KgalagadiPardo et al., 2021 South Africa and Botswana 2,060 2,938 Snapshot Safari KrugerPardo et al., 2021 South Africa 3,112 6,343 Snapshot Safari Mountain ZebraPardo et al., 2021 South Africa 5,535 9,333 SWG Camera TrapsSaola Working Group, 2021 Vietnam and Laos 87,309 100,677 WCS Camera TrapsWildlife Conservation Society, 2019 12 countries 523,897 920,471 Wellington Camera TrapsAnton et al., 2018 New Zealand 203,038 269,146 <p></p>"},{"location":"models/species-detection/#default-configuration_4","title":"Default configuration","text":"<p>The full default configuration is available on Github.</p> <p>The default configuration will use megadetector to identify bounding boxes for animals and then use the <code>lila.science</code> model to identify the species in the bounding box.</p> <p>It will generate a CSV file with predicted species probabilities for each bounding box.</p>"},{"location":"models/species-detection/#user-contributed-models","title":"User contributed models","text":"<p>We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the Model Zoo Wiki for others to be able to use!</p> <p>To use one of these models, download the weights file and the configuration file from the Model Zoo Wiki. You'll need to create a configuration yaml to use that at least contains the same <code>video_loader_config</code> from the configuration yaml you downloaded. Then you can run the model with:</p> <pre><code>$ zamba predict --checkpoint downloaded_weights.ckpt --config predict_config.yaml\n</code></pre>"},{"location":"models/td-full-metrics/","title":"Time-distributed model performance","text":""},{"location":"models/td-full-metrics/#african-forest-model","title":"African forest model","text":"<p>The African species time-distributed model was trained using almost 250,000 videos from 14 countries in West, Central, and East Africa. These videos include examples of 30 animal species, plus some blank videos and some showing humans. To evaluate the performance of the model, we held out 30,324 videos from 101 randomly-chosen sites.</p>"},{"location":"models/td-full-metrics/#removing-blank-videos","title":"Removing blank videos","text":"<p>One use of this model is to identify blank videos so they can be discarded or ignored. In this dataset, 42% of the videos are blank, so removing them can save substantial amounts of viewing time and storage space.</p> <p>The model assigns a probability that each video is blank, so one strategy is to remove videos  if their probability exceeds a given threshold. Of course, the model is not perfect, so there is a chance we will wrongly remove a video that actually contains an animal.</p> <p>To assess this tradeoff, we can use the holdout set to simulate this strategy with a range of thresholds. For each threshold, we compute the fraction of blank videos correctly discarded and the fraction of non-blank videos incorrectly discarded. The following figure shows the results.</p> <p></p> <p>The markers indicate three levels of tolerance for losing non-blank videos. For example, if it's acceptable to lose 5% of non-blank videos, we can choose a threshold that removes 63% of the blank videos. If we can tolerate a loss of 10%, we can remove 80% of the blanks. And if we can tolerate a loss of 15%, we can remove 90% of the blanks. Above that, the percentage of lost videos increases steeply.</p>"},{"location":"models/td-full-metrics/#accuracy","title":"Accuracy","text":"<p>In addition to identifying blank videos, the model also computes a probability that each of 30 animal species appears in each video (plus human and blank). We can use these probabilities to quantify the accuracy of the model for species classification. Specifically, we computed:</p> <ul> <li> <p>Top-1 accuracy, which is the fraction of videos where the species with the highest predicted probability is, in fact, present.</p> </li> <li> <p>Top-3 accuracy, which is the fraction of videos where one of the three species the model considered most likely is present.</p> </li> </ul> <p>Over all videos in the holdout set, the top-1 accuracy is 82%; the top-3 accuracy is 94%. As an example, if you choose a video at random and the species with the highest predicted probability is elephant, the probability is 82% that the video contains an elephant, according to the human-generated labels. If the three most likely species were elephant, hippopotamus, and cattle, the probability is 94% that the video contains at least one of those species.</p> <p>These results depend in part on the species represented in a particular dataset. For example, in the small number of videos from Equatorial Guinea, only three species appear. For these videos, the top-1 accuracy is 97%, much higher than the overall accuracy. In the videos from Ivory Coast, 21 species are represented, so the problem is harder. For these videos, top-1 accuracy is 80%, a little lower than the overall accuracy.</p>"},{"location":"models/td-full-metrics/#recall-and-precision-by-species","title":"Recall and precision by species","text":"<p>One of the goals of classification is to help with retrieval, that is, efficiently finding videos containing a particular species. To evaluate the performance of the model for retrieval, we can use</p> <ul> <li> <p>Recall, which is the fraction of videos containing a particular species that are correctly classified, and</p> </li> <li> <p>Precision, which is the fraction of videos the model labels with a particular species that actually contain that species.</p> </li> </ul> <p>The following figure shows recall and precision for the species in the holdout set,  excluding 11 species where there are too few examples in the holdout set to compute meaningful estimates of these metrics.</p> <p></p> <p>It's clear that we are able to retrieve some species more efficiently than others. For example, elephants are relatively easy to find. Of the videos that contain elephants, 84% are correctly classified; and of the videos that the model labels \"elephant\", 94% contain elephants. So a researcher using the model to find elephant videos could find a large majority of them while viewing only a small number of non-elephant videos.</p> <p>Not surprisingly, smaller animals are harder to find. For example, the recall for rodent videos is only 22%. However, it is still possible to search for rodents by selecting videos that assign a relatively high probability to \"rodent\", even if it assigns a higher probability to another species or \"blank\".</p>"},{"location":"models/td-full-metrics/#description-of-the-holdout-set","title":"Description of the holdout set","text":"<p>The videos in the holdout set are a random sample from the complete set of labeled videos, but they are selected on a transect-by-transect basis; that is, videos from each transect are assigned entirely to the training set or entirely to the holdout set. So the performance of the model on the holdout set should reflect its performance on videos from a transect the model has never seen.</p> <p>All 14 countries are represented in the holdout set; the following table shows the number of videos from each country. These proportions are roughly consistent with the proportions in the complete set.</p> Country Number of videos Ivory Coast 10,987 Guinea 4,300 DR Congo 2,750 Uganda 2,497 Tanzania 1,794 Mozambique 1,168 Senegal 1,131 Gabon 1,116 Cameroon 1,114 Liberia 1,065 Central African Republic 997 Nigeria 889 Congo Republic 678 Equatorial Guinea 38 <p>The following figure shows the number of videos containing each of 30 animal species, plus some videos showing humans and a substantial number of blank videos.</p> <p></p> <p>One of the challenges of this kind of classification is that some species are  much more common than others. For species that appear in a small number of videos, we expect the model to be less accurate because it has fewer examples to learn from. Also, for these species it is hard to assess performance precisely because there are few examples in the holdout set. If you would like to add more examples of the species you work with, see how to build on the model.</p>"}]}