
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Zamba is a command-line tool built in Python to automatically identify the species seen in camera trap videos from sites in central Africa.">
      
      
      
      
        <link rel="canonical" href="https://zamba.drivendata.org/docs/~latest/api-reference/models-efficientnet_models/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.6">
    
    
      
        <title>zamba.models.efficientnet_models - Zamba</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_mkdocstrings.css">
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#zambamodelsefficientnet_models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Zamba" class="md-header__button md-logo" aria-label="Zamba" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Zamba
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              zamba.models.efficientnet_models
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/drivendataorg/zamba/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Zamba" class="md-nav__button md-logo" aria-label="Zamba" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Zamba
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/drivendataorg/zamba/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2">
          Getting started
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Getting started" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Getting started
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../install/" class="md-nav__link">
        Installing zamba
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../quickstart/" class="md-nav__link">
        Quickstart
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3">
          User Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="User Tutorials" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          User Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../predict-tutorial/" class="md-nav__link">
        Classifying unlabeled videos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../train-tutorial/" class="md-nav__link">
        Training a model on labeled videos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../debugging/" class="md-nav__link">
        Debugging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../extra-options/" class="md-nav__link">
        Guide to common optional parameters
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4">
          Available Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Available Models" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Available Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../models/species-detection/" class="md-nav__link">
        Species detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../models/densepose/" class="md-nav__link">
        DensePose
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          Advanced Options
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advanced Options" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Advanced Options
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../configurations/" class="md-nav__link">
        All configuration options
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../yaml-config/" class="md-nav__link">
        Using YAML configuration files
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
        
          
            
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../contribute/">Contribute to zamba</a>
          
        </div>
      
      <nav class="md-nav" aria-label="Contribute to zamba" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Contribute to zamba
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7">
          Changelog
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Changelog" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Changelog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../changelog/" class="md-nav__link">
        `zamba` changelog
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          API Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1" type="checkbox" id="__nav_8_1" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_1">
          zamba.data
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="zamba.data" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_1">
          <span class="md-nav__icon md-icon"></span>
          zamba.data
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data-metadata/" class="md-nav__link">
        zamba.data.metadata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data-video/" class="md-nav__link">
        zamba.data.video
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_2" type="checkbox" id="__nav_8_2" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_2">
          zamba.models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="zamba.models" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          zamba.models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models-config/" class="md-nav__link">
        zamba.models.config
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_2_2" type="checkbox" id="__nav_8_2_2" >
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_2_2">
          zamba.models.densepose
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="zamba.models.densepose" data-md-level="3">
        <label class="md-nav__title" for="__nav_8_2_2">
          <span class="md-nav__icon md-icon"></span>
          zamba.models.densepose
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../densepose_config/" class="md-nav__link">
        zamba.models.densepose.config
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../densepose_manager/" class="md-nav__link">
        zamba.models.densepose.densepose_manager
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          zamba.models.efficientnet_models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        zamba.models.efficientnet_models
      </a>
      
        


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#zamba.models.efficientnet_models-classes" class="md-nav__link">
    Classes
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models-megadetector_lite_yolox/" class="md-nav__link">
        zamba.models.megadetector_lite_yolox
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models-model_manager/" class="md-nav__link">
        zamba.models.model_manager
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models-slowfast_models/" class="md-nav__link">
        zamba.models.slowfast_models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models-utils/" class="md-nav__link">
        zamba.models.utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models-yolox_models/" class="md-nav__link">
        zamba.models.yolox_models
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_3" type="checkbox" id="__nav_8_3" >
      
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_3">
          zamba.pytorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="zamba.pytorch" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_3">
          <span class="md-nav__icon md-icon"></span>
          zamba.pytorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-dataloaders/" class="md-nav__link">
        zamba.pytorch.dataloaders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-finetuning/" class="md-nav__link">
        zamba.pytorch.finetuning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-layers/" class="md-nav__link">
        zamba.pytorch.layers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-transforms/" class="md-nav__link">
        zamba.pytorch.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-utils/" class="md-nav__link">
        zamba.pytorch.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_4" type="checkbox" id="__nav_8_4" >
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8_4">
          zamba.pytorch_lightning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="zamba.pytorch_lightning" data-md-level="2">
        <label class="md-nav__title" for="__nav_8_4">
          <span class="md-nav__icon md-icon"></span>
          zamba.pytorch_lightning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_lightning-utils/" class="md-nav__link">
        zamba.pytorch_lightning.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../exceptions/" class="md-nav__link">
        zamba.exceptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../metrics/" class="md-nav__link">
        zamba.metrics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../settings/" class="md-nav__link">
        zamba.settings
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#zamba.models.efficientnet_models-classes" class="md-nav__link">
    Classes
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/drivendataorg/zamba/edit/master/docs/api-reference/models-efficientnet_models.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="zambamodelsefficientnet_models">zamba.models.efficientnet_models<a class="headerlink" href="#zambamodelsefficientnet_models" title="Permanent link">&para;</a></h1>


  <div class="doc doc-object doc-module">


    <div class="doc doc-contents first">




  <div class="doc doc-children">





<h2 id="zamba.models.efficientnet_models-classes">Classes<a href="#zamba.models.efficientnet_models-classes" class="headerlink" title="Permanent link">&para;</a></h2>

  <div class="doc doc-object doc-class">



<h3 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet" class="doc doc-heading">
        <code>
TimeDistributedEfficientNet            (<a title="zamba.pytorch_lightning.utils.ZambaVideoClassificationLightningModule" href="../pytorch_lightning-utils/#zamba.pytorch_lightning.utils.ZambaVideoClassificationLightningModule">ZambaVideoClassificationLightningModule</a>)
        </code>



<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet" class="headerlink" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">





  <div class="doc doc-children">




<h4 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet-attributes">Attributes<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet-attributes" class="headerlink" title="Permanent link">&para;</a></h4>

  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.CHECKPOINT_HYPER_PARAMS_KEY" class="doc doc-heading">
<code class="highlight language-python"><span class="n">CHECKPOINT_HYPER_PARAMS_KEY</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.CHECKPOINT_HYPER_PARAMS_KEY" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.CHECKPOINT_HYPER_PARAMS_NAME" class="doc doc-heading">
<code class="highlight language-python"><span class="n">CHECKPOINT_HYPER_PARAMS_NAME</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.CHECKPOINT_HYPER_PARAMS_NAME" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.CHECKPOINT_HYPER_PARAMS_TYPE" class="doc doc-heading">
<code class="highlight language-python"><span class="n">CHECKPOINT_HYPER_PARAMS_TYPE</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.CHECKPOINT_HYPER_PARAMS_TYPE" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.T_destination" class="doc doc-heading">
<code class="highlight language-python"><span class="n">T_destination</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.T_destination" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.automatic_optimization" class="doc doc-heading">
<code class="highlight language-python"><span class="n">automatic_optimization</span><span class="p">:</span> <span class="nb">bool</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.automatic_optimization" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.current_epoch" class="doc doc-heading">
<code class="highlight language-python"><span class="n">current_epoch</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.current_epoch" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>The current epoch in the Trainer. If no Trainer is attached, this propery is 0.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.datamodule" class="doc doc-heading">
<code class="highlight language-python"><span class="n">datamodule</span><span class="p">:</span> <span class="n">Any</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.datamodule" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.device" class="doc doc-heading">
<code class="highlight language-python"><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.device" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.dtype" class="doc doc-heading">
<code class="highlight language-python"><span class="n">dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.dtype" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.dump_patches" class="doc doc-heading">
<code class="highlight language-python"><span class="n">dump_patches</span><span class="p">:</span> <span class="nb">bool</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.dump_patches" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>This allows better BC support for :meth:<code>load_state_dict</code>. In
:meth:<code>state_dict</code>, the version number will be saved as in the attribute
<code>_metadata</code> of the returned state dict, and thus pickled. <code>_metadata</code> is a
dictionary with keys that follow the naming convention of state dict. See
<code>_load_from_state_dict</code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module's <code>_load_from_state_dict</code> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.example_input_array" class="doc doc-heading">
<code class="highlight language-python"><span class="n">example_input_array</span><span class="p">:</span> <span class="n">Any</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.example_input_array" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>The example input array is a specification of what the module can consume in the :meth:<code>forward</code> method.
The return type is interpreted as follows:</p>
<ul>
<li>Single tensor: It is assumed the model takes a single argument, i.e.,
    <code>model.forward(model.example_input_array)</code></li>
<li>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
    <code>model.forward(*model.example_input_array)</code></li>
<li>Dict: The input array represents named keyword arguments, i.e.,
    <code>model.forward(**model.example_input_array)</code></li>
</ul>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.global_rank" class="doc doc-heading">
<code class="highlight language-python"><span class="n">global_rank</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.global_rank" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>The index of the current process across all nodes and devices.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.global_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">global_step</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.global_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Total training batches seen across all epochs. If no Trainer is attached, this propery is 0.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.hparams" class="doc doc-heading">
<code class="highlight language-python"><span class="n">hparams</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">parsing</span><span class="o">.</span><span class="n">AttributeDict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.hparams" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.hparams_initial" class="doc doc-heading">
<code class="highlight language-python"><span class="n">hparams_initial</span><span class="p">:</span> <span class="n">AttributeDict</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.hparams_initial" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.loaded_optimizer_states_dict" class="doc doc-heading">
<code class="highlight language-python"><span class="n">loaded_optimizer_states_dict</span><span class="p">:</span> <span class="nb">dict</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.loaded_optimizer_states_dict" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.local_rank" class="doc doc-heading">
<code class="highlight language-python"><span class="n">local_rank</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.local_rank" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>The index of the current process within a single node.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.logger" class="doc doc-heading">
<code class="highlight language-python"><span class="n">logger</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.logger" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Reference to the logger object in the Trainer.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.model_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">model_size</span><span class="p">:</span> <span class="nb">float</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.model_size" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>The model's size in megabytes. The computation includes everything in the
:meth:<code>~torch.nn.Module.state_dict</code>, i.e., by default the parameteters and buffers.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_gpu" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_gpu</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_gpu" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns <code>True</code> if this model is currently located on a GPU.
Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.truncated_bptt_steps" class="doc doc-heading">
<code class="highlight language-python"><span class="n">truncated_bptt_steps</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-writable"><code>writable</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.truncated_bptt_steps" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Enables <code>Truncated Backpropagation Through Time</code> in the Trainer when set to a positive integer. It represents
the number of times :meth:<code>training_step</code> gets called before backpropagation. If this is &gt; 0, the
:meth:<code>training_step</code> receives an additional argument <code>hiddens</code> and is expected to return a hidden state.</p>
    </div>

  </div>




<h4 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet-methods">Methods<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet-methods" class="headerlink" title="Permanent link">&para;</a></h4>

  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_frames</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">finetune_from</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.__init__" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">num_frames</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">finetune_from</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">finetune_from</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">efficientnet</span> <span class="o">=</span> <span class="n">timm</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s2">&quot;efficientnetv2_rw_m&quot;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">efficientnet</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">efficientnet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">finetune_from</span><span class="p">)</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">module</span>

    <span class="c1"># freeze base layers</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">efficientnet</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">num_backbone_final_features</span> <span class="o">=</span> <span class="n">efficientnet</span><span class="o">.</span><span class="n">num_features</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">efficientnet</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="s2">&quot;blocks.5&quot;</span><span class="p">),</span>
            <span class="n">efficientnet</span><span class="o">.</span><span class="n">conv_head</span><span class="p">,</span>
            <span class="n">efficientnet</span><span class="o">.</span><span class="n">bn2</span><span class="p">,</span>
            <span class="n">efficientnet</span><span class="o">.</span><span class="n">act2</span><span class="p">,</span>
            <span class="n">efficientnet</span><span class="o">.</span><span class="n">global_pool</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">efficientnet</span><span class="p">,</span> <span class="n">tdim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_backbone_final_features</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="n">num_frames</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s2">&quot;num_frames&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.add_module" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Module</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.add_module" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>string</code></td>
        <td><p>name of the child module. The child module can be
accessed from this module using the given name</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>module</code></td>
        <td><code>Module</code></td>
        <td><p>child module to be added to the module.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>

<span class="sd">    The module can be accessed as an attribute using the given name.</span>

<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.add_to_queue" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_to_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">bound</span> <span class="n">method</span> <span class="n">BaseContext</span><span class="o">.</span><span class="n">SimpleQueue</span> <span class="n">of</span> <span class="o">&lt;</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">DefaultContext</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7fa5ae66f4f0</span><span class="o">&gt;&gt;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.add_to_queue" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Appends the :attr:<code>trainer.callback_metrics</code> dictionary to the given queue.
To avoid issues with memory sharing, we cast the data to numpy.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>queue</code></td>
        <td><code>&lt;bound method BaseContext.SimpleQueue of &lt;multiprocessing.context.DefaultContext object at 0x7fa5ae66f4f0&gt;&gt;</code></td>
        <td><p>the instance of the queue to append the data.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">add_to_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">SimpleQueue</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Appends the :attr:`trainer.callback_metrics` dictionary to the given queue.</span>
<span class="sd">    To avoid issues with memory sharing, we cast the data to numpy.</span>

<span class="sd">    Args:</span>
<span class="sd">        queue: the instance of the queue to append the data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">callback_metrics</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">apply_to_collection</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="p">)</span>  <span class="c1"># send as numpy to avoid issues with memory sharing</span>
    <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">callback_metrics</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.aggregate_step_outputs" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_step_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.aggregate_step_outputs" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">aggregate_step_outputs</span><span class="p">(</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;y_true&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;y_pred&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>
    <span class="n">y_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;y_proba&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_proba</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.all_gather" class="doc doc-heading">
<code class="highlight language-python"><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sync_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.all_gather" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Allows users to call <code>self.all_gather()</code> from the LightningModule, thus making the <code>all_gather</code> operation
accelerator agnostic. <code>all_gather</code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Union[torch.Tensor, Dict, List, Tuple]</code></td>
        <td><p>int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>group</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>the process group to gather results from. Defaults to all processes (world)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sync_grads</code></td>
        <td><code>bool</code></td>
        <td><p>flag that allows users to synchronize gradients for the all_gather operation</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>A tensor of shape (world_size, batch, ...), or if the input was a collection
the output will also be a collection with tensors of this shape.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sync_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Allows users to call ``self.all_gather()`` from the LightningModule, thus making the ``all_gather`` operation</span>
<span class="sd">    accelerator agnostic. ``all_gather`` is a function provided by accelerators to gather a tensor from several</span>
<span class="sd">    distributed processes.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.</span>
<span class="sd">        group: the process group to gather results from. Defaults to all processes (world)</span>
<span class="sd">        sync_grads: flag that allows users to synchronize gradients for the all_gather operation</span>

<span class="sd">    Return:</span>
<span class="sd">        A tensor of shape (world_size, batch, ...), or if the input was a collection</span>
<span class="sd">        the output will also be a collection with tensors of this shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="n">all_gather</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">all_gather</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">all_gather</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">sync_grads</span><span class="o">=</span><span class="n">sync_grads</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.apply" class="doc doc-heading">
<code class="highlight language-python"><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.apply" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>fn</code></td>
        <td></td>
        <td><p>class:<code>Module</code> -&gt; None): function to be applied to each submodule</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.backward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.backward" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called to perform backward on the loss returned in :meth:<code>training_step</code>.
Override this hook with your own implementation if you need to.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>loss</code></td>
        <td><code>Tensor</code></td>
        <td><p>The loss tensor returned by :meth:<code>training_step</code>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Optional[torch.optim.optimizer.Optimizer]</code></td>
        <td><p>Current optimizer being used. <code>None</code> if using manual optimization.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Index of the current optimizer being used. <code>None</code> if using manual optimization.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def backward(self, loss, optimizer, optimizer_idx):
    loss.backward()
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called to perform backward on the loss returned in :meth:`training_step`.</span>
<span class="sd">    Override this hook with your own implementation if you need to.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here</span>
<span class="sd">            holds the normalized value (scaled by 1 / accumulation steps).</span>
<span class="sd">        optimizer: Current optimizer being used. ``None`` if using manual optimization.</span>
<span class="sd">        optimizer_idx: Index of the current optimizer being used. ``None`` if using manual optimization.</span>

<span class="sd">    Example::</span>

<span class="sd">        def backward(self, loss, optimizer, optimizer_idx):</span>
<span class="sd">            loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.bfloat16" class="doc doc-heading">
<code class="highlight language-python"><span class="n">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.bfloat16" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.buffers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.buffers" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over module buffers.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>recurse</code></td>
        <td><code>bool</code></td>
        <td><p>if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>!!! yields
    torch.Tensor: module buffer</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class &#39;torch.Tensor&#39;&gt; (20L,)
&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>

<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>

<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.children" class="doc doc-heading">
<code class="highlight language-python"><span class="n">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Module</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.children" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over immediate children modules.</p>
<p>!!! yields
    Module: a child module</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>

<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.compute_and_log_metrics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute_and_log_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">:</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">subset</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.compute_and_log_metrics" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_and_log_metrics</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">subset</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_macro_f1&quot;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="c1"># if only two classes, skip top_k accuracy since not enough classes</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">DEFAULT_TOP_K</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_top_</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">_accuracy&quot;</span><span class="p">,</span>
                    <span class="n">top_k_accuracy_score</span><span class="p">(</span>
                        <span class="n">y_true</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
                        <span class="p">),</span>  <span class="c1"># top k accuracy only supports single label case</span>
                        <span class="n">y_proba</span><span class="p">,</span>
                        <span class="n">labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_proba</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                        <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_accuracy&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">compute_species_specific_metrics</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">species</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;species/</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.configure_callbacks" class="doc doc-heading">
<code class="highlight language-python"><span class="n">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.configure_callbacks" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Configure model-specific callbacks.
When the model gets attached, e.g., when <code>.fit()</code> or <code>.test()</code> gets called,
the list returned here will be merged with the list of callbacks passed to the Trainer's <code>callbacks</code> argument.
If a callback returned here has the same type as one or several callbacks already present in
the Trainer's callbacks list, it will take priority and replace them.
In addition, Lightning will make sure :class:<code>~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint</code>
callbacks run last.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def configure_callbacks(self):
    early_stop = EarlyStopping(monitor&quot;val_acc&quot;, mode=&quot;max&quot;)
    checkpoint = ModelCheckpoint(monitor=&quot;val_loss&quot;)
    return [early_stop, checkpoint]
</code></pre></div>
<p>!!! note
    Certain callback methods like :meth:<code>~pytorch_lightning.callbacks.base.Callback.on_init_start</code>
    will never be invoked on the new callbacks returned here.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configure model-specific callbacks.</span>
<span class="sd">    When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets called,</span>
<span class="sd">    the list returned here will be merged with the list of callbacks passed to the Trainer&#39;s ``callbacks`` argument.</span>
<span class="sd">    If a callback returned here has the same type as one or several callbacks already present in</span>
<span class="sd">    the Trainer&#39;s callbacks list, it will take priority and replace them.</span>
<span class="sd">    In addition, Lightning will make sure :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`</span>
<span class="sd">    callbacks run last.</span>

<span class="sd">    Return:</span>
<span class="sd">        A list of callbacks which will extend the list of callbacks in the Trainer.</span>

<span class="sd">    Example::</span>

<span class="sd">        def configure_callbacks(self):</span>
<span class="sd">            early_stop = EarlyStopping(monitor&quot;val_acc&quot;, mode=&quot;max&quot;)</span>
<span class="sd">            checkpoint = ModelCheckpoint(monitor=&quot;val_loss&quot;)</span>
<span class="sd">            return [early_stop, checkpoint]</span>

<span class="sd">    Note:</span>
<span class="sd">        Certain callback methods like :meth:`~pytorch_lightning.callbacks.base.Callback.on_init_start`</span>
<span class="sd">        will never be invoked on the new callbacks returned here.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.configure_optimizers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.configure_optimizers" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is
usually useful for training video models.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is</span>
<span class="sd">    usually useful for training video models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_optimizer</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">optim</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optim</span><span class="p">,</span>
            <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">(</span>
                <span class="n">optim</span><span class="p">,</span> <span class="o">**</span><span class="p">({}</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_params</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_params</span><span class="p">)</span>
            <span class="p">),</span>
        <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.configure_sharded_model" class="doc doc-heading">
<code class="highlight language-python"><span class="n">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.configure_sharded_model" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we'd like to shard the model instantly, which is useful for extremely large models
which can save memory and initialization time.</p>
<p>The accelerator manages whether to call this hook at every given stage.
For sharded plugins where model parallelism is required, the hook is usually on called once
to initialize the sharded parameters, and not called again in the same process.</p>
<p>By default for accelerators/plugins that do not use model sharding techniques,
this hook is called during each fit/val/test/predict stages.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,</span>
<span class="sd">    where we&#39;d like to shard the model instantly, which is useful for extremely large models</span>
<span class="sd">    which can save memory and initialization time.</span>

<span class="sd">    The accelerator manages whether to call this hook at every given stage.</span>
<span class="sd">    For sharded plugins where model parallelism is required, the hook is usually on called once</span>
<span class="sd">    to initialize the sharded parameters, and not called again in the same process.</span>

<span class="sd">    By default for accelerators/plugins that do not use model sharding techniques,</span>
<span class="sd">    this hook is called during each fit/val/test/predict stages.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.cpu" class="doc doc-heading">
<code class="highlight language-python"><span class="n">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.cpu" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Moves all model parameters and buffers to the CPU.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.cuda" class="doc doc-heading">
<code class="highlight language-python"><span class="n">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.cuda" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Moves all model parameters and buffers to the GPU.
This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>device</code></td>
        <td><code>Union[torch.device, int]</code></td>
        <td><p>if specified, all parameters will be
copied to that device</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device: if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.double" class="doc doc-heading">
<code class="highlight language-python"><span class="n">double</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.double" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.eval" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.eval" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>

<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>

<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>

<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.extra_repr" class="doc doc-heading">
<code class="highlight language-python"><span class="n">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.extra_repr" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>

<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.float" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.float" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.forward" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Same as :meth:<code>torch.nn.Module.forward()</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*args</code></td>
        <td></td>
        <td><p>Whatever you decide to pass into the forward method.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Keyword arguments are also possible.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>Your model's output</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.freeze" class="doc doc-heading">
<code class="highlight language-python"><span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.freeze" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Freeze all params for inference.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>model = MyLightningModule(...)
model.freeze()
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Freeze all params for inference.</span>

<span class="sd">    Example::</span>

<span class="sd">        model = MyLightningModule(...)</span>
<span class="sd">        model.freeze()</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_buffer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_buffer" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>target</code></td>
        <td><code>str</code></td>
        <td><p>The fully-qualified string name of the buffer
to look for. (See <code>get_submodule</code> for how to specify a
fully-qualified string.)</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>The buffer referenced by <code>target</code></p></td>
    </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>AttributeError</code></td>
        <td><p>If the target string references an invalid
path or resolves to something that is not a
buffer</p></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>

<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>

<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>

    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>

    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">buffer</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_extra_state" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_extra_state" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>object</code></td>
      <td><p>Any extra state to store in the module's state_dict</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>

<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>

<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_from_queue" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_from_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">bound</span> <span class="n">method</span> <span class="n">BaseContext</span><span class="o">.</span><span class="n">SimpleQueue</span> <span class="n">of</span> <span class="o">&lt;</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">DefaultContext</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7fa5ae66f4f0</span><span class="o">&gt;&gt;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_from_queue" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Retrieve the :attr:<code>trainer.callback_metrics</code> dictionary from the given queue.
To preserve consistency, we cast back the data to <code>torch.Tensor</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>queue</code></td>
        <td><code>&lt;bound method BaseContext.SimpleQueue of &lt;multiprocessing.context.DefaultContext object at 0x7fa5ae66f4f0&gt;&gt;</code></td>
        <td><p>the instance of the queue from where to get the data.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_from_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">SimpleQueue</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue.</span>
<span class="sd">    To preserve consistency, we cast back the data to ``torch.Tensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        queue: the instance of the queue from where to get the data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># NOTE: `add_to_queue` needs to be called before</span>
    <span class="n">callback_metrics</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">callback_metrics</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_parameter" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Parameter</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_parameter" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>target</code></td>
        <td><code>str</code></td>
        <td><p>The fully-qualified string name of the Parameter
to look for. (See <code>get_submodule</code> for how to specify a
fully-qualified string.)</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.nn.Parameter</code></td>
      <td><p>The Parameter referenced by <code>target</code></p></td>
    </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>AttributeError</code></td>
        <td><p>If the target string references an invalid
path or resolves to something that is not an
<code>nn.Parameter</code></p></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>

<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>

<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>

    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>

    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_progress_bar_dict" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_progress_bar_dict" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<p>.. code-block::</p>
<div class="highlight"><pre><span></span><code>Epoch 1:   4%|         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</code></pre></div>
<p>Here is an example how to override the defaults:</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>def get_progress_bar_dict(self):
    # don&#39;t show the version number
    items = super().get_progress_bar_dict()
    items.pop(&quot;v_num&quot;, None)
    return items
</code></pre></div>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Union[int, str]]</code></td>
      <td><p>Dictionary with the items to be displayed in the progress bar.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement this to override the default items displayed in the progress bar.</span>
<span class="sd">    By default it includes the average loss value, split index of BPTT (if used)</span>
<span class="sd">    and the version of the experiment when using a logger.</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        Epoch 1:   4%|         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]</span>

<span class="sd">    Here is an example how to override the defaults:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def get_progress_bar_dict(self):</span>
<span class="sd">            # don&#39;t show the version number</span>
<span class="sd">            items = super().get_progress_bar_dict()</span>
<span class="sd">            items.pop(&quot;v_num&quot;, None)</span>
<span class="sd">            return items</span>

<span class="sd">    Return:</span>
<span class="sd">        Dictionary with the items to be displayed in the progress bar.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># call .item() only once but store elements without graphs</span>
    <span class="n">running_train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">fit_loop</span><span class="o">.</span><span class="n">running_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">avg_training_loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">running_train_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">avg_training_loss</span> <span class="o">=</span> <span class="n">running_train_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span><span class="p">:</span>
        <span class="n">avg_training_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;NaN&quot;</span><span class="p">)</span>

    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">avg_training_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tqdm_dict</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">avg_training_loss</span><span class="si">:</span><span class="s2">.3g</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">module_tbptt_enabled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_bptt_steps</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">trainer_tbptt_enabled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">truncated_bptt_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">truncated_bptt_steps</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">module_tbptt_enabled</span> <span class="ow">or</span> <span class="n">trainer_tbptt_enabled</span><span class="p">:</span>
        <span class="n">tqdm_dict</span><span class="p">[</span><span class="s2">&quot;split_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">fit_loop</span><span class="o">.</span><span class="n">split_idx</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">version</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">version</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">version</span>
        <span class="c1"># show last 4 places of long version strings</span>
        <span class="n">version</span> <span class="o">=</span> <span class="n">version</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">version</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">version</span>
        <span class="n">tqdm_dict</span><span class="p">[</span><span class="s2">&quot;v_num&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">version</span>

    <span class="k">return</span> <span class="n">tqdm_dict</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_submodule" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.get_submodule" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<div class="highlight"><pre><span></span><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre></div>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>target</code></td>
        <td><code>str</code></td>
        <td><p>The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.nn.Module</code></td>
      <td><p>The submodule referenced by <code>target</code></p></td>
    </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>AttributeError</code></td>
        <td><p>If the target string references an invalid
path or resolves to something that is not an
<code>nn.Module</code></p></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>

<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>

<span class="sd">    .. code-block::text</span>

<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>

<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>

<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>

<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>

<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>

    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>

        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mod</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.grad_norm" class="doc doc-heading">
<code class="highlight language-python"><span class="n">grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.grad_norm" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Compute each parameter's gradient's norm and their overall norm.</p>
<p>.. deprecated:: v1.3
    Will be removed in v1.5.0. Use :func:<code>pytorch_lightning.utilities.grads.grad_norm</code> instead.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Compute each parameter&#39;s gradient&#39;s norm and their overall norm.</span>

<span class="sd">    .. deprecated:: v1.3</span>
<span class="sd">        Will be removed in v1.5.0. Use :func:`pytorch_lightning.utilities.grads.grad_norm` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rank_zero_deprecation</span><span class="p">(</span>
        <span class="s2">&quot;LightningModule.grad_norm is deprecated in v1.3 and will be removed in v1.5.&quot;</span>
        <span class="s2">&quot; Use grad_norm from pytorch_lightning.utilities.grads instead.&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.half" class="doc doc-heading">
<code class="highlight language-python"><span class="n">half</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.half" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.load_state_dict" class="doc doc-heading">
<code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.load_state_dict" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>state_dict</code></td>
        <td><code>dict</code></td>
        <td><p>a dict containing parameters and
persistent buffers.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>strict</code></td>
        <td><code>bool</code></td>
        <td><p>whether to strictly enforce that the keys
in :attr:<code>state_dict</code> match the keys returned by this module's
:meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields</code></td>
      <td><ul>
<li><strong>missing_keys</strong> is a list of str containing the missing keys<ul>
<li><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</li>
</ul>
</li>
</ul></td>
    </tr>
  </tbody>
</table>      <p>!!! note
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>

<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>

    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>

    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.log" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torchmetrics</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">Metric</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torchmetrics</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">Metric</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">]]],</span> <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;default&#39;</span><span class="p">,</span> <span class="n">tbptt_reduce_fx</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tbptt_pad_token</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">sync_dist_op</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">metric_attribute</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rank_zero_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.log" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Log a key, value pair.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>self.log(&#39;train_loss&#39;, loss)
</code></pre></div>
<p>The default behavior per hook is as follows:</p>
<p>.. csv-table:: <code>*</code> also applies to the test loop
   :header: "LightningModule Hook", "on_step", "on_epoch", "prog_bar", "logger"
   :widths: 20, 10, 10, 10, 10</p>
<p>"training_step", "T", "F", "F", "T"
   "training_step_end", "T", "F", "F", "T"
   "training_epoch_end", "F", "T", "F", "T"
   "validation_step<em>", "F", "T", "F", "T"
   "validation_step_end</em>", "F", "T", "F", "T"
   "validation_epoch_end*", "F", "T", "F", "T"</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>str</code></td>
        <td><p>key to log</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>value</code></td>
        <td><code>Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]]</code></td>
        <td><p>value to log. Can be a <code>float</code>, <code>Tensor</code>, <code>Metric</code>, or a dictionary of the former.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prog_bar</code></td>
        <td><code>bool</code></td>
        <td><p>if True logs to the progress bar</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>logger</code></td>
        <td><code>bool</code></td>
        <td><p>if True logs to the logger</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>on_step</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>on_epoch</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>reduce_fx</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>reduction function over step values for end of epoch. :meth:<code>torch.mean</code> by default.</p></td>
        <td><code>&#39;default&#39;</code></td>
      </tr>
      <tr>
        <td><code>enable_graph</code></td>
        <td><code>bool</code></td>
        <td><p>if True, will not auto detach the graph</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sync_dist</code></td>
        <td><code>bool</code></td>
        <td><p>if True, reduces the metric across GPUs/TPUs</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sync_dist_group</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>the ddp group to sync across</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>add_dataloader_idx</code></td>
        <td><code>bool</code></td>
        <td><p>if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>batch_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>metric_attribute</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>To restore the metric state, Lightning requires the reference of the
:class:<code>torchmetrics.Metric</code> in your model. This is found automatically if it is a model attribute.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>rank_zero_only</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">_METRIC_COLLECTION</span><span class="p">,</span>
    <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>  <span class="c1"># TODO: change to &#39;mean&#39; when `sync_dist_op` is removed in 1.6</span>
    <span class="n">tbptt_reduce_fx</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: Remove in 1.6</span>
    <span class="n">tbptt_pad_token</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: Remove in 1.6</span>
    <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sync_dist_op</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: Remove in 1.6</span>
    <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metric_attribute</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rank_zero_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log a key, value pair.</span>

<span class="sd">    Example::</span>

<span class="sd">        self.log(&#39;train_loss&#39;, loss)</span>

<span class="sd">    The default behavior per hook is as follows:</span>

<span class="sd">    .. csv-table:: ``*`` also applies to the test loop</span>
<span class="sd">       :header: &quot;LightningModule Hook&quot;, &quot;on_step&quot;, &quot;on_epoch&quot;, &quot;prog_bar&quot;, &quot;logger&quot;</span>
<span class="sd">       :widths: 20, 10, 10, 10, 10</span>

<span class="sd">       &quot;training_step&quot;, &quot;T&quot;, &quot;F&quot;, &quot;F&quot;, &quot;T&quot;</span>
<span class="sd">       &quot;training_step_end&quot;, &quot;T&quot;, &quot;F&quot;, &quot;F&quot;, &quot;T&quot;</span>
<span class="sd">       &quot;training_epoch_end&quot;, &quot;F&quot;, &quot;T&quot;, &quot;F&quot;, &quot;T&quot;</span>
<span class="sd">       &quot;validation_step*&quot;, &quot;F&quot;, &quot;T&quot;, &quot;F&quot;, &quot;T&quot;</span>
<span class="sd">       &quot;validation_step_end*&quot;, &quot;F&quot;, &quot;T&quot;, &quot;F&quot;, &quot;T&quot;</span>
<span class="sd">       &quot;validation_epoch_end*&quot;, &quot;F&quot;, &quot;T&quot;, &quot;F&quot;, &quot;T&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        name: key to log</span>
<span class="sd">        value: value to log. Can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former.</span>
<span class="sd">        prog_bar: if True logs to the progress bar</span>
<span class="sd">        logger: if True logs to the logger</span>
<span class="sd">        on_step: if True logs at this step. None auto-logs at the training_step but not validation/test_step</span>
<span class="sd">        on_epoch: if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</span>
<span class="sd">        reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.</span>
<span class="sd">        enable_graph: if True, will not auto detach the graph</span>
<span class="sd">        sync_dist: if True, reduces the metric across GPUs/TPUs</span>
<span class="sd">        sync_dist_group: the ddp group to sync across</span>
<span class="sd">        add_dataloader_idx: if True, appends the index of the current dataloader to</span>
<span class="sd">            the name (when using multiple). If False, user needs to give unique names for</span>
<span class="sd">            each dataloader to not mix values</span>
<span class="sd">        batch_size: Current batch_size. This will be directly inferred from the loaded batch,</span>
<span class="sd">            but some data structures might need to explicitly provide it.</span>
<span class="sd">        metric_attribute: To restore the metric state, Lightning requires the reference of the</span>
<span class="sd">            :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.</span>
<span class="sd">        rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which</span>
<span class="sd">            would produce a deadlock as not all processes would perform this log call.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">tbptt_reduce_fx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank_zero_deprecation</span><span class="p">(</span>
            <span class="s2">&quot;`self.log(tbptt_reduce_fx=...)` is no longer supported. The flag will be removed in v1.6.&quot;</span>
            <span class="s2">&quot; Please, open a discussion explaining your use-case in&quot;</span>
            <span class="s2">&quot; `https://github.com/PyTorchLightning/pytorch-lightning/discussions`&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">tbptt_pad_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank_zero_deprecation</span><span class="p">(</span>
            <span class="s2">&quot;`self.log(tbptt_pad_token=...)` is no longer supported. The flag will be removed in v1.6.&quot;</span>
            <span class="s2">&quot; Please, open a discussion explaining your use-case in&quot;</span>
            <span class="s2">&quot; `https://github.com/PyTorchLightning/pytorch-lightning/discussions`&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">sync_dist_op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank_zero_deprecation</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`self.log(sync_dist_op=&#39;</span><span class="si">{</span><span class="n">sync_dist_op</span><span class="si">}</span><span class="s2">&#39;)` is deprecated and will be removed in v.1.6.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; Use `self.log(reduce_fx=</span><span class="si">{</span><span class="n">sync_dist_op</span><span class="si">}</span><span class="s2">)` instead.&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">reduce_fx</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="n">reduce_fx</span> <span class="o">=</span> <span class="n">sync_dist_op</span>
    <span class="k">elif</span> <span class="n">reduce_fx</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
        <span class="n">reduce_fx</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>

    <span class="c1"># check for invalid values</span>
    <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__check_not_nested</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">apply_to_collection</span><span class="p">(</span>
        <span class="n">value</span><span class="p">,</span> <span class="nb">object</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__check_allowed</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">wrong_dtype</span><span class="o">=</span><span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="n">Metric</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># set the default depending on the fx_name</span>
    <span class="n">on_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__auto_choose_log_on_step</span><span class="p">(</span><span class="n">on_step</span><span class="p">)</span>
    <span class="n">on_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__auto_choose_log_on_epoch</span><span class="p">(</span><span class="n">on_epoch</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_results</span>
    <span class="k">assert</span> <span class="n">results</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">FxValidator</span><span class="o">.</span><span class="n">check_logging</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">)</span>

    <span class="c1"># make sure user doesn&#39;t introduce logic for multi-dataloaders</span>
    <span class="k">if</span> <span class="s2">&quot;/dataloader_idx_&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You called `self.log` with the key `</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">`&quot;</span>
            <span class="s2">&quot; but it should not contain information about `dataloader_idx`&quot;</span>
        <span class="p">)</span>

    <span class="n">value</span> <span class="o">=</span> <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__to_tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger_connector</span><span class="o">.</span><span class="n">should_reset_tensors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">):</span>
        <span class="c1"># if we started a new epoch (running it&#39;s first batch) the hook name has changed</span>
        <span class="c1"># reset any tensors for the new hook name</span>
        <span class="n">results</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">metric_attribute</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Metric</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># compute once</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">):</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Metric</span><span class="p">)</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                    <span class="s2">&quot;Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.&quot;</span>
                    <span class="s2">&quot; You can fix this by setting an attribute for the metric in your `LightningModule`.&quot;</span>
                <span class="p">)</span>
        <span class="c1"># try to find the passed metric in the LightningModule</span>
        <span class="n">metric_attribute</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">metric_attribute</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; You can fix this by calling `self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, ..., metric_attribute=name)` where `name` is one&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; of </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="n">results</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">,</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">value</span><span class="p">,</span>
        <span class="n">prog_bar</span><span class="o">=</span><span class="n">prog_bar</span><span class="p">,</span>
        <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
        <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span>
        <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">,</span>
        <span class="n">reduce_fx</span><span class="o">=</span><span class="n">reduce_fx</span><span class="p">,</span>
        <span class="n">enable_graph</span><span class="o">=</span><span class="n">enable_graph</span><span class="p">,</span>
        <span class="n">dataloader_idx</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_dataloader_idx</span> <span class="k">if</span> <span class="n">add_dataloader_idx</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">sync_dist</span><span class="o">=</span><span class="n">sync_dist</span> <span class="ow">and</span> <span class="n">distributed_available</span><span class="p">(),</span>
        <span class="n">sync_dist_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">or</span> <span class="n">sync_ddp</span><span class="p">,</span>
        <span class="n">sync_dist_group</span><span class="o">=</span><span class="n">sync_dist_group</span><span class="p">,</span>
        <span class="n">metric_attribute</span><span class="o">=</span><span class="n">metric_attribute</span><span class="p">,</span>
        <span class="n">rank_zero_only</span><span class="o">=</span><span class="n">rank_zero_only</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger_connector</span><span class="o">.</span><span class="n">_current_fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.log_dict" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torchmetrics</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">Metric</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torchmetrics</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">Metric</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">]]]],</span> <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;default&#39;</span><span class="p">,</span> <span class="n">tbptt_reduce_fx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tbptt_pad_token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">sync_dist_op</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.log_dict" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Log a dictionary of values at once.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>values = {&#39;loss&#39;: loss, &#39;acc&#39;: acc, ..., &#39;metric_n&#39;: metric_n}
self.log_dict(values)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dictionary</code></td>
        <td><code>Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number, Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, numbers.Number]]]]</code></td>
        <td><p>key value pairs.
The values can be a <code>float</code>, <code>Tensor</code>, <code>Metric</code>, or a dictionary of the former.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>prog_bar</code></td>
        <td><code>bool</code></td>
        <td><p>if True logs to the progress base</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>logger</code></td>
        <td><code>bool</code></td>
        <td><p>if True logs to the logger</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>on_step</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>if True logs at this step. None auto-logs for training_step but not validation/test_step</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>on_epoch</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>reduce_fx</code></td>
        <td><code>Union[str, Callable]</code></td>
        <td><p>reduction function over step values for end of epoch. :meth:<code>torch.mean</code> by default.</p></td>
        <td><code>&#39;default&#39;</code></td>
      </tr>
      <tr>
        <td><code>enable_graph</code></td>
        <td><code>bool</code></td>
        <td><p>if True, will not auto detach the graph</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sync_dist</code></td>
        <td><code>bool</code></td>
        <td><p>if True, reduces the metric across GPUs/TPUs</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sync_dist_group</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>the ddp group sync across</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>add_dataloader_idx</code></td>
        <td><code>bool</code></td>
        <td><p>if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_dict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dictionary</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_METRIC_COLLECTION</span><span class="p">],</span>
    <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>  <span class="c1"># TODO: change to &#39;mean&#39; when `sync_dist_op` is removed in 1.6</span>
    <span class="n">tbptt_reduce_fx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: Remove in 1.6</span>
    <span class="n">tbptt_pad_token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: Remove in 1.6</span>
    <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sync_dist_op</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: Remove in 1.6</span>
    <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log a dictionary of values at once.</span>

<span class="sd">    Example::</span>

<span class="sd">        values = {&#39;loss&#39;: loss, &#39;acc&#39;: acc, ..., &#39;metric_n&#39;: metric_n}</span>
<span class="sd">        self.log_dict(values)</span>

<span class="sd">    Args:</span>
<span class="sd">        dictionary: key value pairs.</span>
<span class="sd">            The values can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former.</span>
<span class="sd">        prog_bar: if True logs to the progress base</span>
<span class="sd">        logger: if True logs to the logger</span>
<span class="sd">        on_step: if True logs at this step. None auto-logs for training_step but not validation/test_step</span>
<span class="sd">        on_epoch: if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</span>
<span class="sd">        reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.</span>
<span class="sd">        enable_graph: if True, will not auto detach the graph</span>
<span class="sd">        sync_dist: if True, reduces the metric across GPUs/TPUs</span>
<span class="sd">        sync_dist_group: the ddp group sync across</span>
<span class="sd">        add_dataloader_idx: if True, appends the index of the current dataloader to</span>
<span class="sd">            the name (when using multiple). If False, user needs to give unique names for</span>
<span class="sd">            each dataloader to not mix values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">v</span><span class="p">,</span>
            <span class="n">prog_bar</span><span class="o">=</span><span class="n">prog_bar</span><span class="p">,</span>
            <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
            <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span>
            <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">,</span>
            <span class="n">reduce_fx</span><span class="o">=</span><span class="n">reduce_fx</span><span class="p">,</span>
            <span class="n">enable_graph</span><span class="o">=</span><span class="n">enable_graph</span><span class="p">,</span>
            <span class="n">sync_dist</span><span class="o">=</span><span class="n">sync_dist</span><span class="p">,</span>
            <span class="n">sync_dist_group</span><span class="o">=</span><span class="n">sync_dist_group</span><span class="p">,</span>
            <span class="n">sync_dist_op</span><span class="o">=</span><span class="n">sync_dist_op</span><span class="p">,</span>
            <span class="n">tbptt_pad_token</span><span class="o">=</span><span class="n">tbptt_pad_token</span><span class="p">,</span>
            <span class="n">tbptt_reduce_fx</span><span class="o">=</span><span class="n">tbptt_reduce_fx</span><span class="p">,</span>
            <span class="n">add_dataloader_idx</span><span class="o">=</span><span class="n">add_dataloader_idx</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.log_grad_norm" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.log_grad_norm" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Override this method to change the default behaviour of <code>log_grad_norm</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>grad_norm_dict</code></td>
        <td><code>Dict[str, torch.Tensor]</code></td>
        <td><p>Dictionary containing current grad norm metrics</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code># DEFAULT
def log_grad_norm(self, grad_norm_dict):
    self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Override this method to change the default behaviour of ``log_grad_norm``.</span>

<span class="sd">    Args:</span>
<span class="sd">        grad_norm_dict: Dictionary containing current grad norm metrics</span>

<span class="sd">    Example::</span>

<span class="sd">        # DEFAULT</span>
<span class="sd">        def log_grad_norm(self, grad_norm_dict):</span>
<span class="sd">            self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.lr_schedulers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">lr_schedulers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.lr_schedulers" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no
schedulers were returned in </code></td>
      <td><p>meth:<code>configure_optimizers</code>.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">lr_schedulers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no</span>
<span class="sd">        schedulers were returned in :meth:`configure_optimizers`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># ignore other keys &quot;interval&quot;, &quot;frequency&quot;, etc.</span>
    <span class="n">lr_schedulers</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">]</span>

    <span class="c1"># single scheduler</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_schedulers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lr_schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># multiple schedulers</span>
    <span class="k">return</span> <span class="n">lr_schedulers</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.manual_backward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">manual_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.manual_backward" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Call this directly from your :meth:<code>training_step</code> when doing optimizations manually.
By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See :ref:<code>manual optimization&lt;common/optimizers:Manual optimization&gt;</code> for more examples.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>def training_step(...):
    opt = self.optimizers()
    loss = ...
    opt.zero_grad()
    # automatically applies scaling, etc...
    self.manual_backward(loss)
    opt.step()
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>loss</code></td>
        <td><code>Tensor</code></td>
        <td><p>The tensor on which to compute gradients. Must have a graph attached.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>*args</code></td>
        <td></td>
        <td><p>Additional positional arguments to be forwarded to :meth:<code>~torch.Tensor.backward</code></p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Additional keyword arguments to be forwarded to :meth:<code>~torch.Tensor.backward</code></p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">manual_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Call this directly from your :meth:`training_step` when doing optimizations manually.</span>
<span class="sd">    By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision.</span>

<span class="sd">    See :ref:`manual optimization&lt;common/optimizers:Manual optimization&gt;` for more examples.</span>

<span class="sd">    Example::</span>

<span class="sd">        def training_step(...):</span>
<span class="sd">            opt = self.optimizers()</span>
<span class="sd">            loss = ...</span>
<span class="sd">            opt.zero_grad()</span>
<span class="sd">            # automatically applies scaling, etc...</span>
<span class="sd">            self.manual_backward(loss)</span>
<span class="sd">            opt.step()</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: The tensor on which to compute gradients. Must have a graph attached.</span>
<span class="sd">        *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`</span>
<span class="sd">        **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># make sure we&#39;re using manual opt</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_verify_is_manual_optimization</span><span class="p">(</span><span class="s2">&quot;manual_backward&quot;</span><span class="p">)</span>

    <span class="c1"># backward</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">fit_loop</span><span class="o">.</span><span class="n">epoch_loop</span><span class="o">.</span><span class="n">batch_loop</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.modules" class="doc doc-heading">
<code class="highlight language-python"><span class="n">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Module</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.modules" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over all modules in the network.</p>
<p>!!! yields
    Module: a module in the network</p>
<p>!!! note
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, &#39;-&gt;&#39;, m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>

<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>

<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>

<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_buffers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_buffers" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prefix</code></td>
        <td><code>str</code></td>
        <td><p>prefix to prepend to all buffer names.</p></td>
        <td><code>&#39;&#39;</code></td>
      </tr>
      <tr>
        <td><code>recurse</code></td>
        <td><code>bool</code></td>
        <td><p>if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>!!! yields
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in [&#39;running_var&#39;]:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>

<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>

<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_children" class="doc doc-heading">
<code class="highlight language-python"><span class="n">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Module</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_children" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>!!! yields
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:
&gt;&gt;&gt;         print(module)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>

<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_modules" class="doc doc-heading">
<code class="highlight language-python"><span class="n">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_modules" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>memo</code></td>
        <td><code>Optional[Set[Module]]</code></td>
        <td><p>a memo to store the set of modules already added to the result</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prefix</code></td>
        <td><code>str</code></td>
        <td><p>a prefix that will be added to the name of the module</p></td>
        <td><code>&#39;&#39;</code></td>
      </tr>
      <tr>
        <td><code>remove_duplicate</code></td>
        <td><code>bool</code></td>
        <td><p>whether to remove the duplicated module instances in the result</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>!!! yields
    (string, Module): Tuple of name and module</p>
<p>!!! note
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, &#39;-&gt;&#39;, m)

0 -&gt; (&#39;&#39;, Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>

<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>

<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>

<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>

<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_parameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.named_parameters" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prefix</code></td>
        <td><code>str</code></td>
        <td><p>prefix to prepend to all parameter names.</p></td>
        <td><code>&#39;&#39;</code></td>
      </tr>
      <tr>
        <td><code>recurse</code></td>
        <td><code>bool</code></td>
        <td><p>if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>!!! yields
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in [&#39;bias&#39;]:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>

<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>

<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_after_backward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_after_backward" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called after <code>loss.backward()</code> and before optimizers are stepped.</p>
<p>!!! note
    If using native AMP, the gradients will not be unscaled at this point.
    Use the <code>on_before_optimizer_step</code> if you need the unscaled gradients.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called after ``loss.backward()`` and before optimizers are stepped.</span>

<span class="sd">    Note:</span>
<span class="sd">        If using native AMP, the gradients will not be unscaled at this point.</span>
<span class="sd">        Use the ``on_before_optimizer_step`` if you need the unscaled gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_after_batch_transfer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_after_batch_transfer" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<p>!!! note
    To check the current state of execution of this hook you can use
    <code>self.trainer.training/testing/validating/predicting</code> so that you can
    add different logic as per your requirement.</p>
<p>!!! note
    This hook only runs on single GPU training and DDP (no data-parallel).
    Data-Parallel support will come in near future.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>A batch of data that needs to be altered or augmented.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of the dataloader to which the batch belongs.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Any</code></td>
      <td><p>A batch of data</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def on_after_batch_transfer(self, batch, dataloader_idx):
    batch[&#39;x&#39;] = gpu_transforms(batch[&#39;x&#39;])
    return batch
</code></pre></div>
      <p>See Also:
    - :meth:<code>on_before_batch_transfer</code>
    - :meth:<code>transfer_batch_to_device</code></p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override to alter or apply batch augmentations to your batch after it is transferred to the device.</span>

<span class="sd">    Note:</span>
<span class="sd">        To check the current state of execution of this hook you can use</span>
<span class="sd">        ``self.trainer.training/testing/validating/predicting`` so that you can</span>
<span class="sd">        add different logic as per your requirement.</span>

<span class="sd">    Note:</span>
<span class="sd">        This hook only runs on single GPU training and DDP (no data-parallel).</span>
<span class="sd">        Data-Parallel support will come in near future.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: A batch of data that needs to be altered or augmented.</span>
<span class="sd">        dataloader_idx: The index of the dataloader to which the batch belongs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A batch of data</span>

<span class="sd">    Example::</span>

<span class="sd">        def on_after_batch_transfer(self, batch, dataloader_idx):</span>
<span class="sd">            batch[&#39;x&#39;] = gpu_transforms(batch[&#39;x&#39;])</span>
<span class="sd">            return batch</span>

<span class="sd">    Raises:</span>
<span class="sd">        MisconfigurationException:</span>
<span class="sd">            If using data-parallel, ``Trainer(accelerator=&#39;dp&#39;)``.</span>

<span class="sd">    See Also:</span>
<span class="sd">        - :meth:`on_before_batch_transfer`</span>
<span class="sd">        - :meth:`transfer_batch_to_device`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">batch</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_backward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_before_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_backward" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called before <code>loss.backward()</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>loss</code></td>
        <td><code>Tensor</code></td>
        <td><p>Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_before_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called before ``loss.backward()``.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_batch_transfer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_batch_transfer" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<p>!!! note
    To check the current state of execution of this hook you can use
    <code>self.trainer.training/testing/validating/predicting</code> so that you can
    add different logic as per your requirement.</p>
<p>!!! note
    This hook only runs on single GPU training and DDP (no data-parallel).
    Data-Parallel support will come in near future.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>A batch of data that needs to be altered or augmented.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of the dataloader to which the batch belongs.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Any</code></td>
      <td><p>A batch of data</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def on_before_batch_transfer(self, batch, dataloader_idx):
    batch[&#39;x&#39;] = transforms(batch[&#39;x&#39;])
    return batch
</code></pre></div>
      <p>See Also:
    - :meth:<code>on_after_batch_transfer</code>
    - :meth:<code>transfer_batch_to_device</code></p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override to alter or apply batch augmentations to your batch before it is transferred to the device.</span>

<span class="sd">    Note:</span>
<span class="sd">        To check the current state of execution of this hook you can use</span>
<span class="sd">        ``self.trainer.training/testing/validating/predicting`` so that you can</span>
<span class="sd">        add different logic as per your requirement.</span>

<span class="sd">    Note:</span>
<span class="sd">        This hook only runs on single GPU training and DDP (no data-parallel).</span>
<span class="sd">        Data-Parallel support will come in near future.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: A batch of data that needs to be altered or augmented.</span>
<span class="sd">        dataloader_idx: The index of the dataloader to which the batch belongs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A batch of data</span>

<span class="sd">    Example::</span>

<span class="sd">        def on_before_batch_transfer(self, batch, dataloader_idx):</span>
<span class="sd">            batch[&#39;x&#39;] = transforms(batch[&#39;x&#39;])</span>
<span class="sd">            return batch</span>

<span class="sd">    Raises:</span>
<span class="sd">        MisconfigurationException:</span>
<span class="sd">            If using data-parallel, ``Trainer(accelerator=&#39;dp&#39;)``.</span>

<span class="sd">    See Also:</span>
<span class="sd">        - :meth:`on_after_batch_transfer`</span>
<span class="sd">        - :meth:`transfer_batch_to_device`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">batch</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_optimizer_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_optimizer_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called before <code>optimizer.step()</code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: :paramref:<code>~pytorch_lightning.trainer.Trainer.accumulate_grad_batches</code>.
If using native AMP, the loss will be unscaled before calling this hook.
See these <code>docs &lt;https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients&gt;</code>__
for more information on the scaling of gradients.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Optimizer</code></td>
        <td><p>Current optimizer being used.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>int</code></td>
        <td><p>Index of the current optimizer being used.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def on_before_optimizer_step(self, optimizer, optimizer_idx):
    # example to inspect gradient information in tensorboard
    if self.trainer.global_step % 25 == 0:  # don&#39;t make the tf file huge
        for k, v in self.named_parameters():
            self.logger.experiment.add_histogram(
                tag=k, values=v.grad, global_step=self.trainer.global_step
            )
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called before ``optimizer.step()``.</span>

<span class="sd">    The hook is only called if gradients do not need to be accumulated.</span>
<span class="sd">    See: :paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`.</span>
<span class="sd">    If using native AMP, the loss will be unscaled before calling this hook.</span>
<span class="sd">    See these `docs &lt;https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients&gt;`__</span>
<span class="sd">    for more information on the scaling of gradients.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer: Current optimizer being used.</span>
<span class="sd">        optimizer_idx: Index of the current optimizer being used.</span>

<span class="sd">    Example::</span>

<span class="sd">        def on_before_optimizer_step(self, optimizer, optimizer_idx):</span>
<span class="sd">            # example to inspect gradient information in tensorboard</span>
<span class="sd">            if self.trainer.global_step % 25 == 0:  # don&#39;t make the tf file huge</span>
<span class="sd">                for k, v in self.named_parameters():</span>
<span class="sd">                    self.logger.experiment.add_histogram(</span>
<span class="sd">                        tag=k, values=v.grad, global_step=self.trainer.global_step</span>
<span class="sd">                    )</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_zero_grad" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_before_zero_grad" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called after <code>training_step()</code> and before <code>optimizer.zero_grad()</code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called::</p>
<div class="highlight"><pre><span></span><code>for optimizer in optimizers:
    out = training_step(...)

    model.on_before_zero_grad(optimizer) # &lt; ---- called here
    optimizer.zero_grad()

    backward()
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Optimizer</code></td>
        <td><p>The optimizer for which grads should be zeroed.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_before_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called after ``training_step()`` and before ``optimizer.zero_grad()``.</span>

<span class="sd">    Called in the training loop after taking an optimizer step and before zeroing grads.</span>
<span class="sd">    Good place to inspect weight information with weights updated.</span>

<span class="sd">    This is where it is called::</span>

<span class="sd">        for optimizer in optimizers:</span>
<span class="sd">            out = training_step(...)</span>

<span class="sd">            model.on_before_zero_grad(optimizer) # &lt; ---- called here</span>
<span class="sd">            optimizer.zero_grad()</span>

<span class="sd">            backward()</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer: The optimizer for which grads should be zeroed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called when either of train/val/test epoch ends.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called when either of train/val/test epoch ends.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_epoch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_epoch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called when either of train/val/test epoch begins.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called when either of train/val/test epoch begins.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_fit_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_fit_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_fit_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the very end of fit.
If on DDP it is called on every process</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_fit_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the very end of fit.</span>
<span class="sd">    If on DDP it is called on every process</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_fit_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_fit_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the very beginning of fit.
If on DDP it is called on every process</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the very beginning of fit.</span>
<span class="sd">    If on DDP it is called on every process</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_hpc_load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_hpc_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_hpc_load" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Hook to do whatever you need right before Slurm manager loads the model.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>checkpoint</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>A dictionary with variables from the checkpoint.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_hpc_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook to do whatever you need right before Slurm manager loads the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoint: A dictionary with variables from the checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_hpc_save" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_hpc_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_hpc_save" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Hook to do whatever you need right before Slurm manager saves the model.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>checkpoint</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_hpc_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook to do whatever you need right before Slurm manager saves the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoint: A dictionary in which you can save variables to save in a checkpoint.</span>
<span class="sd">            Contents need to be pickleable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_load_checkpoint" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_load_checkpoint" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Do something with the checkpoint.
Gives model a chance to load something before <code>state_dict</code> is restored.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>checkpoint</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>A dictionary with variables from the checkpoint.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Do something with the checkpoint.</span>
<span class="sd">    Gives model a chance to load something before ``state_dict`` is restored.</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoint: A dictionary with variables from the checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_post_move_to_device" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_post_move_to_device" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the <code>parameter_validation</code> decorator after :meth:<code>~pytorch_lightning.core.LightningModule.to</code>
is called. This is a good place to tie weights between modules after moving them to a device. Can be
used when training models with weight sharing properties on TPU.</p>
<p>Addresses the handling of shared weights on TPU:
https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>def on_post_move_to_device(self):
    self.decoder.weight = self.encoder.weight
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the ``parameter_validation`` decorator after :meth:`~pytorch_lightning.core.LightningModule.to`</span>
<span class="sd">    is called. This is a good place to tie weights between modules after moving them to a device. Can be</span>
<span class="sd">    used when training models with weight sharing properties on TPU.</span>

<span class="sd">    Addresses the handling of shared weights on TPU:</span>
<span class="sd">    https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</span>

<span class="sd">    Example::</span>

<span class="sd">        def on_post_move_to_device(self):</span>
<span class="sd">            self.decoder.weight = self.encoder.weight</span>

<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_batch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_batch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the predict loop after the batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>The outputs of predict_step_end(test_step(x))</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the test DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the predict loop after the batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs: The outputs of predict_step_end(test_step(x))</span>
<span class="sd">        batch: The batched data as it is returned by the test DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_batch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_batch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the predict loop before anything happens for that batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the test DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the predict loop before anything happens for that batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batched data as it is returned by the test DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called before requesting the predict dataloader.</p>
<p>.. deprecated:: v1.5
    :meth:<code>on_predict_dataloader</code> is deprecated and will be removed in v1.7.0.
    Please use :meth:<code>predict_dataloader()</code> directly.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Called before requesting the predict dataloader.</span>

<span class="sd">    .. deprecated:: v1.5</span>
<span class="sd">        :meth:`on_predict_dataloader` is deprecated and will be removed in v1.7.0.</span>
<span class="sd">        Please use :meth:`predict_dataloader()` directly.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of predicting.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of predicting.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of predicting.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of predicting.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_epoch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_epoch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of predicting.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the beginning of predicting.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_model_eval" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_model_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_model_eval" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the model to eval during the predict loop</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_model_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the model to eval during the predict loop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_predict_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_predict_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of predicting.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the beginning of predicting.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_pretrain_routine_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_pretrain_routine_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_pretrain_routine_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul>
<li>fit</li>
<li>pretrain_routine start</li>
<li>pretrain_routine end</li>
<li>training_start</li>
</ul>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_pretrain_routine_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of the pretrain routine (between fit and train start).</span>

<span class="sd">    - fit</span>
<span class="sd">    - pretrain_routine start</span>
<span class="sd">    - pretrain_routine end</span>
<span class="sd">    - training_start</span>

<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_pretrain_routine_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_pretrain_routine_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_pretrain_routine_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul>
<li>fit</li>
<li>pretrain_routine start</li>
<li>pretrain_routine end</li>
<li>training_start</li>
</ul>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_pretrain_routine_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the beginning of the pretrain routine (between fit and train start).</span>

<span class="sd">    - fit</span>
<span class="sd">    - pretrain_routine start</span>
<span class="sd">    - pretrain_routine end</span>
<span class="sd">    - training_start</span>

<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_save_checkpoint" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_save_checkpoint" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Give the model a chance to add something to the checkpoint.
<code>state_dict</code> is already there.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>checkpoint</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Give the model a chance to add something to the checkpoint.</span>
<span class="sd">    ``state_dict`` is already there.</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoint: A dictionary in which you can save variables to save in a checkpoint.</span>
<span class="sd">            Contents need to be pickleable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_batch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_batch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the test loop after the batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>Union[torch.Tensor, Dict[str, Any]]</code></td>
        <td><p>The outputs of test_step_end(test_step(x))</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the test DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_batch_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the test loop after the batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs: The outputs of test_step_end(test_step(x))</span>
<span class="sd">        batch: The batched data as it is returned by the test DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_batch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_batch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the test loop before anything happens for that batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the test DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the test loop before anything happens for that batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batched data as it is returned by the test DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called before requesting the test dataloader.</p>
<p>.. deprecated:: v1.5
    :meth:<code>on_test_dataloader</code> is deprecated and will be removed in v1.7.0.
    Please use :meth:<code>test_dataloader()</code> directly.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Called before requesting the test dataloader.</span>

<span class="sd">    .. deprecated:: v1.5</span>
<span class="sd">        :meth:`on_test_dataloader` is deprecated and will be removed in v1.7.0.</span>
<span class="sd">        Please use :meth:`test_dataloader()` directly.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of testing.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of testing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the test loop at the very end of the epoch.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the test loop at the very end of the epoch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_epoch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_epoch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the test loop at the very beginning of the epoch.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the test loop at the very beginning of the epoch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_model_eval" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_model_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_model_eval" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the model to eval during the test loop</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_model_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the model to eval during the test loop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_model_train" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_model_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_model_train" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the model to train during the test loop</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_model_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the model to train during the test loop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_test_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_test_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of testing.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_test_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the beginning of testing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_batch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_batch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the training loop after the batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>Union[torch.Tensor, Dict[str, Any]]</code></td>
        <td><p>The outputs of training_step_end(training_step(x))</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the training DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">STEP_OUTPUT</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the training loop after the batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs: The outputs of training_step_end(training_step(x))</span>
<span class="sd">        batch: The batched data as it is returned by the training DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_batch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_batch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the training DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the training loop before anything happens for that batch.</span>

<span class="sd">    If you return -1 here, you will skip training for the rest of the current epoch.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batched data as it is returned by the training DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called before requesting the train dataloader.</p>
<p>.. deprecated:: v1.5
    :meth:<code>on_train_dataloader</code> is deprecated and will be removed in v1.7.0.
    Please use :meth:<code>train_dataloader()</code> directly.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Called before requesting the train dataloader.</span>

<span class="sd">    .. deprecated:: v1.5</span>
<span class="sd">        :meth:`on_train_dataloader` is deprecated and will be removed in v1.7.0.</span>
<span class="sd">        Please use :meth:`train_dataloader()` directly.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of training before logger experiment is closed.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of training before logger experiment is closed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol>
<li>Implement <code>training_epoch_end</code> in the LightningModule OR</li>
<li>Cache data across steps on the attribute(s) of the <code>LightningModule</code> and access them in this hook</li>
</ol>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused</span><span class="p">:</span> <span class="n">Optional</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the training loop at the very end of the epoch.</span>

<span class="sd">    To access all batch outputs at the end of the epoch, either:</span>

<span class="sd">    1. Implement `training_epoch_end` in the LightningModule OR</span>
<span class="sd">    2. Cache data across steps on the attribute(s) of the `LightningModule` and access them in this hook</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_epoch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_epoch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the training loop at the very beginning of the epoch.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the training loop at the very beginning of the epoch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_train_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of training after sanity check.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;val_macro_f1&quot;</span><span class="p">:</span> <span class="p">{}}</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;val_top_</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">_accuracy&quot;</span><span class="p">:</span> <span class="p">{}</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">DEFAULT_TOP_K</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">}</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;val_accuracy&quot;</span><span class="p">:</span> <span class="p">{}})</span>

    <span class="c1"># write hparams to hparams.yaml file, log metrics to tb hparams tab</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log_hyperparams</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">,</span> <span class="n">metrics</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_val_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_val_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called before requesting the val dataloader.</p>
<p>.. deprecated:: v1.5
    :meth:<code>on_val_dataloader</code> is deprecated and will be removed in v1.7.0.
    Please use :meth:<code>val_dataloader()</code> directly.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Called before requesting the val dataloader.</span>

<span class="sd">    .. deprecated:: v1.5</span>
<span class="sd">        :meth:`on_val_dataloader` is deprecated and will be removed in v1.7.0.</span>
<span class="sd">        Please use :meth:`val_dataloader()` directly.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_batch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_batch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the validation loop after the batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>Union[torch.Tensor, Dict[str, Any]]</code></td>
        <td><p>The outputs of validation_step_end(validation_step(x))</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the validation DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_batch_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">],</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the validation loop after the batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs: The outputs of validation_step_end(validation_step(x))</span>
<span class="sd">        batch: The batched data as it is returned by the validation DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_batch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_batch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the validation loop before anything happens for that batch.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>The batched data as it is returned by the validation DataLoader.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>the index of the dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the validation loop before anything happens for that batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batched data as it is returned by the validation DataLoader.</span>
<span class="sd">        batch_idx: the index of the batch</span>
<span class="sd">        dataloader_idx: the index of the dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of validation.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of validation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the validation loop at the very end of the epoch.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the validation loop at the very end of the epoch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_epoch_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_epoch_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called in the validation loop at the very beginning of the epoch.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called in the validation loop at the very beginning of the epoch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_model_eval" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_model_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_model_eval" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the model to eval during the val loop</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_model_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the model to eval during the val loop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_model_train" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_model_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_model_train" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the model to train during the val loop</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_model_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the model to train during the val loop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_start" class="doc doc-heading">
<code class="highlight language-python"><span class="n">on_validation_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.on_validation_start" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of validation.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_start</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the beginning of validation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.optimizer_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.optimizer_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Override this method to adjust the default way the
:class:<code>~pytorch_lightning.trainer.trainer.Trainer</code> calls each optimizer.
By default, Lightning calls <code>step()</code> and <code>zero_grad()</code> as shown in the example
once per optimizer. This method (and <code>zero_grad()</code>) won't be called during the
accumulation phase when <code>Trainer(accumulate_grad_batches != 1)</code>.</p>
<p>!!! warning
    If you are overriding this method, make sure that you pass the <code>optimizer_closure</code> parameter
    to <code>optimizer.step()</code> function as shown in the examples. This ensures that
    <code>training_step()</code>, <code>optimizer.zero_grad()</code>, <code>backward()</code> are called within the training loop.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>epoch</code></td>
        <td><code>int</code></td>
        <td><p>Current epoch</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>Index of current batch</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Optimizer</code></td>
        <td><p>A PyTorch optimizer</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>int</code></td>
        <td><p>If you used multiple optimizers, this indexes into that list.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>optimizer_closure</code></td>
        <td><code>Optional[Callable]</code></td>
        <td><p>Closure for all optimizers</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>on_tpu</code></td>
        <td><code>bool</code></td>
        <td><p><code>True</code> if TPU backward is required</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>using_native_amp</code></td>
        <td><code>bool</code></td>
        <td><p><code>True</code> if using native amp</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>using_lbfgs</code></td>
        <td><code>bool</code></td>
        <td><p>True if the matching optimizer is :class:<code>torch.optim.LBFGS</code></p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>      <p>Examples::</p>
<div class="highlight"><pre><span></span><code># DEFAULT
def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,
                   optimizer_closure, on_tpu, using_native_amp, using_lbfgs):
    optimizer.step(closure=optimizer_closure)

# Alternating schedule for optimizer steps (i.e.: GANs)
def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,
                   optimizer_closure, on_tpu, using_native_amp, using_lbfgs):
    # update generator opt every step
    if optimizer_idx == 0:
        optimizer.step(closure=optimizer_closure)

    # update discriminator opt every 2 steps
    if optimizer_idx == 1:
        if (batch_idx + 1) % 2 == 0 :
            optimizer.step(closure=optimizer_closure)

    # ...
    # add as many optimizers as you want
</code></pre></div>
<p>Here's another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># learning rate warm-up
def optimizer_step(
    self,
    epoch,
    batch_idx,
    optimizer,
    optimizer_idx,
    optimizer_closure,
    on_tpu,
    using_native_amp,
    using_lbfgs,
):
    # warm up lr
    if self.trainer.global_step &lt; 500:
        lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0)
        for pg in optimizer.param_groups:
            pg[&quot;lr&quot;] = lr_scale * self.learning_rate

    # update params
    optimizer.step(closure=optimizer_closure)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override this method to adjust the default way the</span>
<span class="sd">    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.</span>
<span class="sd">    By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example</span>
<span class="sd">    once per optimizer. This method (and ``zero_grad()``) won&#39;t be called during the</span>
<span class="sd">    accumulation phase when ``Trainer(accumulate_grad_batches != 1)``.</span>

<span class="sd">    Warning:</span>
<span class="sd">        If you are overriding this method, make sure that you pass the ``optimizer_closure`` parameter</span>
<span class="sd">        to ``optimizer.step()`` function as shown in the examples. This ensures that</span>
<span class="sd">        ``training_step()``, ``optimizer.zero_grad()``, ``backward()`` are called within the training loop.</span>

<span class="sd">    Args:</span>
<span class="sd">        epoch: Current epoch</span>
<span class="sd">        batch_idx: Index of current batch</span>
<span class="sd">        optimizer: A PyTorch optimizer</span>
<span class="sd">        optimizer_idx: If you used multiple optimizers, this indexes into that list.</span>
<span class="sd">        optimizer_closure: Closure for all optimizers</span>
<span class="sd">        on_tpu: ``True`` if TPU backward is required</span>
<span class="sd">        using_native_amp: ``True`` if using native amp</span>
<span class="sd">        using_lbfgs: True if the matching optimizer is :class:`torch.optim.LBFGS`</span>

<span class="sd">    Examples::</span>

<span class="sd">        # DEFAULT</span>
<span class="sd">        def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,</span>
<span class="sd">                           optimizer_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="sd">            optimizer.step(closure=optimizer_closure)</span>

<span class="sd">        # Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="sd">        def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,</span>
<span class="sd">                           optimizer_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="sd">            # update generator opt every step</span>
<span class="sd">            if optimizer_idx == 0:</span>
<span class="sd">                optimizer.step(closure=optimizer_closure)</span>

<span class="sd">            # update discriminator opt every 2 steps</span>
<span class="sd">            if optimizer_idx == 1:</span>
<span class="sd">                if (batch_idx + 1) % 2 == 0 :</span>
<span class="sd">                    optimizer.step(closure=optimizer_closure)</span>

<span class="sd">            # ...</span>
<span class="sd">            # add as many optimizers as you want</span>

<span class="sd">    Here&#39;s another example showing how to use this for more advanced things such as</span>
<span class="sd">    learning rate warm-up:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # learning rate warm-up</span>
<span class="sd">        def optimizer_step(</span>
<span class="sd">            self,</span>
<span class="sd">            epoch,</span>
<span class="sd">            batch_idx,</span>
<span class="sd">            optimizer,</span>
<span class="sd">            optimizer_idx,</span>
<span class="sd">            optimizer_closure,</span>
<span class="sd">            on_tpu,</span>
<span class="sd">            using_native_amp,</span>
<span class="sd">            using_lbfgs,</span>
<span class="sd">        ):</span>
<span class="sd">            # warm up lr</span>
<span class="sd">            if self.trainer.global_step &lt; 500:</span>
<span class="sd">                lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0)</span>
<span class="sd">                for pg in optimizer.param_groups:</span>
<span class="sd">                    pg[&quot;lr&quot;] = lr_scale * self.learning_rate</span>

<span class="sd">            # update params</span>
<span class="sd">            optimizer.step(closure=optimizer_closure)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.optimizer_zero_grad" class="doc doc-heading">
<code class="highlight language-python"><span class="n">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.optimizer_zero_grad" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Override this method to change the default behaviour of <code>optimizer.zero_grad()</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>epoch</code></td>
        <td><code>int</code></td>
        <td><p>Current epoch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>Index of current batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Optimizer</code></td>
        <td><p>A PyTorch optimizer</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>int</code></td>
        <td><p>If you used multiple optimizers this indexes into that list.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>Examples::</p>
<div class="highlight"><pre><span></span><code># DEFAULT
def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
    optimizer.zero_grad()

# Set gradients to `None` instead of zero to improve performance.
def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):
    optimizer.zero_grad(set_to_none=True)
</code></pre></div>
<p>See :meth:<code>torch.optim.Optimizer.zero_grad</code> for the explanation of the above example.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Override this method to change the default behaviour of ``optimizer.zero_grad()``.</span>

<span class="sd">    Args:</span>
<span class="sd">        epoch: Current epoch</span>
<span class="sd">        batch_idx: Index of current batch</span>
<span class="sd">        optimizer: A PyTorch optimizer</span>
<span class="sd">        optimizer_idx: If you used multiple optimizers this indexes into that list.</span>

<span class="sd">    Examples::</span>

<span class="sd">        # DEFAULT</span>
<span class="sd">        def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):</span>
<span class="sd">            optimizer.zero_grad()</span>

<span class="sd">        # Set gradients to `None` instead of zero to improve performance.</span>
<span class="sd">        def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):</span>
<span class="sd">            optimizer.zero_grad(set_to_none=True)</span>

<span class="sd">    See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.optimizers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">LightningOptimizer</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.optimizers" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>use_pl_optimizer</code></td>
        <td><code>bool</code></td>
        <td><p>If <code>True</code>, will wrap the optimizer(s) in a
:class:<code>~pytorch_lightning.core.optimizer.LightningOptimizer</code> for automatic handling of precision and
profiling.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer, List[torch.optim.optimizer.Optimizer], List[pytorch_lightning.core.optimizer.LightningOptimizer]]</code></td>
      <td><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the optimizer(s) that are being used during training. Useful for manual optimization.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a</span>
<span class="sd">            :class:`~pytorch_lightning.core.optimizer.LightningOptimizer` for automatic handling of precision and</span>
<span class="sd">            profiling.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A single optimizer, or a list of optimizers in case multiple ones are present.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">use_pl_optimizer</span><span class="p">:</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lightning_optimizers</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span>

    <span class="c1"># single optimizer</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">opts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># multiple opts</span>
    <span class="k">return</span> <span class="n">opts</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.parameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.parameters" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>recurse</code></td>
        <td><code>bool</code></td>
        <td><p>if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>!!! yields
    Parameter: module parameter</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class &#39;torch.Tensor&#39;&gt; (20L,)
&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>

<span class="sd">    This is typically passed to an optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>

<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.predict_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">predict_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.predict_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>...</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<p>!!! note
    Lightning adds the correct sampler for distributed and arbitrary hardware
    There is no need to set it yourself.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>A </code></td>
      <td><p>class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying prediction samples.</p></td>
    </tr>
  </tbody>
</table>      <p>!!! note
    In the case where you return multiple prediction dataloaders, the :meth:<code>predict</code>
    will have an argument <code>dataloader_idx</code> which matches the order here.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EVAL_DATALOADERS</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement one or multiple PyTorch DataLoaders for prediction.</span>

<span class="sd">    It&#39;s recommended that all data downloads and preparation happen in :meth:`prepare_data`.</span>

<span class="sd">    - :meth:`~pytorch_lightning.trainer.Trainer.fit`</span>
<span class="sd">    - ...</span>
<span class="sd">    - :meth:`prepare_data`</span>
<span class="sd">    - :meth:`train_dataloader`</span>
<span class="sd">    - :meth:`val_dataloader`</span>
<span class="sd">    - :meth:`test_dataloader`</span>

<span class="sd">    Note:</span>
<span class="sd">        Lightning adds the correct sampler for distributed and arbitrary hardware</span>
<span class="sd">        There is no need to set it yourself.</span>

<span class="sd">    Return:</span>
<span class="sd">        A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.</span>

<span class="sd">    Note:</span>
<span class="sd">        In the case where you return multiple prediction dataloaders, the :meth:`predict`</span>
<span class="sd">        will have an argument ``dataloader_idx`` which matches the order here.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.predict_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.predict_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Step function called during :meth:<code>~pytorch_lightning.trainer.trainer.Trainer.predict</code>.
By default, it calls :meth:<code>~pytorch_lightning.core.lightning.LightningModule.forward</code>.
Override to add any processing logic.</p>
<p>The :meth:<code>~pytorch_lightning.core.lightning.LightningModule.predict_step</code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use :class:<code>~pytorch_lightning.callbacks.BasePredictionWriter</code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The :class:<code>~pytorch_lightning.callbacks.BasePredictionWriter</code> should be used while using a spawn
based accelerator. This happens for <code>Trainer(accelerator="ddp_spawn")</code>
or training on 8 TPU cores with <code>Trainer(tpu_cores=8)</code> as predictions won't be returned.</p>
<p>Example ::</p>
<div class="highlight"><pre><span></span><code>class MyModel(LightningModule):

    def predicts_step(self, batch, batch_idx, dataloader_idx):
        return self(batch)

dm = ...
model = MyModel()
trainer = Trainer(gpus=2)
predictions = trainer.predict(model, dm)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>Current batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td></td>
        <td><p>Index of current batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Index of the current dataloader</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>Predicted output</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">pred</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.prepare_data" class="doc doc-heading">
<code class="highlight language-python"><span class="n">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.prepare_data" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Use this to download and prepare data.</p>
<p>.. warning:: DO NOT set state to the model (use <code>setup</code> instead)
    since this is NOT called on every GPU in DDP/TPU</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre></div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<div class="highlight"><pre><span></span><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
Trainer(prepare_data_per_node=True)

# call on GLOBAL_RANK=0 (great for shared file systems)
Trainer(prepare_data_per_node=False)
</code></pre></div>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>model.prepare_data()
initialize_distributed()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use this to download and prepare data.</span>

<span class="sd">    .. warning:: DO NOT set state to the model (use `setup` instead)</span>
<span class="sd">        since this is NOT called on every GPU in DDP/TPU</span>

<span class="sd">    Example::</span>

<span class="sd">        def prepare_data(self):</span>
<span class="sd">            # good</span>
<span class="sd">            download_data()</span>
<span class="sd">            tokenize()</span>
<span class="sd">            etc()</span>

<span class="sd">            # bad</span>
<span class="sd">            self.split = data_split</span>
<span class="sd">            self.some_state = some_other_state()</span>

<span class="sd">    In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</span>

<span class="sd">    1. Once per node. This is the default and is only called on LOCAL_RANK=0.</span>
<span class="sd">    2. Once in total. Only called on GLOBAL_RANK=0.</span>

<span class="sd">    Example::</span>

<span class="sd">        # DEFAULT</span>
<span class="sd">        # called once per node on LOCAL_RANK=0 of that node</span>
<span class="sd">        Trainer(prepare_data_per_node=True)</span>

<span class="sd">        # call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="sd">        Trainer(prepare_data_per_node=False)</span>

<span class="sd">    This is called before requesting the dataloaders:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        model.prepare_data()</span>
<span class="sd">        initialize_distributed()</span>
<span class="sd">        model.setup(stage)</span>
<span class="sd">        model.train_dataloader()</span>
<span class="sd">        model.val_dataloader()</span>
<span class="sd">        model.test_dataloader()</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.print" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.print" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Prints only from process 0. Use this in any distributed mode to log only once.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*args</code></td>
        <td></td>
        <td><p>The thing to print. The same as for Python's built-in print function.</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>The same as for Python's built-in print function.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def forward(self, x):
    self.print(x, &#39;in forward&#39;)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prints only from process 0. Use this in any distributed mode to log only once.</span>

<span class="sd">    Args:</span>
<span class="sd">        *args: The thing to print. The same as for Python&#39;s built-in print function.</span>
<span class="sd">        **kwargs: The same as for Python&#39;s built-in print function.</span>

<span class="sd">    Example::</span>

<span class="sd">        def forward(self, x):</span>
<span class="sd">            self.print(x, &#39;in forward&#39;)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
        <span class="n">progress_bar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">progress_bar_callback</span>
        <span class="k">if</span> <span class="n">progress_bar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">progress_bar</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">:</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_backward_hook" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">Union</span><span class="p">[</span><span class="n">NoneType</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_backward_hook" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>class:<code>torch.utils.hooks.RemovableHandle</code>:
    a handle that can be used to remove the added hook by calling
    <code>handle.remove()</code></p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>

<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_buffer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_buffer" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>string</code></td>
        <td><p>name of the buffer. The buffer can be accessed
from this module using the given name</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>tensor</code></td>
        <td><code>Tensor or None</code></td>
        <td><p>buffer to be registered. If <code>None</code>, then operations
that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>persistent</code></td>
        <td><code>bool</code></td>
        <td><p>whether the buffer is part of this module's
:attr:<code>state_dict</code>.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>

<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>

<span class="sd">    Buffers can be accessed as attributes using given names.</span>

<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_forward_hook" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_forward_hook" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<div class="highlight"><pre><span></span><code>hook(module, input, output) -&gt; None or modified output
</code></pre></div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>class:<code>torch.utils.hooks.RemovableHandle</code>:
    a handle that can be used to remove the added hook by calling
    <code>handle.remove()</code></p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>

<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>

<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>

<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_forward_pre_hook" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_forward_pre_hook" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<div class="highlight"><pre><span></span><code>hook(module, input) -&gt; None or modified input
</code></pre></div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>class:<code>torch.utils.hooks.RemovableHandle</code>:
    a handle that can be used to remove the added hook by calling
    <code>handle.remove()</code></p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>

<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>

<span class="sd">        hook(module, input) -&gt; None or modified input</span>

<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_full_backward_hook" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register_full_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">Union</span><span class="p">[</span><span class="n">NoneType</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_full_backward_hook" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<div class="highlight"><pre><span></span><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre></div>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>class:<code>torch.utils.hooks.RemovableHandle</code>:
    a handle that can be used to remove the added hook by calling
    <code>handle.remove()</code></p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>

<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>

<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>

<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>

<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>

<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_parameter" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.register_parameter" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>string</code></td>
        <td><p>name of the parameter. The parameter can be accessed
from this module using the given name</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>param</code></td>
        <td><code>Parameter or None</code></td>
        <td><p>parameter to be added to the module. If
<code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
module's :attr:<code>state_dict</code>.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>

<span class="sd">    The parameter can be accessed as an attribute using given name.</span>

<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.requires_grad_" class="doc doc-heading">
<code class="highlight language-python"><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.requires_grad_" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>requires_grad</code></td>
        <td><code>bool</code></td>
        <td><p>whether autograd should record operations on
                  parameters in this module. Default: <code>True</code>.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>

<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>

<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>

<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>

<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.save_hyperparameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">save_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">ignore</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">frame</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">frame</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.save_hyperparameters" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Save arguments to <code>hparams</code> attribute.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>args</code></td>
        <td></td>
        <td><p>single object of <code>dict</code>, <code>NameSpace</code> or <code>OmegaConf</code>
or string names or arguments from class <code>__init__</code></p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>ignore</code></td>
        <td><code>Union[Sequence[str], str]</code></td>
        <td><p>an argument name or a list of argument names from
class <code>__init__</code> to be ignored</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>frame</code></td>
        <td><code>Optional[frame]</code></td>
        <td><p>a frame object. Default is None</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>logger</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to send the hyperparameters to the logger. Default: True</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>      <p>Example::
    &gt;&gt;&gt; class ManuallyArgsModel(HyperparametersMixin):
    ...     def <strong>init</strong>(self, arg1, arg2, arg3):
    ...         super().<strong>init</strong>()
    ...         # manually assign arguments
    ...         self.save_hyperparameters('arg1', 'arg3')
    ...     def forward(self, <em>args, </em>*kwargs):
    ...         ...
    &gt;&gt;&gt; model = ManuallyArgsModel(1, 'abc', 3.14)
    &gt;&gt;&gt; model.hparams
    "arg1": 1
    "arg3": 3.14</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; class AutomaticArgsModel(HyperparametersMixin):
...     def __init__(self, arg1, arg2, arg3):
...         super().__init__()
...         # equivalent automatic
...         self.save_hyperparameters()
...     def forward(self, *args, **kwargs):
...         ...
&gt;&gt;&gt; model = AutomaticArgsModel(1, &#39;abc&#39;, 3.14)
&gt;&gt;&gt; model.hparams
&quot;arg1&quot;: 1
&quot;arg2&quot;: abc
&quot;arg3&quot;: 3.14

&gt;&gt;&gt; class SingleArgModel(HyperparametersMixin):
...     def __init__(self, params):
...         super().__init__()
...         # manually assign single argument
...         self.save_hyperparameters(params)
...     def forward(self, *args, **kwargs):
...         ...
&gt;&gt;&gt; model = SingleArgModel(Namespace(p1=1, p2=&#39;abc&#39;, p3=3.14))
&gt;&gt;&gt; model.hparams
&quot;p1&quot;: 1
&quot;p2&quot;: abc
&quot;p3&quot;: 3.14

&gt;&gt;&gt; class ManuallyArgsModel(HyperparametersMixin):
...     def __init__(self, arg1, arg2, arg3):
...         super().__init__()
...         # pass argument(s) to ignore as a string or in a list
...         self.save_hyperparameters(ignore=&#39;arg2&#39;)
...     def forward(self, *args, **kwargs):
...         ...
&gt;&gt;&gt; model = ManuallyArgsModel(1, &#39;abc&#39;, 3.14)
&gt;&gt;&gt; model.hparams
&quot;arg1&quot;: 1
&quot;arg3&quot;: 3.14
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">save_hyperparameters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="n">ignore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frame</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">types</span><span class="o">.</span><span class="n">FrameType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Save arguments to ``hparams`` attribute.</span>

<span class="sd">    Args:</span>
<span class="sd">        args: single object of `dict`, `NameSpace` or `OmegaConf`</span>
<span class="sd">            or string names or arguments from class ``__init__``</span>
<span class="sd">        ignore: an argument name or a list of argument names from</span>
<span class="sd">            class ``__init__`` to be ignored</span>
<span class="sd">        frame: a frame object. Default is None</span>
<span class="sd">        logger: Whether to send the hyperparameters to the logger. Default: True</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; class ManuallyArgsModel(HyperparametersMixin):</span>
<span class="sd">        ...     def __init__(self, arg1, arg2, arg3):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         # manually assign arguments</span>
<span class="sd">        ...         self.save_hyperparameters(&#39;arg1&#39;, &#39;arg3&#39;)</span>
<span class="sd">        ...     def forward(self, *args, **kwargs):</span>
<span class="sd">        ...         ...</span>
<span class="sd">        &gt;&gt;&gt; model = ManuallyArgsModel(1, &#39;abc&#39;, 3.14)</span>
<span class="sd">        &gt;&gt;&gt; model.hparams</span>
<span class="sd">        &quot;arg1&quot;: 1</span>
<span class="sd">        &quot;arg3&quot;: 3.14</span>

<span class="sd">        &gt;&gt;&gt; class AutomaticArgsModel(HyperparametersMixin):</span>
<span class="sd">        ...     def __init__(self, arg1, arg2, arg3):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         # equivalent automatic</span>
<span class="sd">        ...         self.save_hyperparameters()</span>
<span class="sd">        ...     def forward(self, *args, **kwargs):</span>
<span class="sd">        ...         ...</span>
<span class="sd">        &gt;&gt;&gt; model = AutomaticArgsModel(1, &#39;abc&#39;, 3.14)</span>
<span class="sd">        &gt;&gt;&gt; model.hparams</span>
<span class="sd">        &quot;arg1&quot;: 1</span>
<span class="sd">        &quot;arg2&quot;: abc</span>
<span class="sd">        &quot;arg3&quot;: 3.14</span>

<span class="sd">        &gt;&gt;&gt; class SingleArgModel(HyperparametersMixin):</span>
<span class="sd">        ...     def __init__(self, params):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         # manually assign single argument</span>
<span class="sd">        ...         self.save_hyperparameters(params)</span>
<span class="sd">        ...     def forward(self, *args, **kwargs):</span>
<span class="sd">        ...         ...</span>
<span class="sd">        &gt;&gt;&gt; model = SingleArgModel(Namespace(p1=1, p2=&#39;abc&#39;, p3=3.14))</span>
<span class="sd">        &gt;&gt;&gt; model.hparams</span>
<span class="sd">        &quot;p1&quot;: 1</span>
<span class="sd">        &quot;p2&quot;: abc</span>
<span class="sd">        &quot;p3&quot;: 3.14</span>

<span class="sd">        &gt;&gt;&gt; class ManuallyArgsModel(HyperparametersMixin):</span>
<span class="sd">        ...     def __init__(self, arg1, arg2, arg3):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         # pass argument(s) to ignore as a string or in a list</span>
<span class="sd">        ...         self.save_hyperparameters(ignore=&#39;arg2&#39;)</span>
<span class="sd">        ...     def forward(self, *args, **kwargs):</span>
<span class="sd">        ...         ...</span>
<span class="sd">        &gt;&gt;&gt; model = ManuallyArgsModel(1, &#39;abc&#39;, 3.14)</span>
<span class="sd">        &gt;&gt;&gt; model.hparams</span>
<span class="sd">        &quot;arg1&quot;: 1</span>
<span class="sd">        &quot;arg3&quot;: 3.14</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_log_hyperparams</span> <span class="o">=</span> <span class="n">logger</span>
    <span class="c1"># the frame needs to be created in this file.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">frame</span><span class="p">:</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span><span class="o">.</span><span class="n">f_back</span>
    <span class="n">save_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">ignore</span><span class="o">=</span><span class="n">ignore</span><span class="p">,</span> <span class="n">frame</span><span class="o">=</span><span class="n">frame</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.set_extra_state" class="doc doc-heading">
<code class="highlight language-python"><span class="n">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.set_extra_state" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>state</code></td>
        <td><code>dict</code></td>
        <td><p>Extra state from the <code>state_dict</code></p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.setup" class="doc doc-heading">
<code class="highlight language-python"><span class="n">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.setup" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the beginning of fit (train + validate), validate, test, and predict.
This is a good hook when you need to build models dynamically or adjust something about them.
This hook is called on every process when using DDP.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>stage</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>either <code>'fit'</code>, <code>'validate'</code>, <code>'test'</code>, or <code>'predict'</code></p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>class LitModel(...):
    def __init__(self):
        self.l1 = None

    def prepare_data(self):
        download_data()
        tokenize()

        # don&#39;t do this
        self.something = else

    def setup(stage):
        data = Load_data(...)
        self.l1 = nn.Linear(28, data.num_classes)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the beginning of fit (train + validate), validate, test, and predict.</span>
<span class="sd">    This is a good hook when you need to build models dynamically or adjust something about them.</span>
<span class="sd">    This hook is called on every process when using DDP.</span>

<span class="sd">    Args:</span>
<span class="sd">        stage: either ``&#39;fit&#39;``, ``&#39;validate&#39;``, ``&#39;test&#39;``, or ``&#39;predict&#39;``</span>

<span class="sd">    Example::</span>

<span class="sd">        class LitModel(...):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                self.l1 = None</span>

<span class="sd">            def prepare_data(self):</span>
<span class="sd">                download_data()</span>
<span class="sd">                tokenize()</span>

<span class="sd">                # don&#39;t do this</span>
<span class="sd">                self.something = else</span>

<span class="sd">            def setup(stage):</span>
<span class="sd">                data = Load_data(...)</span>
<span class="sd">                self.l1 = nn.Linear(28, data.num_classes)</span>

<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.share_memory" class="doc doc-heading">
<code class="highlight language-python"><span class="n">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.share_memory" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>See :meth:<code>torch.Tensor.share_memory_</code></p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.state_dict" class="doc doc-heading">
<code class="highlight language-python"><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.state_dict" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>dict</code></td>
      <td><p>a dictionary containing a whole state of the module</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; module.state_dict().keys()
[&#39;bias&#39;, &#39;weight&#39;]
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>

<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.summarize" class="doc doc-heading">
<code class="highlight language-python"><span class="n">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">ModelSummary</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.summarize" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Summarize this LightningModule.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>mode</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Can be either <code>'top'</code> (summarize only direct submodules) or <code>'full'</code> (summarize all layers).</p>
<p>.. deprecated:: v1.4
    This parameter was deprecated in v1.4 in favor of <code>max_depth</code> and will be removed in v1.6.</p></td>
        <td><code>&#39;top&#39;</code></td>
      </tr>
      <tr>
        <td><code>max_depth</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Optional[pytorch_lightning.core.memory.ModelSummary]</code></td>
      <td><p>The model summary object</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;top&quot;</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelSummary</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summarize this LightningModule.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode: Can be either ``&#39;top&#39;`` (summarize only direct submodules) or ``&#39;full&#39;`` (summarize all layers).</span>

<span class="sd">            .. deprecated:: v1.4</span>
<span class="sd">                This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6.</span>

<span class="sd">        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the</span>
<span class="sd">            layer summary off. Default: 1.</span>

<span class="sd">    Return:</span>
<span class="sd">        The model summary object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_summary</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># temporary mapping from mode to max_depth</span>
    <span class="k">if</span> <span class="n">max_depth</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">in</span> <span class="n">ModelSummary</span><span class="o">.</span><span class="n">MODES</span><span class="p">:</span>
            <span class="n">max_depth</span> <span class="o">=</span> <span class="n">ModelSummary</span><span class="o">.</span><span class="n">MODES</span><span class="p">[</span><span class="n">mode</span><span class="p">]</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Argument `mode` in `LightningModule.summarize` is deprecated in v1.4&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and will be removed in v1.6. Use `max_depth=</span><span class="si">{</span><span class="n">max_depth</span><span class="si">}</span><span class="s2">` to replicate `mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">` behavior.&quot;</span>
            <span class="p">)</span>
            <span class="n">model_summary</span> <span class="o">=</span> <span class="n">ModelSummary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`mode` can be None, </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ModelSummary</span><span class="o">.</span><span class="n">MODES</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_summary</span> <span class="o">=</span> <span class="n">ModelSummary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">)</span>

    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">model_summary</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">model_summary</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.tbptt_split_batch" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">split_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.tbptt_split_batch" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Tensor</code></td>
        <td><p>Current batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>split_size</code></td>
        <td><code>int</code></td>
        <td><p>The size of the split</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List of batch splits. Each split will be passed to </code></td>
      <td><p>meth:<code>training_step</code> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p></td>
    </tr>
  </tbody>
</table>      <p>Examples::</p>
<div class="highlight"><pre><span></span><code>def tbptt_split_batch(self, batch, split_size):
  splits = []
  for t in range(0, time_dims[0], split_size):
      batch_split = []
      for i, x in enumerate(batch):
          if isinstance(x, torch.Tensor):
              split_x = x[:, t:t + split_size]
          elif isinstance(x, collections.Sequence):
              split_x = [None] * len(x)
              for batch_idx in range(len(x)):
                  split_x[batch_idx] = x[batch_idx][t:t + split_size]

          batch_split.append(split_x)

      splits.append(batch_split)

  return splits
</code></pre></div>
<p>!!! note
    Called in the training loop after
    :meth:<code>~pytorch_lightning.callbacks.base.Callback.on_batch_start</code>
    if :paramref:<code>~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps</code> &gt; 0.
    Each returned batch split is passed separately to :meth:<code>training_step</code>.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">split_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    When using truncated backpropagation through time, each batch must be split along the</span>
<span class="sd">    time dimension. Lightning handles this by default, but for custom behavior override</span>
<span class="sd">    this function.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: Current batch</span>
<span class="sd">        split_size: The size of the split</span>

<span class="sd">    Return:</span>
<span class="sd">        List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated</span>
<span class="sd">        back propagation through time. The default implementation splits root level Tensors and</span>
<span class="sd">        Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</span>

<span class="sd">    Examples::</span>

<span class="sd">        def tbptt_split_batch(self, batch, split_size):</span>
<span class="sd">          splits = []</span>
<span class="sd">          for t in range(0, time_dims[0], split_size):</span>
<span class="sd">              batch_split = []</span>
<span class="sd">              for i, x in enumerate(batch):</span>
<span class="sd">                  if isinstance(x, torch.Tensor):</span>
<span class="sd">                      split_x = x[:, t:t + split_size]</span>
<span class="sd">                  elif isinstance(x, collections.Sequence):</span>
<span class="sd">                      split_x = [None] * len(x)</span>
<span class="sd">                      for batch_idx in range(len(x)):</span>
<span class="sd">                          split_x[batch_idx] = x[batch_idx][t:t + split_size]</span>

<span class="sd">                  batch_split.append(split_x)</span>

<span class="sd">              splits.append(batch_split)</span>

<span class="sd">          return splits</span>

<span class="sd">    Note:</span>
<span class="sd">        Called in the training loop after</span>
<span class="sd">        :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start`</span>
<span class="sd">        if :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` &gt; 0.</span>
<span class="sd">        Each returned batch split is passed separately to :meth:`training_step`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">time_dims</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">))]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">time_dims</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Unable to determine batch time dimension&quot;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">time_dims</span><span class="p">),</span> <span class="s2">&quot;Batch time dimension length is ambiguous&quot;</span>

    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span> <span class="p">:</span> <span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                    <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span> <span class="p">:</span> <span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>

            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>

        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">splits</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.teardown" class="doc doc-heading">
<code class="highlight language-python"><span class="n">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.teardown" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>stage</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>either <code>'fit'</code>, <code>'validate'</code>, <code>'test'</code>, or <code>'predict'</code></p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of fit (train + validate), validate, test, predict, or tune.</span>

<span class="sd">    Args:</span>
<span class="sd">        stage: either ``&#39;fit&#39;``, ``&#39;validate&#39;``, ``&#39;test&#39;``, or ``&#39;predict&#39;``</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<div class="highlight"><pre><span></span><code>- download in :meth:`prepare_data`
- process and split in :meth:`setup`
</code></pre></div>
<p>However, the above are only necessary for distributed processing.</p>
<p>.. warning:: do not assign state in prepare_data</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>...</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<p>!!! note
    Lightning adds the correct sampler for distributed and arbitrary hardware.
    There is no need to set it yourself.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>A </code></td>
      <td><p>class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying testing samples.</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def test_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=False, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def test_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
</code></pre></div>
<p>!!! note
    If you don't need a test dataset and a :meth:<code>test_step</code>, you don't need to implement
    this method.</p>
<p>!!! note
    In the case where you return multiple test dataloaders, the :meth:<code>test_step</code>
    will have an argument <code>dataloader_idx</code> which matches the order here.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EVAL_DATALOADERS</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement one or multiple PyTorch DataLoaders for testing.</span>

<span class="sd">    The dataloader you return will not be reloaded unless you set</span>
<span class="sd">    :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to</span>
<span class="sd">    a postive integer.</span>

<span class="sd">    For data processing use the following pattern:</span>

<span class="sd">        - download in :meth:`prepare_data`</span>
<span class="sd">        - process and split in :meth:`setup`</span>

<span class="sd">    However, the above are only necessary for distributed processing.</span>

<span class="sd">    .. warning:: do not assign state in prepare_data</span>


<span class="sd">    - :meth:`~pytorch_lightning.trainer.Trainer.fit`</span>
<span class="sd">    - ...</span>
<span class="sd">    - :meth:`prepare_data`</span>
<span class="sd">    - :meth:`setup`</span>
<span class="sd">    - :meth:`train_dataloader`</span>
<span class="sd">    - :meth:`val_dataloader`</span>
<span class="sd">    - :meth:`test_dataloader`</span>

<span class="sd">    Note:</span>
<span class="sd">        Lightning adds the correct sampler for distributed and arbitrary hardware.</span>
<span class="sd">        There is no need to set it yourself.</span>

<span class="sd">    Return:</span>
<span class="sd">        A :class:`torch.utils.data.DataLoader` or a sequence of them specifying testing samples.</span>

<span class="sd">    Example::</span>

<span class="sd">        def test_dataloader(self):</span>
<span class="sd">            transform = transforms.Compose([transforms.ToTensor(),</span>
<span class="sd">                                            transforms.Normalize((0.5,), (1.0,))])</span>
<span class="sd">            dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=False, transform=transform,</span>
<span class="sd">                            download=True)</span>
<span class="sd">            loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=dataset,</span>
<span class="sd">                batch_size=self.batch_size,</span>
<span class="sd">                shuffle=False</span>
<span class="sd">            )</span>

<span class="sd">            return loader</span>

<span class="sd">        # can also return multiple dataloaders</span>
<span class="sd">        def test_dataloader(self):</span>
<span class="sd">            return [loader_a, loader_b, ..., loader_n]</span>

<span class="sd">    Note:</span>
<span class="sd">        If you don&#39;t need a test dataset and a :meth:`test_step`, you don&#39;t need to implement</span>
<span class="sd">        this method.</span>

<span class="sd">    Note:</span>
<span class="sd">        In the case where you return multiple test dataloaders, the :meth:`test_step`</span>
<span class="sd">        will have an argument ``dataloader_idx`` which matches the order here.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]])</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of a test epoch with the output of all test steps.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[Dict[str, numpy.ndarray]]</code></td>
        <td><p>List of outputs you defined in :meth:<code>test_step_end</code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>None</p></td>
    </tr>
  </tbody>
</table>      <p>!!! note
    If you didn't define a :meth:<code>test_step</code>, this won't be called.</p>

<p><strong>Examples:</strong></p>
    <p>With a single dataloader:</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>def test_epoch_end(self, outputs):
    # do something with the outputs of all test batches
    all_test_preds = test_step_outputs.predictions

    some_result = calc_all_results(all_test_preds)
    self.log(some_result)
</code></pre></div>
<p>With multiple dataloaders, <code>outputs</code> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>def test_epoch_end(self, outputs):
    final_value = 0
    for dataloader_outputs in outputs:
        for test_step_out in dataloader_outputs:
            # do something
            final_value += test_step_out

    self.log(&quot;final_metric&quot;, final_value)
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]):</span>
    <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_step_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">compute_and_log_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Operates on a single batch of data from the test set.
In this step you'd normally generate examples or calculate anything of interest
such as accuracy.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># the pseudocode for these calls
test_outs = []
for test_batch in test_data:
    out = test_step(test_batch)
    test_outs.append(out)
test_epoch_end(test_outs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, ...) | [:class:<code>~torch.Tensor</code>, ...]):
The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of this batch.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>Any of.</p>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Testing will skip to the next batch</li>
</ul></td>
    </tr>
  </tbody>
</table>      <p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># if you have one test dataloader:
def test_step(self, batch, batch_idx):
    ...


# if you have multiple test dataloaders:
def test_step(self, batch, batch_idx, dataloader_idx):
    ...
</code></pre></div>
<p>Examples::</p>
<div class="highlight"><pre><span></span><code># CASE 1: A single test dataset
def test_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({&#39;test_loss&#39;: loss, &#39;test_acc&#39;: test_acc})
</code></pre></div>
<p>If you pass in multiple test dataloaders, :meth:<code>test_step</code> will have an additional argument.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># CASE 2: multiple test dataloaders
def test_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre></div>
<p>!!! note
    If you don't need to test you don't need to implement this method.</p>
<p>!!! note
    When the :meth:<code>test_step</code> is called, the model has been put in eval mode and
    PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
    to training mode and gradients are enabled.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_step_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.test_step_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Use this when testing with dp or ddp2 because :meth:<code>test_step</code> will operate
on only part of the batch. However, this is still optional
and only needed for things like softmax or NCE loss.</p>
<p>!!! note
    If you later switch to ddp or some other mode, this will still be called
    so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]
test_step_end(batch_parts_outputs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch_parts_outputs</code></td>
        <td></td>
        <td><p>What you return in :meth:<code>test_step</code> for each batch part.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Dict[str, Any]]</code></td>
      <td><p>None or anything</p></td>
    </tr>
  </tbody>
</table>      <p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># WITHOUT test_step_end
# if used in DP or DDP2, this batch is 1/num_gpus large
def test_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)
    loss = self.softmax(out)
    self.log(&quot;test_loss&quot;, loss)


# --------------
# with test_step_end to do softmax over the full batch
def test_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    return out


def test_step_end(self, output_results):
    # this out is now the full size of the batch
    all_test_step_outs = output_results.out
    loss = nce_loss(all_test_step_outs)
    self.log(&quot;test_loss&quot;, loss)
</code></pre></div>
<p>See Also:
    See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use this when testing with dp or ddp2 because :meth:`test_step` will operate</span>
<span class="sd">    on only part of the batch. However, this is still optional</span>
<span class="sd">    and only needed for things like softmax or NCE loss.</span>

<span class="sd">    Note:</span>
<span class="sd">        If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">        so that you don&#39;t have to change your code.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # pseudocode</span>
<span class="sd">        sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">        batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">        test_step_end(batch_parts_outputs)</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_parts_outputs: What you return in :meth:`test_step` for each batch part.</span>

<span class="sd">    Return:</span>
<span class="sd">        None or anything</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # WITHOUT test_step_end</span>
<span class="sd">        # if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="sd">        def test_step(self, batch, batch_idx):</span>
<span class="sd">            # batch is 1/num_gpus big</span>
<span class="sd">            x, y = batch</span>

<span class="sd">            out = self(x)</span>
<span class="sd">            loss = self.softmax(out)</span>
<span class="sd">            self.log(&quot;test_loss&quot;, loss)</span>


<span class="sd">        # --------------</span>
<span class="sd">        # with test_step_end to do softmax over the full batch</span>
<span class="sd">        def test_step(self, batch, batch_idx):</span>
<span class="sd">            # batch is 1/num_gpus big</span>
<span class="sd">            x, y = batch</span>

<span class="sd">            out = self.encoder(x)</span>
<span class="sd">            return out</span>


<span class="sd">        def test_step_end(self, output_results):</span>
<span class="sd">            # this out is now the full size of the batch</span>
<span class="sd">            all_test_step_outs = output_results.out</span>
<span class="sd">            loss = nce_loss(all_test_step_outs)</span>
<span class="sd">            self.log(&quot;test_loss&quot;, loss)</span>

<span class="sd">    See Also:</span>
<span class="sd">        See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.to" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.to" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point desired :attr:<code>dtype</code> s. In addition, this method will
only cast the floating point parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<p>!!! note
    This method modifies the module in-place.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>device</code></td>
        <td></td>
        <td><p>the desired device of the parameters
and buffers in this module</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td></td>
        <td><p>the desired floating point type of
the floating point parameters and buffers in this module</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>tensor</code></td>
        <td></td>
        <td><p>Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>      <p>Example::
    &gt;&gt;&gt; class ExampleModule(DeviceDtypeModuleMixin):
    ...     def <strong>init</strong>(self, weight: torch.Tensor):
    ...         super().<strong>init</strong>()
    ...         self.register_buffer('weight', weight)
    &gt;&gt;&gt; _ = torch.manual_seed(0)
    &gt;&gt;&gt; module = ExampleModule(torch.rand(3, 4))
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]])
    &gt;&gt;&gt; module.to(torch.double)
    ExampleModule()
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]], dtype=torch.float64)
    &gt;&gt;&gt; cpu = torch.device('cpu')
    &gt;&gt;&gt; module.to(cpu, dtype=torch.half, non_blocking=True)
    ExampleModule()
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]], dtype=torch.float16)
    &gt;&gt;&gt; module.to(cpu)
    ExampleModule()
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]], dtype=torch.float16)
    &gt;&gt;&gt; module.device
    device(type='cpu')
    &gt;&gt;&gt; module.dtype
    torch.float16</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>

<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point desired :attr:`dtype` s. In addition, this method will</span>
<span class="sd">    only cast the floating point parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>

<span class="sd">    Note:</span>
<span class="sd">        This method modifies the module in-place.</span>

<span class="sd">    Args:</span>
<span class="sd">        device: the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype: the desired floating point type of</span>
<span class="sd">            the floating point parameters and buffers in this module</span>
<span class="sd">        tensor: Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; class ExampleModule(DeviceDtypeModuleMixin):</span>
<span class="sd">        ...     def __init__(self, weight: torch.Tensor):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.register_buffer(&#39;weight&#39;, weight)</span>
<span class="sd">        &gt;&gt;&gt; _ = torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; module = ExampleModule(torch.rand(3, 4))</span>
<span class="sd">        &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>
<span class="sd">        tensor([[...]])</span>
<span class="sd">        &gt;&gt;&gt; module.to(torch.double)</span>
<span class="sd">        ExampleModule()</span>
<span class="sd">        &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>
<span class="sd">        tensor([[...]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&#39;cpu&#39;)</span>
<span class="sd">        &gt;&gt;&gt; module.to(cpu, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        ExampleModule()</span>
<span class="sd">        &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>
<span class="sd">        tensor([[...]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; module.to(cpu)</span>
<span class="sd">        ExampleModule()</span>
<span class="sd">        &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>
<span class="sd">        tensor([[...]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; module.device</span>
<span class="sd">        device(type=&#39;cpu&#39;)</span>
<span class="sd">        &gt;&gt;&gt; module.dtype</span>
<span class="sd">        torch.float16</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># there is diff nb vars in PT 1.5</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_disk" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to_disk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">PathLike</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_disk" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">


        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">to_disk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">):</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;state_dict&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="s2">&quot;hyper_parameters&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_empty" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_empty" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Moves the parameters and buffers to the specified device without copying storage.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>device</code></td>
        <td></td>
        <td><p>class:<code>torch.device</code>): The desired device of the parameters
and buffers in this module.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_onnx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to_onnx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">],</span> <span class="n">input_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_onnx" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Saves the model in ONNX format.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>file_path</code></td>
        <td><code>Union[str, pathlib.Path]</code></td>
        <td><p>The path of the file the onnx model should be saved to.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>input_sample</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>An input for tracing. Default: None (Use self.example_input_array)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Will be passed to torch.onnx.export function.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="o">...</span>         <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="o">...</span>         <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">...</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="o">...</span>         <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</code></pre></div>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="o">...</span>     <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="o">...</span>     <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="o">...</span>     <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">...</span>     <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="kc">True</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">to_onnx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">],</span> <span class="n">input_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Saves the model in ONNX format.</span>

<span class="sd">    Args:</span>
<span class="sd">        file_path: The path of the file the onnx model should be saved to.</span>
<span class="sd">        input_sample: An input for tracing. Default: None (Use self.example_input_array)</span>
<span class="sd">        **kwargs: Will be passed to torch.onnx.export function.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; class SimpleModel(LightningModule):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.l1 = torch.nn.Linear(in_features=64, out_features=4)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def forward(self, x):</span>
<span class="sd">        ...         return torch.relu(self.l1(x.view(x.size(0), -1)))</span>

<span class="sd">        &gt;&gt;&gt; with tempfile.NamedTemporaryFile(suffix=&#39;.onnx&#39;, delete=False) as tmpfile:</span>
<span class="sd">        ...     model = SimpleModel()</span>
<span class="sd">        ...     input_sample = torch.randn((1, 64))</span>
<span class="sd">        ...     model.to_onnx(tmpfile.name, input_sample, export_params=True)</span>
<span class="sd">        ...     os.path.isfile(tmpfile.name)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>

    <span class="k">if</span> <span class="n">input_sample</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Could not export to ONNX since neither `input_sample` nor&quot;</span>
                <span class="s2">&quot; `model.example_input_array` attribute is set.&quot;</span>
            <span class="p">)</span>
        <span class="n">input_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span>

    <span class="n">input_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_batch_transfer_handler</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;example_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_sample</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">):</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;example_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">*</span><span class="n">input_sample</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;example_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">file_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_torchscript" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to_torchscript</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;script&#39;</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.to_torchscript" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>By default compiles the whole model to a :class:<code>~torch.jit.ScriptModule</code>.
If you want to use tracing, please provided the argument <code>method='trace'</code> and make sure that either the
<code>example_inputs</code> argument is provided, or the model has :attr:<code>example_input_array</code> set.
If you would like to customize the modules that are scripted you should override this method.
In case you want to return multiple modules, we recommend using a dictionary.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>file_path</code></td>
        <td><code>Union[str, pathlib.Path]</code></td>
        <td><p>Path where to save the torchscript. Default: None (no file saved).</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>method</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Whether to use TorchScript's script or trace method. Default: 'script'</p></td>
        <td><code>&#39;script&#39;</code></td>
      </tr>
      <tr>
        <td><code>example_inputs</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>An input to be used to do tracing when method is set to 'trace'.
Default: None (uses :attr:<code>example_input_array</code>)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td></td>
        <td><p>Additional arguments that will be passed to the :func:<code>torch.jit.script</code> or
:func:<code>torch.jit.trace</code> function.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>      <p>!!! note
    - Requires the implementation of the
      :meth:<code>~pytorch_lightning.core.lightning.LightningModule.forward</code> method.
    - The exported script will be set to evaluation mode.
    - It is recommended that you install the latest supported version of PyTorch
      to use this feature without limitations. See also the :mod:<code>torch.jit</code>
      documentation for supported features.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="o">...</span>         <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="o">...</span>         <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">...</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="o">...</span>         <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> <span class="c1"># doctest: +SKIP</span>
<span class="o">...</span>                                     <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  <span class="c1"># doctest: +SKIP</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="kc">True</span>
</code></pre></div>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch._C.ScriptModule, Dict[str, torch._C.ScriptModule]]</code></td>
      <td><p>This LightningModule as a torchscript, regardless of whether <code>file_path</code> is
defined or not.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">to_torchscript</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;script&quot;</span><span class="p">,</span>
    <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ScriptModule</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ScriptModule</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By default compiles the whole model to a :class:`~torch.jit.ScriptModule`.</span>
<span class="sd">    If you want to use tracing, please provided the argument ``method=&#39;trace&#39;`` and make sure that either the</span>
<span class="sd">    `example_inputs` argument is provided, or the model has :attr:`example_input_array` set.</span>
<span class="sd">    If you would like to customize the modules that are scripted you should override this method.</span>
<span class="sd">    In case you want to return multiple modules, we recommend using a dictionary.</span>

<span class="sd">    Args:</span>
<span class="sd">        file_path: Path where to save the torchscript. Default: None (no file saved).</span>
<span class="sd">        method: Whether to use TorchScript&#39;s script or trace method. Default: &#39;script&#39;</span>
<span class="sd">        example_inputs: An input to be used to do tracing when method is set to &#39;trace&#39;.</span>
<span class="sd">          Default: None (uses :attr:`example_input_array`)</span>
<span class="sd">        **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or</span>
<span class="sd">          :func:`torch.jit.trace` function.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Requires the implementation of the</span>
<span class="sd">          :meth:`~pytorch_lightning.core.lightning.LightningModule.forward` method.</span>
<span class="sd">        - The exported script will be set to evaluation mode.</span>
<span class="sd">        - It is recommended that you install the latest supported version of PyTorch</span>
<span class="sd">          to use this feature without limitations. See also the :mod:`torch.jit`</span>
<span class="sd">          documentation for supported features.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; class SimpleModel(LightningModule):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.l1 = torch.nn.Linear(in_features=64, out_features=4)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def forward(self, x):</span>
<span class="sd">        ...         return torch.relu(self.l1(x.view(x.size(0), -1)))</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; model = SimpleModel()</span>
<span class="sd">        &gt;&gt;&gt; torch.jit.save(model.to_torchscript(), &quot;model.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; os.path.isfile(&quot;model.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; torch.jit.save(model.to_torchscript(file_path=&quot;model_trace.pt&quot;, method=&#39;trace&#39;, # doctest: +SKIP</span>
<span class="sd">        ...                                     example_inputs=torch.randn(1, 64)))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; os.path.isfile(&quot;model_trace.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">        True</span>

<span class="sd">    Return:</span>
<span class="sd">        This LightningModule as a torchscript, regardless of whether `file_path` is</span>
<span class="sd">        defined or not.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;script&quot;</span><span class="p">:</span>
        <span class="n">torchscript_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;trace&quot;</span><span class="p">:</span>
        <span class="c1"># if no example inputs are provided, try to see if model has example_input_array set</span>
        <span class="k">if</span> <span class="n">example_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Choosing method=`trace` requires either `example_inputs`&quot;</span>
                    <span class="s2">&quot; or `model.example_input_array` to be defined.&quot;</span>
                <span class="p">)</span>
            <span class="n">example_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span>

        <span class="c1"># automatically send example inputs to the right device and use trace</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_batch_transfer_handler</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
        <span class="n">torchscript_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The &#39;method&#39; parameter only supports &#39;script&#39; or &#39;trace&#39;, but value given was: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">file_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fs</span> <span class="o">=</span> <span class="n">get_filesystem</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">fs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">torchscript_module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torchscript_module</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.toggle_optimizer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">toggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.toggle_optimizer" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Makes sure only the gradients of the current optimizer's parameters are calculated
in the training step to prevent dangling gradients in multiple-optimizer setup.
It works with :meth:<code>untoggle_optimizer</code> to make sure <code>param_requires_grad_state</code> is properly reset.
Override for your own behavior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Optimizer</code></td>
        <td><p>Current optimizer used in the training loop</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>int</code></td>
        <td><p>Current optimizer idx in the training loop</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>!!! note
    Only called when using multiple optimizers</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">toggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Makes sure only the gradients of the current optimizer&#39;s parameters are calculated</span>
<span class="sd">    in the training step to prevent dangling gradients in multiple-optimizer setup.</span>
<span class="sd">    It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.</span>
<span class="sd">    Override for your own behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer: Current optimizer used in the training loop</span>
<span class="sd">        optimizer_idx: Current optimizer idx in the training loop</span>

<span class="sd">    Note:</span>
<span class="sd">        Only called when using multiple optimizers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Iterate over all optimizer parameters to preserve their `requires_grad` information</span>
    <span class="c1"># in case these are pre-defined during `configure_optimizers`</span>
    <span class="n">param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">(</span><span class="n">use_pl_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="c1"># If a param already appear in param_requires_grad_state, continue</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_requires_grad_state</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Then iterate over the current optimizer&#39;s parameters and set its `requires_grad`</span>
    <span class="c1"># properties accordingly</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="n">param_requires_grad_state</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.train" class="doc doc-heading">
<code class="highlight language-python"><span class="n">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.train" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>mode</code></td>
        <td><code>bool</code></td>
        <td><p>whether to set training mode (<code>True</code>) or evaluation
         mode (<code>False</code>). Default: <code>True</code>.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>

<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.train_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.train_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Implement one or more PyTorch DataLoaders for training.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>A collection of </code></td>
      <td><p>class:<code>torch.utils.data.DataLoader</code> specifying training samples.
In the case of multiple dataloaders, please see this :ref:<code>page &lt;multiple-training-dataloaders&gt;</code>.</p></td>
    </tr>
  </tbody>
</table>      <p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<div class="highlight"><pre><span></span><code>- download in :meth:`prepare_data`
- process and split in :meth:`setup`
</code></pre></div>
<p>However, the above are only necessary for distributed processing.</p>
<p>.. warning:: do not assign state in prepare_data</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>...</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
</ul>
<p>!!! note
    Lightning adds the correct sampler for distributed and arbitrary hardware.
    There is no need to set it yourself.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code># single dataloader
def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=True
    )
    return loader

# multiple dataloaders, return as list
def train_dataloader(self):
    mnist = MNIST(...)
    cifar = CIFAR(...)
    mnist_loader = torch.utils.data.DataLoader(
        dataset=mnist, batch_size=self.batch_size, shuffle=True
    )
    cifar_loader = torch.utils.data.DataLoader(
        dataset=cifar, batch_size=self.batch_size, shuffle=True
    )
    # each batch will be a list of tensors: [batch_mnist, batch_cifar]
    return [mnist_loader, cifar_loader]

# multiple dataloader, return as dict
def train_dataloader(self):
    mnist = MNIST(...)
    cifar = CIFAR(...)
    mnist_loader = torch.utils.data.DataLoader(
        dataset=mnist, batch_size=self.batch_size, shuffle=True
    )
    cifar_loader = torch.utils.data.DataLoader(
        dataset=cifar, batch_size=self.batch_size, shuffle=True
    )
    # each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}
    return {&#39;mnist&#39;: mnist_loader, &#39;cifar&#39;: cifar_loader}
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TRAIN_DATALOADERS</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement one or more PyTorch DataLoaders for training.</span>

<span class="sd">    Return:</span>
<span class="sd">        A collection of :class:`torch.utils.data.DataLoader` specifying training samples.</span>
<span class="sd">        In the case of multiple dataloaders, please see this :ref:`page &lt;multiple-training-dataloaders&gt;`.</span>

<span class="sd">    The dataloader you return will not be reloaded unless you set</span>
<span class="sd">    :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to</span>
<span class="sd">    a positive integer.</span>

<span class="sd">    For data processing use the following pattern:</span>

<span class="sd">        - download in :meth:`prepare_data`</span>
<span class="sd">        - process and split in :meth:`setup`</span>

<span class="sd">    However, the above are only necessary for distributed processing.</span>

<span class="sd">    .. warning:: do not assign state in prepare_data</span>

<span class="sd">    - :meth:`~pytorch_lightning.trainer.Trainer.fit`</span>
<span class="sd">    - ...</span>
<span class="sd">    - :meth:`prepare_data`</span>
<span class="sd">    - :meth:`setup`</span>
<span class="sd">    - :meth:`train_dataloader`</span>

<span class="sd">    Note:</span>
<span class="sd">        Lightning adds the correct sampler for distributed and arbitrary hardware.</span>
<span class="sd">        There is no need to set it yourself.</span>

<span class="sd">    Example::</span>

<span class="sd">        # single dataloader</span>
<span class="sd">        def train_dataloader(self):</span>
<span class="sd">            transform = transforms.Compose([transforms.ToTensor(),</span>
<span class="sd">                                            transforms.Normalize((0.5,), (1.0,))])</span>
<span class="sd">            dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=True, transform=transform,</span>
<span class="sd">                            download=True)</span>
<span class="sd">            loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=dataset,</span>
<span class="sd">                batch_size=self.batch_size,</span>
<span class="sd">                shuffle=True</span>
<span class="sd">            )</span>
<span class="sd">            return loader</span>

<span class="sd">        # multiple dataloaders, return as list</span>
<span class="sd">        def train_dataloader(self):</span>
<span class="sd">            mnist = MNIST(...)</span>
<span class="sd">            cifar = CIFAR(...)</span>
<span class="sd">            mnist_loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=mnist, batch_size=self.batch_size, shuffle=True</span>
<span class="sd">            )</span>
<span class="sd">            cifar_loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=cifar, batch_size=self.batch_size, shuffle=True</span>
<span class="sd">            )</span>
<span class="sd">            # each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
<span class="sd">            return [mnist_loader, cifar_loader]</span>

<span class="sd">        # multiple dataloader, return as dict</span>
<span class="sd">        def train_dataloader(self):</span>
<span class="sd">            mnist = MNIST(...)</span>
<span class="sd">            cifar = CIFAR(...)</span>
<span class="sd">            mnist_loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=mnist, batch_size=self.batch_size, shuffle=True</span>
<span class="sd">            )</span>
<span class="sd">            cifar_loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=cifar, batch_size=self.batch_size, shuffle=True</span>
<span class="sd">            )</span>
<span class="sd">            # each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
<span class="sd">            return {&#39;mnist&#39;: mnist_loader, &#39;cifar&#39;: cifar_loader}</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`train_dataloader` must be implemented to be used with the Lightning Trainer&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.training_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.training_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Called at the end of the training epoch with the outputs of all training steps.
Use this in case you need to do something with all the outputs returned by :meth:<code>training_step</code>.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># the pseudocode for these calls
train_outs = []
for train_batch in train_data:
    out = training_step(train_batch)
    train_outs.append(out)
training_epoch_end(train_outs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[Union[torch.Tensor, Dict[str, Any]]]</code></td>
        <td><p>List of outputs you defined in :meth:<code>training_step</code>, or if there are
multiple dataloaders, a list containing a list of outputs for each dataloader.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>None</code></td>
      <td><p>None</p></td>
    </tr>
  </tbody>
</table>      <p>!!! note
    If this method is not overridden, this won't be called.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>def training_epoch_end(self, training_step_outputs):
    # do something with all training_step outputs
    return result
</code></pre></div>
<p>With multiple dataloaders, <code>outputs</code> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each training step for that dataloader.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>def training_epoch_end(self, training_step_outputs):
    for out in training_step_outputs:
        ...
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">EPOCH_OUTPUT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Called at the end of the training epoch with the outputs of all training steps.</span>
<span class="sd">    Use this in case you need to do something with all the outputs returned by :meth:`training_step`.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # the pseudocode for these calls</span>
<span class="sd">        train_outs = []</span>
<span class="sd">        for train_batch in train_data:</span>
<span class="sd">            out = training_step(train_batch)</span>
<span class="sd">            train_outs.append(out)</span>
<span class="sd">        training_epoch_end(train_outs)</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs: List of outputs you defined in :meth:`training_step`, or if there are</span>
<span class="sd">            multiple dataloaders, a list containing a list of outputs for each dataloader.</span>

<span class="sd">    Return:</span>
<span class="sd">        None</span>

<span class="sd">    Note:</span>
<span class="sd">        If this method is not overridden, this won&#39;t be called.</span>

<span class="sd">    Example::</span>

<span class="sd">        def training_epoch_end(self, training_step_outputs):</span>
<span class="sd">            # do something with all training_step outputs</span>
<span class="sd">            return result</span>

<span class="sd">    With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains</span>
<span class="sd">    one entry per dataloader, while the inner list contains the individual outputs of</span>
<span class="sd">    each training step for that dataloader.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def training_epoch_end(self, training_step_outputs):</span>
<span class="sd">            for out in training_step_outputs:</span>
<span class="sd">                ...</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.training_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.training_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, ...) | [:class:<code>~torch.Tensor</code>, ...]):
The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>Integer displaying index of this batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>int</code></td>
        <td><p>When using multiple optimizers, this argument will also be present.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hiddens(</code></td>
        <td></td>
        <td><p>class:<code>~torch.Tensor</code>): Passed in if
:paramref:<code>~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps</code> &gt; 0.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Any of.

- </code></td>
      <td><p>class:<code>~torch.Tensor</code> - The loss tensor
- <code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>
- <code>None</code> - Training will skip to the next batch. This is only for automatic optimization.
    This is not supported for multi-GPU or TPU, or using <code>DeepSpeed</code>.</p></td>
    </tr>
  </tbody>
</table>      <p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre></div>
<p>If you define multiple optimizers, this step will be called with an additional
<code>optimizer_idx</code> parameter.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx, optimizer_idx):
    if optimizer_idx == 0:
        # do training_step with encoder
        ...
    if optimizer_idx == 1:
        # do training_step with decoder
        ...
</code></pre></div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hidden states from the previous truncated backprop step
    ...
    out, hiddens = self.lstm(data, hiddens)
    ...
    return {&quot;loss&quot;: loss, &quot;hiddens&quot;: hiddens}
</code></pre></div>
<p>!!! note
    The loss value shown in the progress bar is smoothed (averaged) over the last values,
    so it differs from the actual loss returned in train/validation step.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.training_step_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.training_step_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Use this when training with dp or ddp2 because :meth:<code>training_step</code>
will operate on only part of the batch. However, this is still optional
and only needed for things like softmax or NCE loss.</p>
<p>!!! note
    If you later switch to ddp or some other mode, this will still be called
    so that you don't have to change your code</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]
training_step_end(batch_parts_outputs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch_parts_outputs</code></td>
        <td></td>
        <td><p>What you return in <code>training_step</code> for each batch part.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Dict[str, Any]]</code></td>
      <td><p>Anything</p></td>
    </tr>
  </tbody>
</table>      <p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>def training_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)

    # softmax uses only a portion of the batch in the denominator
    loss = self.softmax(out)
    loss = nce_loss(loss)
    return loss
</code></pre></div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>def training_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    return {&quot;pred&quot;: out}


def training_step_end(self, training_step_outputs):
    gpu_0_pred = training_step_outputs[0][&quot;pred&quot;]
    gpu_1_pred = training_step_outputs[1][&quot;pred&quot;]
    gpu_n_pred = training_step_outputs[n][&quot;pred&quot;]

    # this softmax now uses the full batch
    loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred])
    return loss
</code></pre></div>
<p>See Also:
    See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">STEP_OUTPUT</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use this when training with dp or ddp2 because :meth:`training_step`</span>
<span class="sd">    will operate on only part of the batch. However, this is still optional</span>
<span class="sd">    and only needed for things like softmax or NCE loss.</span>

<span class="sd">    Note:</span>
<span class="sd">        If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">        so that you don&#39;t have to change your code</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # pseudocode</span>
<span class="sd">        sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">        batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">        training_step_end(batch_parts_outputs)</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_parts_outputs: What you return in `training_step` for each batch part.</span>

<span class="sd">    Return:</span>
<span class="sd">        Anything</span>

<span class="sd">    When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def training_step(self, batch, batch_idx):</span>
<span class="sd">            # batch is 1/num_gpus big</span>
<span class="sd">            x, y = batch</span>

<span class="sd">            out = self(x)</span>

<span class="sd">            # softmax uses only a portion of the batch in the denominator</span>
<span class="sd">            loss = self.softmax(out)</span>
<span class="sd">            loss = nce_loss(loss)</span>
<span class="sd">            return loss</span>

<span class="sd">    If you wish to do something with all the parts of the batch, then use this method to do it:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def training_step(self, batch, batch_idx):</span>
<span class="sd">            # batch is 1/num_gpus big</span>
<span class="sd">            x, y = batch</span>

<span class="sd">            out = self.encoder(x)</span>
<span class="sd">            return {&quot;pred&quot;: out}</span>


<span class="sd">        def training_step_end(self, training_step_outputs):</span>
<span class="sd">            gpu_0_pred = training_step_outputs[0][&quot;pred&quot;]</span>
<span class="sd">            gpu_1_pred = training_step_outputs[1][&quot;pred&quot;]</span>
<span class="sd">            gpu_n_pred = training_step_outputs[n][&quot;pred&quot;]</span>

<span class="sd">            # this softmax now uses the full batch</span>
<span class="sd">            loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred])</span>
<span class="sd">            return loss</span>

<span class="sd">    See Also:</span>
<span class="sd">        See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.transfer_batch_to_device" class="doc doc-heading">
<code class="highlight language-python"><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.transfer_batch_to_device" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Override this hook if your :class:<code>~torch.utils.data.DataLoader</code> returns tensors
wrapped in a custom data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul>
<li>:class:<code>torch.Tensor</code> or anything that implements <code>.to(...)</code></li>
<li>:class:<code>list</code></li>
<li>:class:<code>dict</code></li>
<li>:class:<code>tuple</code></li>
<li>:class:<code>torchtext.data.batch.Batch</code></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).</p>
<p>!!! note
    This hook should only transfer the data and not modify it, nor should it move the data to
    any other device than the one passed in as argument (unless you know what you are doing).
    To check the current state of execution of this hook you can use
    <code>self.trainer.training/testing/validating/predicting</code> so that you can
    add different logic as per your requirement.</p>
<p>!!! note
    This hook only runs on single GPU training and DDP (no data-parallel).
    Data-Parallel support will come in near future.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Any</code></td>
        <td><p>A batch of data that needs to be transferred to a new device.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>device</code></td>
        <td><p>The target device as defined in PyTorch.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of the dataloader to which the batch belongs.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Any</code></td>
      <td><p>A reference to the data on the new device.</p></td>
    </tr>
  </tbody>
</table>      <p>Example::</p>
<div class="highlight"><pre><span></span><code>def transfer_batch_to_device(self, batch, device):
    if isinstance(batch, CustomBatch):
        # move all tensors in your custom data structure to the device
        batch.samples = batch.samples.to(device)
        batch.targets = batch.targets.to(device)
    !!! else
        batch = super().transfer_batch_to_device(data, device)
    return batch
</code></pre></div>
      <p>See Also:
    - :meth:<code>move_data_to_device</code>
    - :meth:<code>apply_to_collection</code></p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors</span>
<span class="sd">    wrapped in a custom data structure.</span>

<span class="sd">    The data types listed below (and any arbitrary nesting of them) are supported out of the box:</span>

<span class="sd">    - :class:`torch.Tensor` or anything that implements `.to(...)`</span>
<span class="sd">    - :class:`list`</span>
<span class="sd">    - :class:`dict`</span>
<span class="sd">    - :class:`tuple`</span>
<span class="sd">    - :class:`torchtext.data.batch.Batch`</span>

<span class="sd">    For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).</span>

<span class="sd">    Note:</span>
<span class="sd">        This hook should only transfer the data and not modify it, nor should it move the data to</span>
<span class="sd">        any other device than the one passed in as argument (unless you know what you are doing).</span>
<span class="sd">        To check the current state of execution of this hook you can use</span>
<span class="sd">        ``self.trainer.training/testing/validating/predicting`` so that you can</span>
<span class="sd">        add different logic as per your requirement.</span>

<span class="sd">    Note:</span>
<span class="sd">        This hook only runs on single GPU training and DDP (no data-parallel).</span>
<span class="sd">        Data-Parallel support will come in near future.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: A batch of data that needs to be transferred to a new device.</span>
<span class="sd">        device: The target device as defined in PyTorch.</span>
<span class="sd">        dataloader_idx: The index of the dataloader to which the batch belongs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A reference to the data on the new device.</span>

<span class="sd">    Example::</span>

<span class="sd">        def transfer_batch_to_device(self, batch, device):</span>
<span class="sd">            if isinstance(batch, CustomBatch):</span>
<span class="sd">                # move all tensors in your custom data structure to the device</span>
<span class="sd">                batch.samples = batch.samples.to(device)</span>
<span class="sd">                batch.targets = batch.targets.to(device)</span>
<span class="sd">            else:</span>
<span class="sd">                batch = super().transfer_batch_to_device(data, device)</span>
<span class="sd">            return batch</span>

<span class="sd">    Raises:</span>
<span class="sd">        MisconfigurationException:</span>
<span class="sd">            If using data-parallel, ``Trainer(accelerator=&#39;dp&#39;)``.</span>

<span class="sd">    See Also:</span>
<span class="sd">        - :meth:`move_data_to_device`</span>
<span class="sd">        - :meth:`apply_to_collection`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">move_data_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.type" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">DeviceDtypeModuleMixin</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.type" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dst_type</code></td>
        <td><code>type or string</code></td>
        <td><p>the desired type</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeviceDtypeModuleMixin&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        dst_type (type or string): the desired type</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">__update_properties</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dst_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="o">=</span><span class="n">dst_type</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.unfreeze" class="doc doc-heading">
<code class="highlight language-python"><span class="n">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.unfreeze" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Unfreeze all parameters for training.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code>model = MyLightningModule(...)
model.unfreeze()
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unfreeze all parameters for training.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        model = MyLightningModule(...)</span>
<span class="sd">        model.unfreeze()</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.untoggle_optimizer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">untoggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.untoggle_optimizer" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Resets the state of required gradients that were toggled with :meth:<code>toggle_optimizer</code>.
Override for your own behavior.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>optimizer_idx</code></td>
        <td><code>int</code></td>
        <td><p>Current optimizer idx in the training loop</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>!!! note
    Only called when using multiple optimizers</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">untoggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.</span>
<span class="sd">    Override for your own behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer_idx: Current optimizer idx in the training loop</span>

<span class="sd">    Note:</span>
<span class="sd">        Only called when using multiple optimizers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">opt_idx</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">(</span><span class="n">use_pl_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">!=</span> <span class="n">opt_idx</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span><span class="p">:</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
    <span class="c1"># save memory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.val_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.val_dataloader" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>...</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<p>!!! note
    Lightning adds the correct sampler for distributed and arbitrary hardware
    There is no need to set it yourself.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>A </code></td>
      <td><p>class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying validation samples.</p></td>
    </tr>
  </tbody>
</table>      <p>Examples::</p>
<div class="highlight"><pre><span></span><code>def val_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=False,
                    transform=transform, download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.batch_size,
        shuffle=False
    )

    return loader

# can also return multiple dataloaders
def val_dataloader(self):
    return [loader_a, loader_b, ..., loader_n]
</code></pre></div>
<p>!!! note
    If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
    implement this method.</p>
<p>!!! note
    In the case where you return multiple validation dataloaders, the :meth:<code>validation_step</code>
    will have an argument <code>dataloader_idx</code> which matches the order here.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EVAL_DATALOADERS</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement one or multiple PyTorch DataLoaders for validation.</span>

<span class="sd">    The dataloader you return will not be reloaded unless you set</span>
<span class="sd">    :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs` to</span>
<span class="sd">    a positive integer.</span>

<span class="sd">    It&#39;s recommended that all data downloads and preparation happen in :meth:`prepare_data`.</span>

<span class="sd">    - :meth:`~pytorch_lightning.trainer.Trainer.fit`</span>
<span class="sd">    - ...</span>
<span class="sd">    - :meth:`prepare_data`</span>
<span class="sd">    - :meth:`train_dataloader`</span>
<span class="sd">    - :meth:`val_dataloader`</span>
<span class="sd">    - :meth:`test_dataloader`</span>

<span class="sd">    Note:</span>
<span class="sd">        Lightning adds the correct sampler for distributed and arbitrary hardware</span>
<span class="sd">        There is no need to set it yourself.</span>

<span class="sd">    Return:</span>
<span class="sd">        A :class:`torch.utils.data.DataLoader` or a sequence of them specifying validation samples.</span>

<span class="sd">    Examples::</span>

<span class="sd">        def val_dataloader(self):</span>
<span class="sd">            transform = transforms.Compose([transforms.ToTensor(),</span>
<span class="sd">                                            transforms.Normalize((0.5,), (1.0,))])</span>
<span class="sd">            dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=False,</span>
<span class="sd">                            transform=transform, download=True)</span>
<span class="sd">            loader = torch.utils.data.DataLoader(</span>
<span class="sd">                dataset=dataset,</span>
<span class="sd">                batch_size=self.batch_size,</span>
<span class="sd">                shuffle=False</span>
<span class="sd">            )</span>

<span class="sd">            return loader</span>

<span class="sd">        # can also return multiple dataloaders</span>
<span class="sd">        def val_dataloader(self):</span>
<span class="sd">            return [loader_a, loader_b, ..., loader_n]</span>

<span class="sd">    Note:</span>
<span class="sd">        If you don&#39;t need a validation dataset and a :meth:`validation_step`, you don&#39;t need to</span>
<span class="sd">        implement this method.</span>

<span class="sd">    Note:</span>
<span class="sd">        In the case where you return multiple validation dataloaders, the :meth:`validation_step`</span>
<span class="sd">        will have an argument ``dataloader_idx`` which matches the order here.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.validation_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]])</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.validation_epoch_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Aggregates validation_step outputs to compute and log the validation macro F1 and top K
metrics.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[dict]</code></td>
        <td><p>list of output dictionaries from each validation step
containing y_pred and y_true.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]):</span>
    <span class="sd">&quot;&quot;&quot;Aggregates validation_step outputs to compute and log the validation macro F1 and top K</span>
<span class="sd">    metrics.</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs (List[dict]): list of output dictionaries from each validation step</span>
<span class="sd">            containing y_pred and y_true.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_step_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">compute_and_log_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.validation_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.validation_step" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Operates on a single batch of data from the validation set.
In this step you'd might generate examples or calculate anything of interest like accuracy.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    val_outs.append(out)
validation_epoch_end(val_outs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td></td>
        <td><p>class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, ...) | [:class:<code>~torch.Tensor</code>, ...]):
The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of this batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dataloader_idx</code></td>
        <td><code>int</code></td>
        <td><p>The index of the dataloader that produced this batch
(only if multiple val dataloaders used)</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><ul>
<li>Any object or value</li>
<li><code>None</code> - Validation will skip to the next batch</li>
</ul></td>
    </tr>
  </tbody>
</table>      <p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># pseudocode of order
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    if defined(&quot;validation_step_end&quot;):
        out = validation_step_end(out)
    val_outs.append(out)
val_outs = validation_epoch_end(val_outs)
</code></pre></div>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx):
    ...
</code></pre></div>
<p>Examples::</p>
<div class="highlight"><pre><span></span><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({&#39;val_loss&#39;: loss, &#39;val_acc&#39;: val_acc})
</code></pre></div>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre></div>
<p>!!! note
    If you don't need to validate you don't need to implement this method.</p>
<p>!!! note
    When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
    and PyTorch gradients have been disabled. At the end of validation,
    the model goes back to training mode and gradients are enabled.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

    <span class="n">y_proba</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_hat</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;y_true&quot;</span><span class="p">:</span> <span class="n">y</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
        <span class="s2">&quot;y_pred&quot;</span><span class="p">:</span> <span class="n">y_proba</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
        <span class="s2">&quot;y_proba&quot;</span><span class="p">:</span> <span class="n">y_proba</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.validation_step_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.validation_step_end" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Use this when validating with dp or ddp2 because :meth:<code>validation_step</code>
will operate on only part of the batch. However, this is still optional
and only needed for things like softmax or NCE loss.</p>
<p>!!! note
    If you later switch to ddp or some other mode, this will still be called
    so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># pseudocode
sub_batches = split_batches_for_dp(batch)
batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]
validation_step_end(batch_parts_outputs)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch_parts_outputs</code></td>
        <td></td>
        <td><p>What you return in :meth:<code>validation_step</code>
for each batch part.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Dict[str, Any]]</code></td>
      <td><p>None or anything</p></td>
    </tr>
  </tbody>
</table>      <p>.. code-block:: python</p>
<div class="highlight"><pre><span></span><code># WITHOUT validation_step_end
# if used in DP or DDP2, this batch is 1/num_gpus large
def validation_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self.encoder(x)
    loss = self.softmax(out)
    loss = nce_loss(loss)
    self.log(&quot;val_loss&quot;, loss)


# --------------
# with validation_step_end to do softmax over the full batch
def validation_step(self, batch, batch_idx):
    # batch is 1/num_gpus big
    x, y = batch

    out = self(x)
    return out


def validation_step_end(self, val_step_outputs):
    for out in val_step_outputs:
        ...
</code></pre></div>
<p>See Also:
    See the :ref:<code>advanced/multi_gpu:Multi-GPU training</code> guide for more details.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use this when validating with dp or ddp2 because :meth:`validation_step`</span>
<span class="sd">    will operate on only part of the batch. However, this is still optional</span>
<span class="sd">    and only needed for things like softmax or NCE loss.</span>

<span class="sd">    Note:</span>
<span class="sd">        If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">        so that you don&#39;t have to change your code.</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # pseudocode</span>
<span class="sd">        sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">        batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">        validation_step_end(batch_parts_outputs)</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_parts_outputs: What you return in :meth:`validation_step`</span>
<span class="sd">            for each batch part.</span>

<span class="sd">    Return:</span>
<span class="sd">        None or anything</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        # WITHOUT validation_step_end</span>
<span class="sd">        # if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="sd">        def validation_step(self, batch, batch_idx):</span>
<span class="sd">            # batch is 1/num_gpus big</span>
<span class="sd">            x, y = batch</span>

<span class="sd">            out = self.encoder(x)</span>
<span class="sd">            loss = self.softmax(out)</span>
<span class="sd">            loss = nce_loss(loss)</span>
<span class="sd">            self.log(&quot;val_loss&quot;, loss)</span>


<span class="sd">        # --------------</span>
<span class="sd">        # with validation_step_end to do softmax over the full batch</span>
<span class="sd">        def validation_step(self, batch, batch_idx):</span>
<span class="sd">            # batch is 1/num_gpus big</span>
<span class="sd">            x, y = batch</span>

<span class="sd">            out = self(x)</span>
<span class="sd">            return out</span>


<span class="sd">        def validation_step_end(self, val_step_outputs):</span>
<span class="sd">            for out in val_step_outputs:</span>
<span class="sd">                ...</span>

<span class="sd">    See Also:</span>
<span class="sd">        See the :ref:`advanced/multi_gpu:Multi-GPU training` guide for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.write_prediction" class="doc doc-heading">
<code class="highlight language-python"><span class="n">write_prediction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;predictions.pt&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.write_prediction" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Write predictions to disk using <code>torch.save</code></p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>self.write_prediction(&#39;pred&#39;, torch.tensor(...), filename=&#39;my_predictions.pt&#39;)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>str</code></td>
        <td><p>a string indicating the name to save the predictions under</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>value</code></td>
        <td><code>Union[torch.Tensor, List[torch.Tensor]]</code></td>
        <td><p>the predictions, either a single :class:<code>~torch.Tensor</code> or a list of them</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>filename</code></td>
        <td><code>str</code></td>
        <td><p>name of the file to save the predictions to</p></td>
        <td><code>&#39;predictions.pt&#39;</code></td>
      </tr>
  </tbody>
</table>      <p>!!! note
    when running in distributed mode, calling <code>write_prediction</code> will create a file for
    each device with respective names: <code>filename_rank_0.pt</code>, <code>filename_rank_1.pt</code>, ...</p>
<p>.. deprecated::v1.3
    Will be removed in v1.5.0.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">write_prediction</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;predictions.pt&quot;</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Write predictions to disk using ``torch.save``</span>

<span class="sd">    Example::</span>

<span class="sd">        self.write_prediction(&#39;pred&#39;, torch.tensor(...), filename=&#39;my_predictions.pt&#39;)</span>

<span class="sd">    Args:</span>
<span class="sd">        name: a string indicating the name to save the predictions under</span>
<span class="sd">        value: the predictions, either a single :class:`~torch.Tensor` or a list of them</span>
<span class="sd">        filename: name of the file to save the predictions to</span>

<span class="sd">    Note:</span>
<span class="sd">        when running in distributed mode, calling ``write_prediction`` will create a file for</span>
<span class="sd">        each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ...</span>

<span class="sd">    .. deprecated::v1.3</span>
<span class="sd">        Will be removed in v1.5.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rank_zero_deprecation</span><span class="p">(</span>
        <span class="s2">&quot;LightningModule method `write_prediction` was deprecated in v1.3 and will be removed in v1.5.&quot;</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_evaluation_loop</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">_add_prediction</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.write_prediction_dict" class="doc doc-heading">
<code class="highlight language-python"><span class="n">write_prediction_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;predictions.pt&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.write_prediction_dict" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Write a dictonary of predictions to disk at once using <code>torch.save</code></p>
<p>Example::</p>
<div class="highlight"><pre><span></span><code>pred_dict = {&#39;pred1&#39;: torch.tensor(...), &#39;pred2&#39;: torch.tensor(...)}
self.write_prediction_dict(pred_dict)
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>predictions_dict</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>dict containing predictions, where each prediction should
either be single :class:<code>~torch.Tensor</code> or a list of them</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>      <p>!!! note
    when running in distributed mode, calling <code>write_prediction_dict</code> will create a file for
    each device with respective names: <code>filename_rank_0.pt</code>, <code>filename_rank_1.pt</code>, ...</p>
<p>.. deprecated::v1.3
    Will be removed in v1.5.0.</p>

        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">write_prediction_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;predictions.pt&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Write a dictonary of predictions to disk at once using ``torch.save``</span>

<span class="sd">    Example::</span>

<span class="sd">        pred_dict = {&#39;pred1&#39;: torch.tensor(...), &#39;pred2&#39;: torch.tensor(...)}</span>
<span class="sd">        self.write_prediction_dict(pred_dict)</span>

<span class="sd">    Args:</span>
<span class="sd">        predictions_dict: dict containing predictions, where each prediction should</span>
<span class="sd">            either be single :class:`~torch.Tensor` or a list of them</span>

<span class="sd">    Note:</span>
<span class="sd">        when running in distributed mode, calling ``write_prediction_dict`` will create a file for</span>
<span class="sd">        each device with respective names: ``filename_rank_0.pt``, ``filename_rank_1.pt``, ...</span>

<span class="sd">    .. deprecated::v1.3</span>
<span class="sd">        Will be removed in v1.5.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rank_zero_deprecation</span><span class="p">(</span>
        <span class="s2">&quot;LightningModule method `write_prediction_dict` was deprecated in v1.3 and will be removed in v1.5.&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">predictions_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">write_prediction</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.xpu" class="doc doc-heading">
<code class="highlight language-python"><span class="n">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="o">~</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.xpu" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>device</code></td>
        <td><code>int</code></td>
        <td><p>if specified, all parameters will be
copied to that device</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>

<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h5 id="zamba.models.efficientnet_models.TimeDistributedEfficientNet.zero_grad" class="doc doc-heading">
<code class="highlight language-python"><span class="n">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-inherited"><code>inherited</code></small>
  </span>

<a href="#zamba.models.efficientnet_models.TimeDistributedEfficientNet.zero_grad" class="headerlink" title="Permanent link">&para;</a></h5>

    <div class="doc doc-contents ">

      <p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>set_to_none</code></td>
        <td><code>bool</code></td>
        <td><p>instead of setting to zero, set the grads to None.
See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>zamba/models/efficientnet_models.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>

<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../densepose_manager/" class="md-footer__link md-footer__link--prev" aria-label="Previous: zamba.models.densepose.densepose_manager" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              zamba.models.densepose.densepose_manager
            </div>
          </div>
        </a>
      
      
        
        <a href="../models-megadetector_lite_yolox/" class="md-footer__link md-footer__link--next" aria-label="Next: zamba.models.megadetector_lite_yolox" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              zamba.models.megadetector_lite_yolox
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.indexes", "navigation.sections"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.fcfe8b6d.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.b1047164.min.js"></script>
      
    
  </body>
</html>